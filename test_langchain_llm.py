#!/usr/bin/env python3
"""
LangChain ê¸°ë°˜ LLM API í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.agent.enhancements.llm_api_integration import LLMAPIIntegration, LLMConfig


def test_langchain_llm_integration():
    """LangChain LLM í†µí•© í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª LangChain LLM í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    config = LLMConfig(
        provider="bedrock",
        model_name="anthropic.claude-3-sonnet-20240229-v1:0",
        max_tokens=2000,
        temperature=0.1
    )

    try:
        # LLM API í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("ğŸ”§ LLM API í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        llm_system = LLMAPIIntegration(config)
        
        print(f"âœ… LangChain LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„±ê³µ")
        print(f"ğŸ¤– Provider: {config.provider}")
        print(f"ğŸ“Š Model: {config.model_name}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
        test_macro_data = {
            "VIX": pd.DataFrame({
                "Close": [20.5, 21.2, 19.8, 22.1, 20.9],
                "Volume": [1000000, 1200000, 900000, 1100000, 1050000]
            }, index=pd.date_range('2024-01-01', periods=5, freq='D')),
            "TNX": pd.DataFrame({
                "Close": [3.2, 3.4, 3.1, 3.5, 3.3],
                "Volume": [500000, 600000, 450000, 550000, 520000]
            }, index=pd.date_range('2024-01-01', periods=5, freq='D')),
            "TIPS": pd.DataFrame({
                "Close": [105.3, 105.8, 104.9, 106.2, 105.5],
                "Volume": [300000, 350000, 280000, 320000, 310000]
            }, index=pd.date_range('2024-01-01', periods=5, freq='D')),
            "DXY": pd.DataFrame({
                "Close": [102.5, 102.8, 102.1, 103.2, 102.9],
                "Volume": [800000, 850000, 780000, 900000, 870000]
            }, index=pd.date_range('2024-01-01', periods=5, freq='D'))
        }
        
        test_market_metrics = {
            "probabilities": {"UP": 0.6, "DOWN": 0.2, "SIDEWAYS": 0.2},
            "stat_arb_signals": "neutral",
            "rsi": 65.5,
            "macd": 0.25,
            "volume_ratio": 1.2
        }
        
        test_analysis_results = {
            "current_regime": "TRENDING_UP",
            "confidence": 0.75,
            "probabilities": {"UP": 0.6, "DOWN": 0.2, "SIDEWAYS": 0.2},
            "optimization_performance": {
                "sharpe_ratio": 1.85,
                "total_return": 0.12,
                "max_drawdown": -0.08
            },
            "validation_results": {
                "backtest_accuracy": 0.72,
                "out_of_sample_performance": 0.68
            }
        }
        
        # í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ íšë“ í…ŒìŠ¤íŠ¸
        print("\nğŸš€ í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ íšë“ í…ŒìŠ¤íŠ¸...")
        start_time = datetime.now()
        
        insights = llm_system.get_enhanced_insights(
            "TRENDING_UP",
            test_macro_data,
            test_market_metrics,
            test_analysis_results
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"âœ… ì¸ì‚¬ì´íŠ¸ íšë“ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ)")
        
        # API í†µê³„ ì¶œë ¥
        api_stats = llm_system.get_api_stats()
        print(f"\nğŸ“Š API í†µê³„:")
        print(f"   - ì´ í˜¸ì¶œ ìˆ˜: {api_stats['total_calls']}")
        print(f"   - ì„±ê³µ í˜¸ì¶œ ìˆ˜: {api_stats['successful_calls']}")
        print(f"   - ì‹¤íŒ¨ í˜¸ì¶œ ìˆ˜: {api_stats['failed_calls']}")
        print(f"   - ì„±ê³µë¥ : {api_stats['success_rate']:.1f}%")
        print(f"   - í‰ê·  ì‘ë‹µì‹œê°„: {api_stats['avg_response_time']:.2f}ì´ˆ")
        
        # ê²°ê³¼ ë¶„ì„
        print(f"\nğŸ” ê²°ê³¼ ë¶„ì„:")
        
        if "llm_enhanced_insights" in insights:
            llm_result = insights["llm_enhanced_insights"]
            print(f"âœ… LLM í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ íšë“ ì„±ê³µ")
            
            # ì‹œì¥ ì—­í•™ ë¶„ì„
            if "comprehensive_analysis" in llm_result:
                comp_analysis = llm_result["comprehensive_analysis"]
                market_dynamics = comp_analysis.get("market_dynamics", {})
                print(f"\nğŸ“ˆ ì‹œì¥ ì—­í•™:")
                print(f"   - ì£¼ìš” ë™ì¸: {market_dynamics.get('primary_drivers', [])}")
                print(f"   - ë³€ë™ì„± ìš”ì¸: {market_dynamics.get('volatility_factors', [])}")
                print(f"   - íŠ¸ë Œë“œ ê°•ë„: {market_dynamics.get('trend_strength', 'N/A')}")
                print(f"   - ëª¨ë©˜í…€ í’ˆì§ˆ: {market_dynamics.get('momentum_quality', 'N/A')}")
            
            # ë¦¬ìŠ¤í¬ í‰ê°€
            if "risk_assessment" in llm_result:
                risk_assessment = llm_result["risk_assessment"]
                print(f"\nâš ï¸ ë¦¬ìŠ¤í¬ í‰ê°€:")
                print(f"   - ë‹¨ê¸° ìœ„í—˜: {risk_assessment.get('short_term_risks', [])}")
                print(f"   - ì¤‘ê¸° ìœ„í—˜: {risk_assessment.get('medium_term_risks', [])}")
                print(f"   - ì¥ê¸° ìœ„í—˜: {risk_assessment.get('long_term_risks', [])}")
            
            # ì „ëµì  ì œì–¸
            if "strategic_recommendations" in llm_result:
                strategic_rec = llm_result["strategic_recommendations"]
                portfolio_alloc = strategic_rec.get("portfolio_allocation", {})
                print(f"\nğŸ’¼ í¬íŠ¸í´ë¦¬ì˜¤ ë°°ë¶„:")
                print(f"   - ì£¼ì‹: {portfolio_alloc.get('equity_allocation', 'N/A')}")
                print(f"   - ì±„ê¶Œ: {portfolio_alloc.get('bond_allocation', 'N/A')}")
                print(f"   - í˜„ê¸ˆ: {portfolio_alloc.get('cash_allocation', 'N/A')}")
                print(f"   - ëŒ€ì²´ìì‚°: {portfolio_alloc.get('alternative_allocation', 'N/A')}")
            
            # ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
            if "scenario_analysis" in llm_result:
                scenario_analysis = llm_result["scenario_analysis"]
                print(f"\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„:")
                for scenario, details in scenario_analysis.items():
                    if isinstance(details, dict):
                        prob = details.get('probability', 0)
                        print(f"   - {scenario}: {prob:.1%}")
            
            # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
            if "key_insights" in llm_result:
                key_insights = llm_result["key_insights"]
                print(f"\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
                for i, insight in enumerate(key_insights, 1):
                    print(f"   {i}. {insight}")
            
            # ì‹ ë¢°ë„ ìˆ˜ì •ì
            confidence_modifier = llm_result.get("confidence_modifier", 1.0)
            print(f"\nğŸšï¸ ì‹ ë¢°ë„ ìˆ˜ì •ì: {confidence_modifier:.2f}")
            
        else:
            print("âŒ LLM í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ ì—†ìŒ - ê·œì¹™ ê¸°ë°˜ ë¶„ì„ë§Œ ì‚¬ìš©ë¨")
        
        # ê·œì¹™ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ í™•ì¸
        if "rule_based_insights" in insights:
            rule_insights = insights["rule_based_insights"]
            print(f"\nğŸ“‹ ê·œì¹™ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸:")
            print(f"   - ì‹ ë¢°ë„: {rule_insights.get('confidence', 'N/A')}")
            print(f"   - ì „ëµì  ì œì–¸: {rule_insights.get('strategic_recommendations', [])}")
        
        # ì¡°ì •ëœ ì‹ ë¢°ë„
        if "adjusted_confidence" in insights:
            print(f"\nğŸšï¸ ì¡°ì •ëœ ì‹ ë¢°ë„: {insights['adjusted_confidence']:.3f}")
        
        print(f"\nğŸ‰ LangChain LLM í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_llm_config_parsing():
    """LLM ì„¤ì • íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”§ LLM ì„¤ì • íŒŒì‹± í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    try:
        # ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì • ì „ë‹¬
        config_dict = {
            "provider": "bedrock",
            "model_name": "anthropic.claude-3-sonnet-20240229-v1:0",
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        llm_system = LLMAPIIntegration(config_dict)
        print("âœ… ë”•ì…”ë„ˆë¦¬ ì„¤ì • íŒŒì‹± ì„±ê³µ")
        
        # LLMConfig ê°ì²´ë¡œ ì„¤ì • ì „ë‹¬
        config_obj = LLMConfig(
            provider="bedrock",
            model_name="anthropic.claude-3-sonnet-20240229-v1:0",
            max_tokens=2000,
            temperature=0.1
        )
        
        llm_system2 = LLMAPIIntegration(config_obj)
        print("âœ… LLMConfig ê°ì²´ ì„¤ì • íŒŒì‹± ì„±ê³µ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    print("ğŸš€ LangChain ê¸°ë°˜ LLM API í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 80)
    
    # ì„¤ì • íŒŒì‹± í…ŒìŠ¤íŠ¸
    config_test = test_llm_config_parsing()
    
    # ë©”ì¸ í†µí•© í…ŒìŠ¤íŠ¸
    main_test = test_langchain_llm_integration()
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 80)
    if config_test and main_test:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    else:
        print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    print("=" * 80) 