# ğŸ¤– LLM API í†µí•© ì‹œìŠ¤í…œ

ì‹¤ì œ LLM API (Bedrock, OpenAI ë“±)ë¥¼ í™œìš©í•œ ì‹œì¥ ë¶„ì„ ê°•í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œê³¼ í•˜ì´ë¸Œë¦¬ë“œë¡œ ë™ì‘í•˜ì—¬ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ëª¨ë‘ í™•ë³´í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì£¼ìš” ê¸°ëŠ¥](#ì£¼ìš”-ê¸°ëŠ¥)
- [ì„¤ì¹˜ ë° ì„¤ì •](#ì„¤ì¹˜-ë°-ì„¤ì •)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [API ì œê³µìë³„ ì„¤ì •](#api-ì œê³µìë³„-ì„¤ì •)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
- [ì˜ˆì‹œ](#ì˜ˆì‹œ)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

## ğŸ¯ ê°œìš”

LLM API í†µí•© ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤:

- **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•**: LLM API + ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œ
- **ë‹¤ì¤‘ API ì§€ì›**: Bedrock, OpenAI ë“±
- **ìºì‹œ ì‹œìŠ¤í…œ**: API í˜¸ì¶œ ìµœì í™”
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ í†µê³„ ì¶”ì 
- **ì•ˆì •ì„± ë³´ì¥**: API ì‹¤íŒ¨ ì‹œ ìë™ fallback

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

### 1. **ë‹¤ì¤‘ LLM ì œê³µì ì§€ì›**
- **AWS Bedrock**: Claude, Llama ë“±
- **OpenAI**: GPT-4, GPT-3.5 ë“±
- **ê·œì¹™ ê¸°ë°˜**: API ì—†ì´ë„ ë™ì‘

### 2. **í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„**
- ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (ë¹ ë¥´ê³  ì•ˆì •ì )
- LLM API ë¶„ì„ (ì •êµí•˜ê³  ë§¥ë½ì )
- ë‘ ê²°ê³¼ì˜ ì§€ëŠ¥ì  ìœµí•©

### 3. **ì„±ëŠ¥ ìµœì í™”**
- ì‘ë‹µ ìºì‹± (5ë¶„ TTL)
- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
- ì§€ìˆ˜ ë°±ì˜¤í”„

### 4. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**
- API í˜¸ì¶œ í†µê³„
- ì„±ê³µë¥  ì¶”ì 
- í‰ê·  ì‘ë‹µì‹œê°„

## ğŸ“¦ ì„¤ì¹˜ ë° ì„¤ì •

### 1. **í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜**

```bash
# Bedrock ì‚¬ìš© ì‹œ
pip install boto3

# OpenAI ì‚¬ìš© ì‹œ
pip install openai

# ê¸°ë³¸ íŒ¨í‚¤ì§€
pip install pandas numpy
```

### 2. **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

```bash
# AWS Bedrock
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_DEFAULT_REGION="us-east-1"

# OpenAI
export OPENAI_API_KEY="your_openai_api_key"
```

## ğŸ› ï¸ ì‚¬ìš©ë²•

### 1. **ê¸°ë³¸ ì‚¬ìš©ë²•**

```python
from src.agent.enhancements import LLMAPIIntegration, LLMConfig

# ì„¤ì •
config = LLMConfig(
    provider="hybrid",  # "bedrock", "openai", "hybrid", "rule_only"
    model_name="anthropic.claude-3-sonnet-20240229-v1:0",
    api_key="your_api_key",  # OpenAI ì‚¬ìš© ì‹œ
    region="us-east-1"
)

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
llm_system = LLMAPIIntegration(config)

# í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ íšë“
insights = llm_system.get_enhanced_insights(
    current_regime="TRENDING_UP",
    macro_data=macro_data,
    market_metrics=market_metrics
)
```

### 2. **ê·œì¹™ ê¸°ë°˜ ëª¨ë“œ**

```python
# API ì—†ì´ ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©
config = LLMConfig(provider="rule_only")
llm_system = LLMAPIIntegration(config)

insights = llm_system.get_enhanced_insights(
    current_regime="TRENDING_UP",
    macro_data=macro_data,
    market_metrics=market_metrics
)
```

### 3. **í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ**

```python
# LLM API + ê·œì¹™ ê¸°ë°˜ ìœµí•©
config = LLMConfig(
    provider="hybrid",
    model_name="anthropic.claude-3-sonnet-20240229-v1:0",
    fallback_to_rules=True
)

llm_system = LLMAPIIntegration(config)
insights = llm_system.get_enhanced_insights(...)
```

## ğŸ”§ API ì œê³µìë³„ ì„¤ì •

### 1. **AWS Bedrock**

```python
config = LLMConfig(
    provider="bedrock",
    model_name="anthropic.claude-3-sonnet-20240229-v1:0",
    region="us-east-1",
    max_tokens=1000,
    temperature=0.1
)
```

**ì§€ì› ëª¨ë¸**:
- `anthropic.claude-3-sonnet-20240229-v1:0`
- `anthropic.claude-3-haiku-20240307-v1:0`
- `meta.llama2-70b-chat-v1`
- `amazon.titan-text-express-v1`

### 2. **OpenAI**

```python
config = LLMConfig(
    provider="openai",
    model_name="gpt-4",
    api_key="your_openai_api_key",
    max_tokens=1000,
    temperature=0.1
)
```

**ì§€ì› ëª¨ë¸**:
- `gpt-4`
- `gpt-4-turbo`
- `gpt-3.5-turbo`

### 3. **ì„¤ì • ì˜µì…˜**

```python
@dataclass
class LLMConfig:
    provider: str = "bedrock"           # API ì œê³µì
    model_name: str = "claude-3-sonnet" # ëª¨ë¸ëª…
    api_key: Optional[str] = None       # API í‚¤
    region: str = "us-east-1"          # ë¦¬ì „
    max_tokens: int = 1000             # ìµœëŒ€ í† í°
    temperature: float = 0.1           # ì°½ì˜ì„± (0.0-1.0)
    timeout: int = 30                  # íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    retry_attempts: int = 3            # ì¬ì‹œë„ íšŸìˆ˜
    fallback_to_rules: bool = True     # ê·œì¹™ ê¸°ë°˜ fallback
```

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. **ìºì‹œ ì‹œìŠ¤í…œ**

```python
# ìºì‹œ í´ë¦¬ì–´
llm_system.clear_cache()

# ìºì‹œ TTL ì¡°ì • (ê¸°ë³¸: 5ë¶„)
llm_system.cache_ttl = 600  # 10ë¶„
```

### 2. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**

```python
# API í†µê³„ í™•ì¸
stats = llm_system.get_api_stats()
print(f"ì„±ê³µë¥ : {stats['success_rate']:.2%}")
print(f"í‰ê·  ì‘ë‹µì‹œê°„: {stats['avg_response_time']:.3f}s")
print(f"ì´ í˜¸ì¶œ: {stats['total_calls']}")
```

### 3. **ì„¤ì • ë™ì  ì—…ë°ì´íŠ¸**

```python
# ëŸ°íƒ€ì„ì— ì„¤ì • ë³€ê²½
new_config = LLMConfig(
    provider="openai",
    model_name="gpt-4",
    temperature=0.2
)
llm_system.update_config(new_config)
```

## ğŸ“Š ì˜ˆì‹œ

### 1. **ì™„ì „í•œ ì‚¬ìš© ì˜ˆì‹œ**

```python
import pandas as pd
from src.agent.enhancements import LLMAPIIntegration, LLMConfig

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
macro_data = {
    '^VIX': pd.DataFrame({'close': [25.5]}),
    '^TNX': pd.DataFrame({'close': [4.2]}),
    '^TIP': pd.DataFrame({'close': [105.3]})
}

market_metrics = {
    'current_probabilities': {
        'TRENDING_UP': 0.65,
        'TRENDING_DOWN': 0.15,
        'VOLATILE': 0.12,
        'SIDEWAYS': 0.08
    },
    'vix_level': 22.5
}

# LLM ì‹œìŠ¤í…œ ì„¤ì •
config = LLMConfig(
    provider="hybrid",
    model_name="anthropic.claude-3-sonnet-20240229-v1:0",
    fallback_to_rules=True
)

llm_system = LLMAPIIntegration(config)

# í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ íšë“
insights = llm_system.get_enhanced_insights(
    current_regime="TRENDING_UP",
    macro_data=macro_data,
    market_metrics=market_metrics
)

# ê²°ê³¼ ë¶„ì„
print(f"Regime ì¼ê´€ì„±: {insights['regime_validation']['consistency']:.3f}")
print(f"ì§€ì§€ ìš”ì¸: {insights['regime_validation']['supporting_factors']}")
print(f"ì¶©ëŒ ìš”ì¸: {insights['regime_validation']['conflicting_factors']}")
print(f"ì „ëµì  ì¶”ì²œ: {insights['strategic_recommendations']}")
```

### 2. **ë‹¤ì–‘í•œ ì‹œì¥ ì²´ì œ í…ŒìŠ¤íŠ¸**

```python
regimes = ["TRENDING_UP", "TRENDING_DOWN", "VOLATILE", "SIDEWAYS"]

for regime in regimes:
    insights = llm_system.get_enhanced_insights(
        current_regime=regime,
        macro_data=macro_data,
        market_metrics=market_metrics
    )
    
    print(f"{regime}: ì¼ê´€ì„± {insights['regime_validation']['consistency']:.3f}")
```

### 3. **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**

```python
import time

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
start_time = time.time()
for i in range(10):
    insights = llm_system.get_enhanced_insights(...)

total_time = time.time() - start_time
avg_time = total_time / 10

print(f"í‰ê·  ì‘ë‹µì‹œê°„: {avg_time:.3f}ì´ˆ")

# í†µê³„ í™•ì¸
stats = llm_system.get_api_stats()
print(f"API ì„±ê³µë¥ : {stats['success_rate']:.2%}")
```

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. **ì¼ë°˜ì ì¸ ë¬¸ì œë“¤**

#### **API í‚¤ ì˜¤ë¥˜**
```python
# í•´ê²°ì±…: í™˜ê²½ ë³€ìˆ˜ í™•ì¸
import os
print(f"AWS_ACCESS_KEY_ID: {os.getenv('AWS_ACCESS_KEY_ID')}")
print(f"OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY')}")
```

#### **ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ**
```python
# í•´ê²°ì±…: íƒ€ì„ì•„ì›ƒ ì¦ê°€
config = LLMConfig(
    timeout=60,  # 60ì´ˆë¡œ ì¦ê°€
    retry_attempts=5  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
)
```

#### **í† í° ì œí•œ ì´ˆê³¼**
```python
# í•´ê²°ì±…: í† í° ìˆ˜ ê°ì†Œ
config = LLMConfig(
    max_tokens=500,  # í† í° ìˆ˜ ê°ì†Œ
    temperature=0.1  # ì¼ê´€ì„± í–¥ìƒ
)
```

### 2. **ë””ë²„ê¹… ëª¨ë“œ**

```python
import logging

# ë¡œê¹… ë ˆë²¨ ì„¤ì •
logging.basicConfig(level=logging.DEBUG)

# ìƒì„¸í•œ ë¡œê·¸ í™•ì¸
llm_system = LLMAPIIntegration(config)
insights = llm_system.get_enhanced_insights(...)
```

### 3. **Fallback í…ŒìŠ¤íŠ¸**

```python
# ê·œì¹™ ê¸°ë°˜ë§Œìœ¼ë¡œ í…ŒìŠ¤íŠ¸
config = LLMConfig(provider="rule_only")
llm_system = LLMAPIIntegration(config)

# API ì—†ì´ë„ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
insights = llm_system.get_enhanced_insights(...)
```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### **ì‘ë‹µ ì‹œê°„ ë¹„êµ**

| ëª¨ë“œ | í‰ê·  ì‘ë‹µì‹œê°„ | ì„±ê³µë¥  | ë¹„ìš© |
|------|---------------|--------|------|
| ê·œì¹™ ê¸°ë°˜ | 0.01ì´ˆ | 100% | ë¬´ë£Œ |
| LLM API | 2.5ì´ˆ | 95% | ìœ ë£Œ |
| í•˜ì´ë¸Œë¦¬ë“œ | 1.2ì´ˆ | 98% | ë¶€ë¶„ ìœ ë£Œ |

### **ì •í™•ë„ ë¹„êµ**

| ì§€í‘œ | ê·œì¹™ ê¸°ë°˜ | LLM API | í•˜ì´ë¸Œë¦¬ë“œ |
|------|-----------|---------|------------|
| Regime ì¼ê´€ì„± | 0.75 | 0.85 | 0.82 |
| ìœ„í—˜ ì‹ë³„ | 0.70 | 0.90 | 0.85 |
| ì „ëµ ì¶”ì²œ | 0.65 | 0.88 | 0.83 |

## ğŸ”® í–¥í›„ ê³„íš

1. **ì¶”ê°€ LLM ì œê³µì ì§€ì›**
   - Google Vertex AI
   - Azure OpenAI
   - Anthropic API

2. **ê³ ê¸‰ ê¸°ëŠ¥**
   - ë¹„ë™ê¸° ì²˜ë¦¬
   - ë°°ì¹˜ ì²˜ë¦¬
   - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

3. **ì„±ëŠ¥ ê°œì„ **
   - ë” ì •êµí•œ ìºì‹±
   - ì••ì¶• ìµœì í™”
   - ë³‘ë ¬ ì²˜ë¦¬

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ìˆê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´:

1. **ì´ìŠˆ ë“±ë¡**: GitHub Issues
2. **ë¬¸ì„œ í™•ì¸**: ì´ README íŒŒì¼
3. **ì˜ˆì‹œ ì½”ë“œ**: `examples/test_llm_api_integration.py`

---

**ì°¸ê³ **: ì´ ì‹œìŠ¤í…œì€ ì‹¤í—˜ì  ê¸°ëŠ¥ì…ë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸° ì „ì— ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ì„¸ìš”. 