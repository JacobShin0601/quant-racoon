#!/usr/bin/env python3
"""
Swing ì „ëµ ê²°ê³¼ ì¡°íšŒ ìŠ¤í¬ë¦½íŠ¸
config_swing.json ê¸°ë°˜ evaluator ê²°ê³¼ë¥¼ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import sys
import os
import argparse
from pathlib import Path
import json
import glob
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)

from src.agent.evaluator import TrainTestEvaluator


def list_available_results(results_dir: str = "results/swing"):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ê²°ê³¼ íŒŒì¼ë“¤ ë‚˜ì—´"""
    print("ğŸ“‚ ì‚¬ìš© ê°€ëŠ¥í•œ Swing ì „ëµ ê²°ê³¼ íŒŒì¼ë“¤:")
    print("=" * 60)
    
    results_path = Path(results_dir)
    if not results_path.exists():
        print(f"âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {results_dir}")
        return
    
    # ìµœì í™” ê²°ê³¼ íŒŒì¼ë“¤
    optimization_files = list(results_path.glob("hyperparam_optimization_*.json"))
    portfolio_files = list(results_path.glob("portfolio_optimization_*.json"))
    performance_files = list(results_path.glob("performance_evaluation_*.txt"))
    
    print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼: {len(optimization_files)}ê°œ")
    for i, file in enumerate(sorted(optimization_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5], 1):
        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
        print(f"  {i}. {file.name} ({mod_time})")
    
    print(f"\nğŸ’¼ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ê²°ê³¼: {len(portfolio_files)}ê°œ")
    for i, file in enumerate(sorted(portfolio_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5], 1):
        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
        print(f"  {i}. {file.name} ({mod_time})")
    
    print(f"\nğŸ“ˆ ì„±ê³¼ í‰ê°€ ê²°ê³¼: {len(performance_files)}ê°œ")
    for i, file in enumerate(sorted(performance_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5], 1):
        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
        print(f"  {i}. {file.name} ({mod_time})")
    
    print("=" * 60)


def run_evaluator(config_path: str = "config/config_swing.json", 
                 data_dir: str = "data/swing",
                 symbols: list = None,
                 optimization_results: str = None,
                 portfolio_results: str = None):
    """Evaluator ì‹¤í–‰"""
    print("ğŸš€ Train/Test í‰ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰")
    print("=" * 60)
    
    try:
        # Evaluator ì´ˆê¸°í™”
        evaluator = TrainTestEvaluator(
            data_dir=data_dir,
            log_mode="summary",
            config_path=config_path,
            optimization_results_path=optimization_results,
            portfolio_results_path=portfolio_results,
        )
        
        # í‰ê°€ ì‹¤í–‰
        results = evaluator.run_train_test_evaluation(
            symbols=symbols,
            save_results=True,
        )
        
        if results:
            print("\nâœ… í‰ê°€ ì™„ë£Œ!")
            if results.get("table_path"):
                print(f"ğŸ“„ ì„±ê³¼ í…Œì´ë¸”: {results['table_path']}")
        else:
            print("âŒ í‰ê°€ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


def view_latest_performance(results_dir: str = "results/swing"):
    """ìµœì‹  ì„±ê³¼ í‰ê°€ ê²°ê³¼ íŒŒì¼ ë‚´ìš© ì¶œë ¥"""
    results_path = Path(results_dir)
    
    # ìµœì‹  ì„±ê³¼ í‰ê°€ íŒŒì¼ ì°¾ê¸°
    performance_files = list(results_path.glob("performance_evaluation_*.txt"))
    if not performance_files:
        print(f"âŒ ì„±ê³¼ í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_dir}")
        return
    
    latest_file = max(performance_files, key=lambda x: x.stat().st_mtime)
    
    print(f"ğŸ“„ ìµœì‹  ì„±ê³¼ í‰ê°€ ê²°ê³¼: {latest_file.name}")
    print("=" * 80)
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            content = f.read()
            print(content)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")


def analyze_optimization_results(results_dir: str = "results/swing"):
    """ìµœì í™” ê²°ê³¼ ë¶„ì„"""
    results_path = Path(results_dir)
    
    # ìµœì‹  ìµœì í™” ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
    optimization_files = list(results_path.glob("hyperparam_optimization_*.json"))
    if not optimization_files:
        print(f"âŒ ìµœì í™” ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_dir}")
        return
    
    latest_file = max(optimization_files, key=lambda x: x.stat().st_mtime)
    
    print(f"ğŸ“Š ìµœì í™” ê²°ê³¼ ë¶„ì„: {latest_file.name}")
    print("=" * 80)
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # ê²°ê³¼ ë¶„ì„
        strategy_scores = {}
        symbol_scores = {}
        
        for key, result in results.items():
            strategy = result.get('strategy_name', 'UNKNOWN')
            symbol = result.get('symbol', 'UNKNOWN')
            score = result.get('best_score', 0)
            
            if strategy not in strategy_scores:
                strategy_scores[strategy] = []
            strategy_scores[strategy].append(score)
            
            if symbol not in symbol_scores:
                symbol_scores[symbol] = []
            symbol_scores[symbol].append(score)
        
        # ì „ëµë³„ í‰ê·  ì ìˆ˜
        print("ğŸ“ˆ ì „ëµë³„ í‰ê·  ì ìˆ˜:")
        print("-" * 40)
        strategy_avg = {s: sum(scores)/len(scores) for s, scores in strategy_scores.items()}
        for strategy, avg_score in sorted(strategy_avg.items(), key=lambda x: x[1], reverse=True):
            print(f"  {strategy:<25}: {avg_score:>8.2f}")
        
        # ì¢…ëª©ë³„ í‰ê·  ì ìˆ˜
        print(f"\nğŸ’¼ ì¢…ëª©ë³„ í‰ê·  ì ìˆ˜:")
        print("-" * 40)
        symbol_avg = {s: sum(scores)/len(scores) for s, scores in symbol_scores.items()}
        for symbol, avg_score in sorted(symbol_avg.items(), key=lambda x: x[1], reverse=True):
            print(f"  {symbol:<8}: {avg_score:>8.2f}")
        
        # ì „ì²´ í†µê³„
        all_scores = [result.get('best_score', 0) for result in results.values()]
        valid_scores = [s for s in all_scores if s > -999999]
        
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print("-" * 40)
        print(f"  ì´ ì¡°í•© ìˆ˜: {len(results)}")
        print(f"  ì„±ê³µ ì¡°í•©: {len(valid_scores)}")
        print(f"  ì„±ê³µë¥ : {len(valid_scores)/len(results)*100:.1f}%")
        if valid_scores:
            print(f"  í‰ê·  ì ìˆ˜: {sum(valid_scores)/len(valid_scores):.2f}")
            print(f"  ìµœê³  ì ìˆ˜: {max(valid_scores):.2f}")
            print(f"  ìµœì € ì ìˆ˜: {min(valid_scores):.2f}")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Swing ì „ëµ ê²°ê³¼ ì¡°íšŒ ë° í‰ê°€ ë„êµ¬",
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ì‚¬ìš© ê°€ëŠ¥í•œ ê²°ê³¼ íŒŒì¼ ëª©ë¡
  python view_swing_results.py --list
  
  # Evaluator ì‹¤í–‰ (ê¸°ë³¸ ì„¤ì •)
  python view_swing_results.py --run
  
  # íŠ¹ì • ì¢…ëª©ë§Œ í‰ê°€
  python view_swing_results.py --run --symbols AAPL META NFLX
  
  # ìµœì‹  ì„±ê³¼ ê²°ê³¼ ë³´ê¸°
  python view_swing_results.py --view
  
  # ìµœì í™” ê²°ê³¼ ë¶„ì„
  python view_swing_results.py --analyze
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("--list", action="store_true", help="ì‚¬ìš© ê°€ëŠ¥í•œ ê²°ê³¼ íŒŒì¼ ëª©ë¡")
    parser.add_argument("--run", action="store_true", help="Evaluator ì‹¤í–‰")
    parser.add_argument("--view", action="store_true", help="ìµœì‹  ì„±ê³¼ ê²°ê³¼ ë³´ê¸°")
    parser.add_argument("--analyze", action="store_true", help="ìµœì í™” ê²°ê³¼ ë¶„ì„")
    
    parser.add_argument("--config", default="config/config_swing.json", help="ì„¤ì • íŒŒì¼")
    parser.add_argument("--data-dir", default="data/swing", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--results-dir", default="results/swing", help="ê²°ê³¼ ë””ë ‰í† ë¦¬")
    parser.add_argument("--symbols", nargs="+", help="í‰ê°€í•  ì¢…ëª© ëª©ë¡")
    parser.add_argument("--optimization-results", help="ìµœì í™” ê²°ê³¼ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--portfolio-results", help="í¬íŠ¸í´ë¦¬ì˜¤ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ì˜µì…˜ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ list ì‹¤í–‰
    if not any([args.list, args.run, args.view, args.analyze]):
        args.list = True
    
    try:
        if args.list:
            list_available_results(args.results_dir)
        
        if args.run:
            run_evaluator(
                config_path=args.config,
                data_dir=args.data_dir,
                symbols=args.symbols,
                optimization_results=args.optimization_results,
                portfolio_results=args.portfolio_results
            )
        
        if args.view:
            view_latest_performance(args.results_dir)
        
        if args.analyze:
            analyze_optimization_results(args.results_dir)
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main()) 