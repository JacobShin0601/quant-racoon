#!/bin/bash

# ê³ ë„í™”ëœ ì‹œìž¥ ë¶„ì„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
# LLM API í†µí•© ë° ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ í¬í•¨

set -e

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# ë¡œê·¸ í•¨ìˆ˜
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_advanced() {
    echo -e "${PURPLE}[ADVANCED]${NC} $1"
}

# ë„ì›€ë§ ì¶œë ¥
show_help() {
    echo "ðŸš€ ê³ ë„í™”ëœ ì‹œìž¥ ë¶„ì„ ì‹œìŠ¤í…œ"
    echo ""
    echo "ì‚¬ìš©ë²•: $0 [ì˜µì…˜]"
    echo ""
    echo "ì˜µì…˜:"
    echo "  -h, --help                    ì´ ë„ì›€ë§ì„ í‘œì‹œ"
    echo "  -t, --type TYPE               ë¶„ì„ ìœ í˜• ì„ íƒ"
    echo "      basic                     ê¸°ë³¸ ë¶„ì„ (ê¸°ë³¸ê°’)"
    echo "      enhanced                  ê³ ë„í™”ëœ ë¶„ì„ (RLMF, ì‹ ë¢°ë„, Regime ê°ì§€)"
    echo "      llm-api                   LLM API í†µí•© ë¶„ì„"
    echo "      full                      ëª¨ë“  ê¸°ëŠ¥ í†µí•© ë¶„ì„"
    echo ""
    echo "  -p, --provider PROVIDER       LLM API ì œê³µìž ì„ íƒ"
    echo "      bedrock                   AWS Bedrock (ê¸°ë³¸ê°’)"
    echo "      openai                    OpenAI"
    echo "      hybrid                    í•˜ì´ë¸Œë¦¬ë“œ (API + ê·œì¹™ ê¸°ë°˜)"
    echo "      rule-only                 ê·œì¹™ ê¸°ë°˜ë§Œ"
    echo ""
    echo "  -m, --model MODEL             LLM ëª¨ë¸ ì„ íƒ"
    echo "      claude-3-sonnet          Claude 3 Sonnet (ê¸°ë³¸ê°’)"
    echo "      claude-3-haiku           Claude 3 Haiku"
    echo "      gpt-4                    GPT-4"
    echo "      gpt-3.5-turbo            GPT-3.5 Turbo"
    echo ""
    echo "  -k, --api-key KEY             API í‚¤ ì„¤ì •"
    echo "  -r, --region REGION           AWS ë¦¬ì „ ì„¤ì • (ê¸°ë³¸ê°’: us-east-1)"
    echo "  -o, --output DIR              ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬"
    echo "  -v, --verbose                 ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    echo ""
    echo "ì˜ˆì‹œ:"
    echo "  $0 --type enhanced                    # ê³ ë„í™”ëœ ë¶„ì„"
    echo "  $0 --type llm-api --provider openai  # OpenAI API ë¶„ì„"
    echo "  $0 --type full --provider hybrid     # ëª¨ë“  ê¸°ëŠ¥ í†µí•©"
    echo ""
}

# ê¸°ë³¸ ì„¤ì •
ANALYSIS_TYPE="basic"
LLM_PROVIDER="bedrock"
LLM_MODEL="claude-3-sonnet"
API_KEY=""
REGION="us-east-1"
OUTPUT_DIR="results/enhanced_analysis"
VERBOSE=false

# ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_help
            exit 0
            ;;
        -t|--type)
            ANALYSIS_TYPE="$2"
            shift 2
            ;;
        -p|--provider)
            LLM_PROVIDER="$2"
            shift 2
            ;;
        -m|--model)
            LLM_MODEL="$2"
            shift 2
            ;;
        -k|--api-key)
            API_KEY="$2"
            shift 2
            ;;
        -r|--region)
            REGION="$2"
            shift 2
            ;;
        -o|--output)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        -v|--verbose)
            VERBOSE=true
            shift
            ;;
        *)
            log_error "ì•Œ ìˆ˜ ì—†ëŠ” ì˜µì…˜: $1"
            show_help
            exit 1
            ;;
    esac
done

# ë¶„ì„ ìœ í˜• ê²€ì¦
case $ANALYSIS_TYPE in
    basic|enhanced|llm-api|full)
        ;;
    *)
        log_error "ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„ì„ ìœ í˜•: $ANALYSIS_TYPE"
        exit 1
        ;;
esac

# LLM ì œê³µìž ê²€ì¦
case $LLM_PROVIDER in
    bedrock|openai|hybrid|rule-only)
        ;;
    *)
        log_error "ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µìž: $LLM_PROVIDER"
        exit 1
        ;;
esac

# ëª¨ë¸ ë§¤í•‘
case $LLM_MODEL in
    claude-3-sonnet)
        MODEL_NAME="anthropic.claude-3-sonnet-20240229-v1:0"
        ;;
    claude-3-haiku)
        MODEL_NAME="anthropic.claude-3-haiku-20240307-v1:0"
        ;;
    gpt-4)
        MODEL_NAME="gpt-4"
        ;;
    gpt-3.5-turbo)
        MODEL_NAME="gpt-3.5-turbo"
        ;;
    *)
        log_error "ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: $LLM_MODEL"
        exit 1
        ;;
esac

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$OUTPUT_DIR"

# íƒ€ìž„ìŠ¤íƒ¬í”„ ìƒì„±
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="$OUTPUT_DIR/analysis_${ANALYSIS_TYPE}_${TIMESTAMP}.log"

# ë¡œê·¸ í•¨ìˆ˜ ì—…ë°ì´íŠ¸
if [ "$VERBOSE" = true ]; then
    exec 1> >(tee -a "$LOG_FILE")
    exec 2> >(tee -a "$LOG_FILE" >&2)
fi

log_info "ðŸš€ ê³ ë„í™”ëœ ì‹œìž¥ ë¶„ì„ ì‹œìž‘"
log_info "ë¶„ì„ ìœ í˜•: $ANALYSIS_TYPE"
log_info "LLM ì œê³µìž: $LLM_PROVIDER"
log_info "ëª¨ë¸: $LLM_MODEL"
log_info "ì¶œë ¥ ë””ë ‰í† ë¦¬: $OUTPUT_DIR"
log_info "ë¡œê·¸ íŒŒì¼: $LOG_FILE"

# Python ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
create_python_script() {
    local script_file="$OUTPUT_DIR/run_analysis_${TIMESTAMP}.py"
    
    cat > "$script_file" << EOF
#!/usr/bin/env python3
"""
ê³ ë„í™”ëœ ì‹œìž¥ ë¶„ì„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ìƒì„± ì‹œê°„: $(date)
ë¶„ì„ ìœ í˜•: $ANALYSIS_TYPE
LLM ì œê³µìž: $LLM_PROVIDER
ëª¨ë¸: $LLM_MODEL
"""

import sys
import os
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.agent.market_sensor import MarketSensor
from src.agent.enhancements import LLMConfig

def main():
    print("ðŸš€ ê³ ë„í™”ëœ ì‹œìž¥ ë¶„ì„ ì‹œìž‘")
    
    # LLM ì„¤ì •
    llm_config = None
    enable_llm_api = False
    
    if "$ANALYSIS_TYPE" in ["llm-api", "full"]:
        llm_config = LLMConfig(
            provider="$LLM_PROVIDER",
            model_name="$MODEL_NAME",
            api_key="$API_KEY" if "$API_KEY" else None,
            region="$REGION",
            fallback_to_rules=True
        )
        enable_llm_api = True
        print(f"ðŸ¤– LLM API ì„¤ì •: {llm_config.provider} - {llm_config.model_name}")
    
    # Market Sensor ì´ˆê¸°í™”
    sensor = MarketSensor(
        enable_llm_api=enable_llm_api,
        llm_config=llm_config
    )
    
    # ë¶„ì„ ìˆ˜í–‰
    if "$ANALYSIS_TYPE" == "basic":
        print("ðŸ“Š ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        analysis = sensor.get_current_market_analysis(
            use_optimized_params=True,
            use_ml_model=True
        )
    else:
        print("ðŸš€ ê³ ë„í™”ëœ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        analysis = sensor.get_enhanced_market_analysis(
            use_optimized_params=True,
            use_ml_model=True,
            enable_advanced_features=True
        )
    
    # ê²°ê³¼ ì €ìž¥
    output_file = "$OUTPUT_DIR/analysis_results_${TIMESTAMP}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"âœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ ì €ìž¥: {output_file}")
    
    # ìš”ì•½ ì¶œë ¥
    print("\\nðŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
    print(f"í˜„ìž¬ ì²´ì œ: {analysis.get('current_regime', 'N/A')}")
    
    if 'final_confidence' in analysis:
        final_conf = analysis['final_confidence'].get('final_confidence', 0.5)
        print(f"ìµœì¢… ì‹ ë¢°ë„: {final_conf:.3f}")
    
    if 'rlmf_analysis' in analysis:
        rlmf = analysis['rlmf_analysis']
        if 'statistical_arbitrage' in rlmf:
            stat_arb = rlmf['statistical_arbitrage']
            print(f"Statistical Arbitrage: {stat_arb.get('direction', 'N/A')} (ì‹ ë¢°ë„: {stat_arb.get('confidence', 0):.3f})")
    
    if 'regime_detection' in analysis:
        regime_det = analysis['regime_detection']
        if 'regime_shift_detection' in regime_det:
            shift_det = regime_det['regime_shift_detection']
            if shift_det.get('regime_shift_detected', False):
                print("âš ï¸ ì‹œìž¥ ì²´ì œ ì „í™˜ ê°ì§€ë¨!")
    
    if 'llm_api_insights' in analysis:
        print("ðŸ¤– LLM API ë¶„ì„ ì™„ë£Œ")
        api_stats = analysis['llm_api_insights'].get('api_stats', {})
        if api_stats:
            print(f"API ì„±ê³µë¥ : {api_stats.get('success_rate', 0):.2%}")
    
    # LLM API í†µê³„ (í™œì„±í™”ëœ ê²½ìš°)
    if sensor.llm_api_system:
        stats = sensor.get_llm_api_stats()
        print(f"\\nðŸ“ˆ LLM API í†µê³„:")
        print(f"ì´ í˜¸ì¶œ: {stats.get('total_calls', 0)}")
        print(f"ì„±ê³µë¥ : {stats.get('success_rate', 0):.2%}")
        print(f"í‰ê·  ì‘ë‹µì‹œê°„: {stats.get('avg_response_time', 0):.3f}ì´ˆ")

if __name__ == "__main__":
    main()
EOF

    echo "$script_file"
}

# Python ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë° ì‹¤í–‰
log_info "ðŸ“ Python ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."
SCRIPT_FILE=$(create_python_script)
chmod +x "$SCRIPT_FILE"

log_info "ðŸ Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘..."
python3 "$SCRIPT_FILE"

if [ $? -eq 0 ]; then
    log_success "âœ… ê³ ë„í™”ëœ ì‹œìž¥ ë¶„ì„ ì™„ë£Œ!"
    log_info "ê²°ê³¼ íŒŒì¼: $OUTPUT_DIR/analysis_results_${TIMESTAMP}.json"
    log_info "ë¡œê·¸ íŒŒì¼: $LOG_FILE"
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    if [ -f "$OUTPUT_DIR/analysis_results_${TIMESTAMP}.json" ]; then
        log_info "ðŸ“Š ê²°ê³¼ ìš”ì•½:"
        python3 -c "
import json
with open('$OUTPUT_DIR/analysis_results_${TIMESTAMP}.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
    print(f'í˜„ìž¬ ì²´ì œ: {data.get(\"current_regime\", \"N/A\")}')
    if 'final_confidence' in data:
        conf = data['final_confidence'].get('final_confidence', 0.5)
        print(f'ìµœì¢… ì‹ ë¢°ë„: {conf:.3f}')
    if 'rlmf_analysis' in data:
        rlmf = data['rlmf_analysis']
        if 'statistical_arbitrage' in rlmf:
            sa = rlmf['statistical_arbitrage']
            print(f'Statistical Arbitrage: {sa.get(\"direction\", \"N/A\")}')
"
    fi
else
    log_error "âŒ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨!"
    exit 1
fi

log_info "ðŸŽ‰ ëª¨ë“  ìž‘ì—… ì™„ë£Œ!" 