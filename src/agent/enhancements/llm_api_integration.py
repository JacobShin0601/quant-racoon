#!/usr/bin/env python3
"""
LLM API í†µí•© ì‹œìŠ¤í…œ (LangChain ê¸°ë°˜)

LangChainì„ í™œìš©í•œ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì‹œì¥ ë¶„ì„ ê°•í™” ì‹œìŠ¤í…œ
ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œê³¼ í•˜ì´ë¸Œë¦¬ë“œë¡œ ë™ì‘í•˜ì—¬ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ëª¨ë‘ í™•ë³´
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
import logging
import json
import time
import hashlib
from dataclasses import dataclass
import warnings

# LangChain ê´€ë ¨ import
try:
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import JsonOutputParser
    from pydantic import BaseModel, Field
    from langchain_aws import ChatBedrock
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_community.cache import InMemoryCache
    from langchain.globals import set_llm_cache
    from langchain.schema import HumanMessage, SystemMessage

    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    warnings.warn(f"LangChain not available: {e}. Using rule-based system only.")
    # LangChainì´ ì—†ì„ ë•Œë¥¼ ìœ„í•œ ëŒ€ì²´ í´ë˜ìŠ¤ë“¤
    BaseModel = object
    Field = lambda **kwargs: lambda x: x

from .llm_insights import LLMPrivilegedInformationSystem


@dataclass
class LLMConfig:
    """LLM ì„¤ì • í´ë˜ìŠ¤"""

    provider: str = "bedrock"  # "bedrock", "openai", "anthropic", "hybrid"
    model_name: str = "anthropic.claude-3-sonnet-20240229-v1:0"
    api_key: Optional[str] = None
    region: str = "us-east-1"
    max_tokens: int = 2000
    temperature: float = 0.1
    timeout: int = 30
    retry_attempts: int = 3
    fallback_to_rules: bool = True


class MarketAnalysisOutput(BaseModel):
    """ì‹œì¥ ë¶„ì„ ê²°ê³¼ë¥¼ ìœ„í•œ Pydantic ëª¨ë¸"""

    def __init__(self, **kwargs):
        if LANGCHAIN_AVAILABLE:
            super().__init__(**kwargs)
        else:
            # LangChainì´ ì—†ì„ ë•ŒëŠ” ë‹¨ìˆœí•œ ê°ì²´ë¡œ ë™ì‘
            self.comprehensive_analysis = kwargs.get("comprehensive_analysis", {})
            self.risk_assessment = kwargs.get("risk_assessment", {})
            self.strategic_recommendations = kwargs.get("strategic_recommendations", {})
            self.scenario_analysis = kwargs.get("scenario_analysis", {})
            self.confidence_modifier = kwargs.get("confidence_modifier", 1.0)
            self.key_insights = kwargs.get("key_insights", [])

    if LANGCHAIN_AVAILABLE:
        comprehensive_analysis: Dict[str, Any] = Field(
            description="ì¢…í•©ì ì¸ ì‹œì¥ ë¶„ì„ ê²°ê³¼"
        )
        risk_assessment: Dict[str, Any] = Field(description="ë¦¬ìŠ¤í¬ í‰ê°€ ê²°ê³¼")
        strategic_recommendations: Dict[str, Any] = Field(description="ì „ëµì  ì œì–¸")
        scenario_analysis: Dict[str, Any] = Field(description="ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„")
        confidence_modifier: float = Field(
            description="ì‹ ë¢°ë„ ìˆ˜ì •ì (0.5-1.5)", ge=0.5, le=1.5
        )
        key_insights: List[str] = Field(description="í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ëª©ë¡")


class LLMAPIIntegration:
    """
    LLM API í†µí•© ì‹œìŠ¤í…œ (LangChain ê¸°ë°˜)

    LangChainì„ í™œìš©í•˜ì—¬ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì‹œì¥ ë¶„ì„ ê°•í™” ì‹œìŠ¤í…œ
    ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œê³¼ í•˜ì´ë¸Œë¦¬ë“œë¡œ ë™ì‘
    """

    def __init__(self, config: LLMConfig = None):
        # ë”•ì…”ë„ˆë¦¬ë¡œ ì „ë‹¬ëœ ì„¤ì •ì„ LLMConfig ê°ì²´ë¡œ ë³€í™˜
        if isinstance(config, dict):
            self.config = LLMConfig(**config)
        else:
            self.config = config or LLMConfig()

        self.logger = logging.getLogger(__name__)

        # ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œ (fallbackìš©)
        self.rule_based_system = LLMPrivilegedInformationSystem()

        # LangChain ìºì‹œ ì„¤ì •
        if LANGCHAIN_AVAILABLE:
            set_llm_cache(InMemoryCache())

        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        self.llm_model = self._initialize_llm_model()

        # ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”
        if LANGCHAIN_AVAILABLE:
            self.output_parser = JsonOutputParser(pydantic_object=MarketAnalysisOutput)
        else:
            self.output_parser = None

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.api_call_stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "avg_response_time": 0.0,
        }

    def _initialize_llm_model(self) -> Optional[Any]:
        """LangChain LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        if not LANGCHAIN_AVAILABLE:
            self.logger.warning(
                "LangChain not available. Using rule-based system only."
            )
            return None

        try:
            self.logger.info(
                f"Initializing LangChain LLM with provider: {self.config.provider}"
            )
            self.logger.info(f"Model name: {self.config.model_name}")

            if self.config.provider == "bedrock":
                self.logger.info(
                    f"Creating Bedrock model for region: {self.config.region}"
                )
                model = ChatBedrock(
                    model_id=self.config.model_name,
                    region_name=self.config.region,
                    model_kwargs={
                        "max_tokens": self.config.max_tokens,
                        "temperature": self.config.temperature,
                    },
                )
                self.logger.info("Bedrock model created successfully")
                return model

            elif self.config.provider == "openai":
                if not self.config.api_key:
                    self.logger.warning(
                        "OpenAI API key not provided. Using rule-based system only."
                    )
                    return None

                self.logger.info("Creating OpenAI model")
                model = ChatOpenAI(
                    model=self.config.model_name,
                    openai_api_key=self.config.api_key,
                    max_tokens=self.config.max_tokens,
                    temperature=self.config.temperature,
                    timeout=self.config.timeout,
                )
                self.logger.info("OpenAI model created successfully")
                return model

            elif self.config.provider == "anthropic":
                if not self.config.api_key:
                    self.logger.warning(
                        "Anthropic API key not provided. Using rule-based system only."
                    )
                    return None

                self.logger.info("Creating Anthropic model")
                model = ChatAnthropic(
                    model=self.config.model_name,
                    anthropic_api_key=self.config.api_key,
                    max_tokens=self.config.max_tokens,
                    temperature=self.config.temperature,
                )
                self.logger.info("Anthropic model created successfully")
                return model

            else:
                self.logger.warning(f"Unknown LLM provider: {self.config.provider}")
                return None

        except Exception as e:
            self.logger.error(f"LangChain LLM initialization failed: {e}")
            import traceback

            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            return None

    def get_enhanced_insights(
        self,
        current_regime: str,
        macro_data: Dict[str, pd.DataFrame],
        market_metrics: Dict[str, Any],
        analysis_results: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """
        í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ íšë“ (LangChain LLM + ê·œì¹™ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ)

        Args:
            current_regime: í˜„ì¬ ì‹œì¥ ì²´ì œ
            macro_data: ë§¤í¬ë¡œ ë°ì´í„°
            market_metrics: ì‹œì¥ ë©”íŠ¸ë¦­
            analysis_results: ê¸°ì¡´ ë¶„ì„ ê²°ê³¼
        """
        try:
            # 1. ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (ê¸°ë³¸)
            rule_based_insights = self.rule_based_system.get_privileged_insights(
                current_regime, macro_data, market_metrics
            )

            # 2. LangChain LLM í˜¸ì¶œ (í–¥ìƒëœ ë¶„ì„)
            if self.llm_model and self.config.provider != "rule_only":
                try:
                    llm_insights = self._call_langchain_llm(
                        current_regime, macro_data, market_metrics, analysis_results
                    )

                    # 3. ë‘ ê²°ê³¼ ìœµí•©
                    enhanced_insights = self._combine_insights(
                        rule_based_insights, llm_insights
                    )

                    self.logger.info("LangChain LLM í†µí•© ë¶„ì„ ì™„ë£Œ")
                    return enhanced_insights

                except Exception as e:
                    self.logger.warning(
                        f"LangChain LLM í˜¸ì¶œ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©: {e}"
                    )
                    self.api_call_stats["failed_calls"] += 1

            # LLM API ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©
            return rule_based_insights

        except Exception as e:
            self.logger.error(f"Enhanced insights generation failed: {e}")
            return self._get_fallback_insights(
                current_regime, macro_data, market_metrics
            )

    def _call_langchain_llm(
        self,
        current_regime: str,
        macro_data: Dict[str, pd.DataFrame],
        market_metrics: Dict[str, Any],
        analysis_results: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """LangChain LLM í˜¸ì¶œ"""
        # LangChainì´ ì—†ìœ¼ë©´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ë§Œ ë°˜í™˜
        if not LANGCHAIN_AVAILABLE or not self.llm_model:
            self.logger.warning(
                "LangChain not available, using rule-based analysis only"
            )
            return {}

        start_time = time.time()

        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_langchain_prompt(
                current_regime, macro_data, market_metrics, analysis_results
            )

            self.logger.info(
                f"Calling LangChain LLM with prompt length: {len(prompt)} characters"
            )

            # LangChain ì²´ì¸ ì‹¤í–‰
            chain = prompt | self.llm_model | self.output_parser

            # API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            response = None
            for attempt in range(self.config.retry_attempts):
                try:
                    response = chain.invoke({})
                    if response:
                        break
                except Exception as e:
                    self.logger.warning(
                        f"LangChain LLM call attempt {attempt + 1} failed: {e}"
                    )
                    if attempt < self.config.retry_attempts - 1:
                        time.sleep(2**attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„

            # ì‘ë‹µ ì²˜ë¦¬
            if response:
                # í†µê³„ ì—…ë°ì´íŠ¸
                response_time = time.time() - start_time
                self.api_call_stats["successful_calls"] += 1
                self.api_call_stats["total_calls"] += 1
                self._update_avg_response_time(response_time)

                self.logger.info("LangChain LLM response received successfully")
                return response

            # API í˜¸ì¶œ ì‹¤íŒ¨
            self.api_call_stats["failed_calls"] += 1
            self.api_call_stats["total_calls"] += 1
            raise Exception("LangChain LLM call failed")

        except Exception as e:
            self.logger.error(f"LangChain LLM call failed: {e}")
            import traceback

            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            raise

    def _create_langchain_prompt(
        self,
        current_regime: str,
        macro_data: Dict[str, pd.DataFrame],
        market_metrics: Dict[str, Any],
        analysis_results: Dict[str, Any] = None,
    ) -> ChatPromptTemplate:
        """LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        # LangChainì´ ì—†ìœ¼ë©´ ë”ë¯¸ ê°ì²´ ë°˜í™˜
        if not LANGCHAIN_AVAILABLE:
            return None

        # ì‹œì¥ ìš”ì•½ ìƒì„±
        market_summary = self._create_market_summary(macro_data)
        metrics_summary = self._create_metrics_summary(market_metrics)
        analysis_summary = (
            self._create_analysis_summary(analysis_results)
            if analysis_results
            else "ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
        )

        # LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        template = f"""
ë‹¹ì‹ ì€ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì¢…í•©ì ì¸ ì‹œì¥ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ë©´ì ì´ê³  ì‹¬ì¸µì ì¸ í•´ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

## í˜„ì¬ ì‹œì¥ ìƒí™©
- ê°ì§€ëœ ì‹œì¥ ì²´ì œ: {current_regime}
- VIX ìˆ˜ì¤€: {market_summary.get('vix_level', 'N/A')}
- 10ë…„ êµ­ì±„ ìˆ˜ìµë¥ : {market_summary.get('tnx_level', 'N/A')}
- TIPS ìˆ˜ì¤€: {market_summary.get('tips_level', 'N/A')}
- ë‹¬ëŸ¬ ì¸ë±ìŠ¤: {market_summary.get('dxy_level', 'N/A')}

## ì‹œì¥ ë©”íŠ¸ë¦­
{metrics_summary}

## ê¸°ì¡´ ë¶„ì„ ê²°ê³¼
{analysis_summary}

## ì¢…í•© ë¶„ì„ ìš”ì²­ì‚¬í•­
ë‹¤ìŒ ê´€ì ì—ì„œ ì‹¬ì¸µì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:

1. **ì§€í‘œ í•´ì„**: ê° ê¸°ìˆ ì /ë§¤í¬ë¡œ ì§€í‘œì˜ ì˜ë¯¸ì™€ í˜„ì¬ ì‹œì¥ì—ì„œì˜ ì¤‘ìš”ì„±
2. **ë‹¤ë©´ì  ë¶„ì„**: ê¸°ìˆ ì , ë§¤í¬ë¡œ, ì„¹í„° ë¶„ì„ ê²°ê³¼ì˜ ì¼ê´€ì„±ê³¼ ìƒì¶©ì 
3. **ì‹œì¥ ì—­í•™**: í˜„ì¬ ì‹œì¥ì˜ ì£¼ìš” ë™ì¸ê³¼ ë³€ë™ì„± ì›ì¸
4. **ë¦¬ìŠ¤í¬ í‰ê°€**: ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° ê´€ì ì—ì„œì˜ ìœ„í—˜ ìš”ì¸
5. **ì „ëµì  ì œì–¸**: í˜„ì¬ ìƒí™©ì— ìµœì í™”ëœ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ë°©ì•ˆ
6. **ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„**: ë‹¤ì–‘í•œ ì‹œì¥ ì‹œë‚˜ë¦¬ì˜¤ë³„ ëŒ€ì‘ ë°©ì•ˆ

ë‹¤ìŒ JSON í˜•íƒœë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{{
    "comprehensive_analysis": {{
        "market_dynamics": {{
            "primary_drivers": ["ì£¼ìš” ë™ì¸1", "ì£¼ìš” ë™ì¸2"],
            "volatility_factors": ["ë³€ë™ì„± ìš”ì¸1", "ë³€ë™ì„± ìš”ì¸2"],
            "trend_strength": "strong/moderate/weak",
            "momentum_quality": "high/medium/low"
        }},
        "indicator_interpretation": {{
            "technical_indicators": {{
                "rsi_interpretation": "RSI í•´ì„",
                "macd_interpretation": "MACD í•´ì„",
                "volume_analysis": "ê±°ë˜ëŸ‰ ë¶„ì„"
            }},
            "macro_indicators": {{
                "yield_curve_analysis": "ìˆ˜ìµë¥  ê³¡ì„  ë¶„ì„",
                "inflation_outlook": "ì¸í”Œë ˆì´ì…˜ ì „ë§",
                "growth_prospects": "ì„±ì¥ ì „ë§"
            }}
        }},
        "consistency_analysis": {{
            "technical_macro_alignment": 0.0-1.0,
            "sector_macro_alignment": 0.0-1.0,
            "conflicting_signals": ["ìƒì¶© ì‹ í˜¸1", "ìƒì¶© ì‹ í˜¸2"],
            "supporting_signals": ["ì§€ì§€ ì‹ í˜¸1", "ì§€ì§€ ì‹ í˜¸2"]
        }}
    }},
    "risk_assessment": {{
        "short_term_risks": ["ë‹¨ê¸° ìœ„í—˜1", "ë‹¨ê¸° ìœ„í—˜2"],
        "medium_term_risks": ["ì¤‘ê¸° ìœ„í—˜1", "ì¤‘ê¸° ìœ„í—˜2"],
        "long_term_risks": ["ì¥ê¸° ìœ„í—˜1", "ì¥ê¸° ìœ„í—˜2"],
        "risk_mitigation": {{
            "portfolio_hedging": ["í—¤ì§• ì „ëµ1", "í—¤ì§• ì „ëµ2"],
            "position_sizing": "conservative/moderate/aggressive",
            "stop_loss_levels": "ì ì • ì†ì ˆ ìˆ˜ì¤€"
        }}
    }},
    "strategic_recommendations": {{
        "portfolio_allocation": {{
            "equity_allocation": "0-100%",
            "bond_allocation": "0-100%",
            "cash_allocation": "0-100%",
            "alternative_allocation": "0-100%"
        }},
        "sector_focus": {{
            "overweight_sectors": ["ê³¼ì¤‘ ë°°ì¹˜ ì„¹í„°1", "ê³¼ì¤‘ ë°°ì¹˜ ì„¹í„°2"],
            "underweight_sectors": ["ê³¼ì†Œ ë°°ì¹˜ ì„¹í„°1", "ê³¼ì†Œ ë°°ì¹˜ ì„¹í„°2"],
            "avoid_sectors": ["íšŒí”¼ ì„¹í„°1", "íšŒí”¼ ì„¹í„°2"]
        }},
        "trading_strategy": {{
            "entry_timing": "immediate/gradual/wait",
            "holding_period": "short/medium/long",
            "exit_strategy": "exit ì „ëµ"
        }}
    }},
    "scenario_analysis": {{
        "bull_scenario": {{
            "probability": 0.0-1.0,
            "triggers": ["ìƒìŠ¹ íŠ¸ë¦¬ê±°1", "ìƒìŠ¹ íŠ¸ë¦¬ê±°2"],
            "actions": ["ìƒìŠ¹ ì‹œ í–‰ë™1", "ìƒìŠ¹ ì‹œ í–‰ë™2"]
        }},
        "bear_scenario": {{
            "probability": 0.0-1.0,
            "triggers": ["í•˜ë½ íŠ¸ë¦¬ê±°1", "í•˜ë½ íŠ¸ë¦¬ê±°2"],
            "actions": ["í•˜ë½ ì‹œ í–‰ë™1", "í•˜ë½ ì‹œ í–‰ë™2"]
        }},
        "sideways_scenario": {{
            "probability": 0.0-1.0,
            "triggers": ["íš¡ë³´ íŠ¸ë¦¬ê±°1", "íš¡ë³´ íŠ¸ë¦¬ê±°2"],
            "actions": ["íš¡ë³´ ì‹œ í–‰ë™1", "íš¡ë³´ ì‹œ í–‰ë™2"]
        }}
    }},
    "confidence_modifier": 0.5-1.5,
    "key_insights": ["í•µì‹¬ ì¸ì‚¬ì´íŠ¸1", "í•µì‹¬ ì¸ì‚¬ì´íŠ¸2", "í•µì‹¬ ì¸ì‚¬ì´íŠ¸3"]
}}
"""

        return ChatPromptTemplate.from_template(template)

    def _create_market_summary(
        self, macro_data: Dict[str, pd.DataFrame]
    ) -> Dict[str, float]:
        """ì‹œì¥ ìš”ì•½ ìƒì„±"""
        summary = {}

        try:
            # VIX ë¶„ì„
            if "VIX" in macro_data:
                vix_data = macro_data["VIX"]
                if not vix_data.empty:
                    latest_vix = vix_data.iloc[-1]["Close"]
                    summary["vix_level"] = latest_vix

                    if latest_vix < 15:
                        summary["vix_level"] = "ë‚®ìŒ (ì•ˆì •)"
                    elif latest_vix < 25:
                        summary["vix_level"] = "ë³´í†µ"
                    else:
                        summary["vix_level"] = "ë†’ìŒ (ë³€ë™ì„±)"

            # TNX (10ë…„ êµ­ì±„ ìˆ˜ìµë¥ ) ë¶„ì„
            if "TNX" in macro_data:
                tnx_data = macro_data["TNX"]
                if not tnx_data.empty:
                    latest_tnx = tnx_data.iloc[-1]["Close"]
                    summary["tnx_level"] = latest_tnx

                    if latest_tnx < 2.0:
                        summary["tnx_level"] = "ë‚®ìŒ (ì„±ì¥ ìš°ë ¤)"
                    elif latest_tnx < 4.0:
                        summary["tnx_level"] = "ë³´í†µ"
                    else:
                        summary["tnx_level"] = "ë†’ìŒ (ì¸í”Œë ˆì´ì…˜ ìš°ë ¤)"

            # TIPS ë¶„ì„
            if "TIPS" in macro_data:
                tips_data = macro_data["TIPS"]
                if not tips_data.empty:
                    latest_tips = tips_data.iloc[-1]["Close"]
                    summary["tips_level"] = latest_tips

                    if latest_tips < 0:
                        summary["tips_level"] = "ìŒìˆ˜ (ë””í”Œë ˆì´ì…˜ ìš°ë ¤)"
                    else:
                        summary["tips_level"] = "ì–‘ìˆ˜ (ì¸í”Œë ˆì´ì…˜ ìš°ë ¤)"

            # DXY (ë‹¬ëŸ¬ ì¸ë±ìŠ¤) ë¶„ì„
            if "DXY" in macro_data:
                dxy_data = macro_data["DXY"]
                if not dxy_data.empty:
                    latest_dxy = dxy_data.iloc[-1]["Close"]
                    summary["dxy_level"] = latest_dxy

                    if latest_dxy < 95:
                        summary["dxy_level"] = "ì•½ì„¸"
                    elif latest_dxy < 105:
                        summary["dxy_level"] = "ë³´í†µ"
                    else:
                        summary["dxy_level"] = "ê°•ì„¸"

        except Exception as e:
            self.logger.warning(f"Market summary creation failed: {e}")

        return summary

    def _create_metrics_summary(self, market_metrics: Dict[str, Any]) -> str:
        """ë©”íŠ¸ë¦­ ìš”ì•½ ìƒì„±"""
        try:
            summary_parts = []

            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            if "probabilities" in market_metrics:
                probs = market_metrics["probabilities"]
                summary_parts.append(
                    f"ì‹œì¥ ì²´ì œ í™•ë¥ : ìƒìŠ¹ {probs.get('UP', 0):.1%}, í•˜ë½ {probs.get('DOWN', 0):.1%}, íš¡ë³´ {probs.get('SIDEWAYS', 0):.1%}"
                )

            # í†µê³„ì  ì°¨ìµê±°ë˜ ì‹ í˜¸
            if "stat_arb_signals" in market_metrics:
                signals = market_metrics["stat_arb_signals"]
                summary_parts.append(f"í†µê³„ì  ì°¨ìµê±°ë˜ ì‹ í˜¸: {signals}")

            # ê¸°íƒ€ ë©”íŠ¸ë¦­ë“¤
            for key, value in market_metrics.items():
                if key not in ["probabilities", "stat_arb_signals"]:
                    summary_parts.append(f"{key}: {value}")

            return "\n".join(summary_parts) if summary_parts else "ë©”íŠ¸ë¦­ ë°ì´í„° ì—†ìŒ"

        except Exception as e:
            self.logger.warning(f"Metrics summary creation failed: {e}")
            return "ë©”íŠ¸ë¦­ ìš”ì•½ ìƒì„± ì‹¤íŒ¨"

    def _create_analysis_summary(self, analysis_results: Dict[str, Any]) -> str:
        """ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        try:
            if not analysis_results:
                return "ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì—†ìŒ"

            summary_parts = []

            # í˜„ì¬ ì²´ì œ
            if "current_regime" in analysis_results:
                summary_parts.append(f"í˜„ì¬ ì²´ì œ: {analysis_results['current_regime']}")

            # ì‹ ë¢°ë„
            if "confidence" in analysis_results:
                summary_parts.append(f"ì‹ ë¢°ë„: {analysis_results['confidence']:.2f}")

            # í™•ë¥ 
            if "probabilities" in analysis_results:
                probs = analysis_results["probabilities"]
                summary_parts.append(f"ì²´ì œ í™•ë¥ : {probs}")

            # ìµœì í™” ì„±ëŠ¥
            if "optimization_performance" in analysis_results:
                perf = analysis_results["optimization_performance"]
                summary_parts.append(f"ìµœì í™” ì„±ëŠ¥: {perf}")

            # ê²€ì¦ ê²°ê³¼
            if "validation_results" in analysis_results:
                validation = analysis_results["validation_results"]
                summary_parts.append(f"ê²€ì¦ ê²°ê³¼: {validation}")

            return "\n".join(summary_parts) if summary_parts else "ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì—†ìŒ"

        except Exception as e:
            self.logger.warning(f"Analysis summary creation failed: {e}")
            return "ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„± ì‹¤íŒ¨"

    def _combine_insights(
        self, rule_insights: Dict[str, Any], llm_insights: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê·œì¹™ ê¸°ë°˜ê³¼ LLM ì¸ì‚¬ì´íŠ¸ ìœµí•©"""
        try:
            combined = {
                "analysis_timestamp": datetime.now().isoformat(),
                "data_source": "hybrid_llm_rules",
                "rule_based_insights": rule_insights,
                "llm_enhanced_insights": llm_insights,
            }

            # LLM ì¸ì‚¬ì´íŠ¸ì—ì„œ ì£¼ìš” ì„¹ì…˜ë“¤ ì¶”ì¶œ
            if "comprehensive_analysis" in llm_insights:
                combined["market_dynamics"] = llm_insights[
                    "comprehensive_analysis"
                ].get("market_dynamics", {})
                combined["indicator_interpretation"] = llm_insights[
                    "comprehensive_analysis"
                ].get("indicator_interpretation", {})
                combined["consistency_analysis"] = llm_insights[
                    "comprehensive_analysis"
                ].get("consistency_analysis", {})

            if "risk_assessment" in llm_insights:
                combined["risk_assessment"] = llm_insights["risk_assessment"]

            if "strategic_recommendations" in llm_insights:
                combined["strategic_recommendations"] = llm_insights[
                    "strategic_recommendations"
                ]

            if "scenario_analysis" in llm_insights:
                combined["scenario_analysis"] = llm_insights["scenario_analysis"]

            if "key_insights" in llm_insights:
                combined["key_insights"] = llm_insights["key_insights"]

            # ì‹ ë¢°ë„ ìˆ˜ì •ì ì ìš©
            confidence_modifier = llm_insights.get("confidence_modifier", 1.0)
            if "confidence" in rule_insights:
                original_confidence = rule_insights["confidence"]
                combined["adjusted_confidence"] = min(
                    1.0, original_confidence * confidence_modifier
                )

            # API í†µê³„ ì¶”ê°€
            combined["api_stats"] = self.get_api_stats()

            return combined

        except Exception as e:
            self.logger.error(f"Insights combination failed: {e}")
            return rule_insights

    def _update_avg_response_time(self, new_response_time: float):
        """í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        total_calls = self.api_call_stats["successful_calls"]
        current_avg = self.api_call_stats["avg_response_time"]

        if total_calls == 1:
            self.api_call_stats["avg_response_time"] = new_response_time
        else:
            self.api_call_stats["avg_response_time"] = (
                current_avg * (total_calls - 1) + new_response_time
            ) / total_calls

    def _get_fallback_insights(
        self,
        current_regime: str,
        macro_data: Dict[str, pd.DataFrame],
        market_metrics: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Fallback ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        return self.rule_based_system.get_privileged_insights(
            current_regime, macro_data, market_metrics
        )

    def get_api_stats(self) -> Dict[str, Any]:
        """API í†µê³„ ë°˜í™˜"""
        total_calls = self.api_call_stats["total_calls"]
        success_rate = (
            self.api_call_stats["successful_calls"] / total_calls * 100
            if total_calls > 0
            else 0
        )

        return {
            **self.api_call_stats,
            "success_rate": success_rate,
            "provider": self.config.provider,
            "model": self.config.model_name,
        }

    def clear_cache(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        if LANGCHAIN_AVAILABLE:
            # LangChain ìºì‹œëŠ” ìë™ìœ¼ë¡œ ê´€ë¦¬ë¨
            pass

    def update_config(self, new_config: LLMConfig):
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.config = new_config
        self.llm_model = self._initialize_llm_model()


def test_langchain_llm_integration():
    """LangChain LLM í†µí•© í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª LangChain LLM í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    config = LLMConfig(
        provider="bedrock",
        model_name="anthropic.claude-3-sonnet-20240229-v1:0",
        max_tokens=2000,
        temperature=0.1,
    )

    try:
        # LLM API í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        llm_system = LLMAPIIntegration(config)

        print(f"âœ… LangChain LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„±ê³µ")
        print(f"ğŸ¤– Provider: {config.provider}")
        print(f"ğŸ“Š Model: {config.model_name}")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_macro_data = {
            "VIX": pd.DataFrame({"Close": [20.5]}, index=[pd.Timestamp.now()]),
            "TNX": pd.DataFrame({"Close": [3.2]}, index=[pd.Timestamp.now()]),
        }

        test_market_metrics = {
            "probabilities": {"UP": 0.6, "DOWN": 0.2, "SIDEWAYS": 0.2},
            "stat_arb_signals": "neutral",
        }

        test_analysis_results = {
            "current_regime": "TRENDING_UP",
            "confidence": 0.75,
            "probabilities": {"UP": 0.6, "DOWN": 0.2, "SIDEWAYS": 0.2},
        }

        # í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ íšë“ í…ŒìŠ¤íŠ¸
        print("\nğŸš€ í–¥ìƒëœ ì¸ì‚¬ì´íŠ¸ íšë“ í…ŒìŠ¤íŠ¸...")
        insights = llm_system.get_enhanced_insights(
            "TRENDING_UP", test_macro_data, test_market_metrics, test_analysis_results
        )

        print(f"âœ… ì¸ì‚¬ì´íŠ¸ íšë“ ì„±ê³µ")
        print(f"ğŸ“Š API í†µê³„: {llm_system.get_api_stats()}")

        # ê²°ê³¼ ì¶œë ¥
        if "llm_enhanced_insights" in insights:
            llm_result = insights["llm_enhanced_insights"]
            print(f"\nğŸ¤– LLM ì¢…í•© ë¶„ì„ ê²°ê³¼:")
            print(
                f"   - ì‹œì¥ ì—­í•™: {llm_result.get('comprehensive_analysis', {}).get('market_dynamics', {})}"
            )
            print(f"   - í•µì‹¬ ì¸ì‚¬ì´íŠ¸: {llm_result.get('key_insights', [])}")
            print(f"   - ì‹ ë¢°ë„ ìˆ˜ì •ì: {llm_result.get('confidence_modifier', 1.0)}")

        return True

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_langchain_llm_integration()
