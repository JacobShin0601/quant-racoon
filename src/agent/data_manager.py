#!/usr/bin/env python3
"""
ì¤‘ì•™í™”ëœ ë°ì´í„° ê´€ë¦¬ì
ëª¨ë“  ë°ì´í„° ë‹¤ìš´ë¡œë“œì™€ ìºì‹œ ê´€ë¦¬ë¥¼ ë‹´ë‹¹
"""

import os
import sys
import json
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).resolve().parent.parent))

from actions.y_finance import YahooFinanceDataCollector
from actions.global_macro import GlobalMacroDataCollector
from agent.helper import Logger, load_config


class DataCacheManager:
    """ë°ì´í„° ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, cache_dir: str = "data/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_info_file = self.cache_dir / "cache_info.json"
        self.cache_info = self._load_cache_info()
        
    def _load_cache_info(self) -> Dict:
        """ìºì‹œ ì •ë³´ ë¡œë“œ"""
        if self.cache_info_file.exists():
            with open(self.cache_info_file, 'r') as f:
                return json.load(f)
        return {}
    
    def _save_cache_info(self):
        """ìºì‹œ ì •ë³´ ì €ì¥"""
        with open(self.cache_info_file, 'w') as f:
            json.dump(self.cache_info, f, indent=2)
    
    def get_cache_key(self, data_type: str, params: Dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # íŒŒë¼ë¯¸í„°ë¥¼ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í‚¤ ìƒì„±
        sorted_params = json.dumps(params, sort_keys=True)
        key_string = f"{data_type}:{sorted_params}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def is_cache_valid(self, cache_key: str, max_age_days: int = 1) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        if cache_key not in self.cache_info:
            return False
        
        cache_time = datetime.fromisoformat(self.cache_info[cache_key]['timestamp'])
        age = datetime.now() - cache_time
        
        return age.days < max_age_days
    
    def get_cache_path(self, cache_key: str) -> Optional[Path]:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        if cache_key in self.cache_info:
            path = Path(self.cache_info[cache_key]['path'])
            if path.exists():
                return path
        return None
    
    def save_cache(self, cache_key: str, file_path: str, metadata: Dict = None):
        """ìºì‹œ ì •ë³´ ì €ì¥"""
        self.cache_info[cache_key] = {
            'path': str(file_path),
            'timestamp': datetime.now().isoformat(),
            'metadata': metadata or {}
        }
        self._save_cache_info()


class UnifiedDataManager:
    """í†µí•© ë°ì´í„° ê´€ë¦¬ì"""
    
    def __init__(
        self,
        config_path: str = "config/config_default.json",
        time_horizon: str = "swing",
        use_cached_data: bool = False,
        cache_days: int = 1,
        uuid: Optional[str] = None
    ):
        self.config_path = config_path
        self.time_horizon = time_horizon
        self.use_cached_data = use_cached_data
        self.cache_days = cache_days
        self.uuid = uuid
        
        # ì„¤ì • ë¡œë“œ
        self.config = load_config(config_path)
        
        # ë¡œê±° ì„¤ì •
        self.logger = Logger()
        log_dir = f"log/{time_horizon}"
        self.logger.set_log_dir(log_dir)
        self.logger.setup_logger(strategy="data_manager", mode="download", uuid=uuid)
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
        self.base_data_dir = Path("data")
        self.time_horizon_dir = self.base_data_dir / time_horizon
        self.macro_dir = self.base_data_dir / "macro"
        self.trader_dir = self.base_data_dir / "trader"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in [self.time_horizon_dir, self.macro_dir, self.trader_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ìºì‹œ ê´€ë¦¬ì
        self.cache_manager = DataCacheManager()
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸°
        self.yahoo_collector = YahooFinanceDataCollector()
        self.macro_collector = GlobalMacroDataCollector()
        
    def download_stock_data(
        self,
        symbols: List[str],
        interval: str = "1d",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        lookback_days: int = 60,
        target_dir: Optional[str] = None
    ) -> Dict[str, str]:
        """ì£¼ì‹ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)"""
        
        if target_dir is None:
            target_dir = str(self.time_horizon_dir)
        
        downloaded_files = {}
        
        for symbol in symbols:
            try:
                # ìºì‹œ í‚¤ ìƒì„±
                cache_params = {
                    'symbol': symbol,
                    'interval': interval,
                    'start_date': start_date,
                    'end_date': end_date,
                    'lookback_days': lookback_days
                }
                cache_key = self.cache_manager.get_cache_key('stock', cache_params)
                
                # ìºì‹œ í™•ì¸
                if self.use_cached_data and self.cache_manager.is_cache_valid(cache_key, self.cache_days):
                    cached_path = self.cache_manager.get_cache_path(cache_key)
                    if cached_path:
                        # ìºì‹œ ì‚¬ìš© - ë¡œê·¸ ìƒëµ
                        downloaded_files[symbol] = str(cached_path)
                        continue
                
                # ìƒˆë¡œìš´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
                # ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘
                
                # ì¢…ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                info = self.yahoo_collector.get_stock_info(symbol)
                # ì¢…ëª© ì •ë³´ í™•ì¸ë¨
                
                # ë°ì´í„° ìˆ˜ì§‘
                df = self.yahoo_collector.get_candle_data(
                    symbol=symbol,
                    interval=interval,
                    start_date=start_date,
                    end_date=end_date,
                    days_back=lookback_days
                )
                
                if df is not None and not df.empty:
                    # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                    from actions.calculate_index import TechnicalIndicators, StrategyParams
                    params = StrategyParams()
                    df_with_indicators = TechnicalIndicators.calculate_all_indicators(df, params)
                    
                    # datetime ì»¬ëŸ¼ ë³´ì¥
                    if 'datetime' not in df_with_indicators.columns:
                        df_with_indicators = df_with_indicators.reset_index()
                    
                    # íŒŒì¼ ì €ì¥
                    filepath = self.yahoo_collector.save_to_csv(
                        df=df_with_indicators,
                        symbol=symbol,
                        interval=interval,
                        start_date=start_date or "auto",
                        end_date=end_date or "auto",
                        output_dir=target_dir,
                        uuid=self.uuid
                    )
                    
                    # ì €ì¥ ì™„ë£Œ
                    downloaded_files[symbol] = filepath
                    
                    # ìºì‹œ ì •ë³´ ì €ì¥
                    self.cache_manager.save_cache(cache_key, filepath, {
                        'symbol': symbol,
                        'rows': len(df_with_indicators),
                        'columns': len(df_with_indicators.columns)
                    })
                    
                else:
                    self.logger.log_info_error(f"  âŒ {symbol} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                    
            except Exception as e:
                self.logger.log_info_error(f"  âŒ {symbol} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return downloaded_files
    
    def download_macro_data(
        self,
        symbols: Optional[List[str]] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        lookback_days: int = 252
    ) -> Dict[str, str]:
        """ë§¤í¬ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)"""
        
        # ê¸°ë³¸ ë§¤í¬ë¡œ ì‹¬ë³¼
        if symbols is None:
            symbols = ['SPY', 'VIX', 'DXY', '^TNX', '^TYX', 'GLD', 'BTC-USD']
        
        downloaded_files = {}
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_params = {
            'symbols': sorted(symbols),
            'start_date': start_date,
            'end_date': end_date,
            'lookback_days': lookback_days
        }
        cache_key = self.cache_manager.get_cache_key('macro', cache_params)
        
        # ìºì‹œ í™•ì¸
        if self.use_cached_data and self.cache_manager.is_cache_valid(cache_key, self.cache_days):
            # ë§¤í¬ë¡œ ë°ì´í„°ëŠ” ì—¬ëŸ¬ íŒŒì¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ ë””ë ‰í† ë¦¬ í™•ì¸
            macro_files = list(self.macro_dir.glob("*.csv"))
            if macro_files:
                # ë§¤í¬ë¡œ ë°ì´í„° ìºì‹œ ì‚¬ìš© (ë¡œê·¸ ìƒëµ)
                for file in macro_files:
                    symbol = file.stem.split('_')[0]
                    downloaded_files[symbol] = str(file)
                return downloaded_files
        
        # ìƒˆë¡œìš´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        self.logger.log_info("ğŸ“ˆ ë§¤í¬ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ...")
        
        # global_macro.pyì˜ collect ëª¨ë“œ ì‹¤í–‰
        import subprocess
        result = subprocess.run(
            [sys.executable, "src/actions/global_macro.py", "--mode", "collect"],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            # ë§¤í¬ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í™•ì¸
            macro_files = list(self.macro_dir.glob("*.csv"))
            for file in macro_files:
                symbol = file.stem.split('_')[0]
                downloaded_files[symbol] = str(file)
            
            # ìºì‹œ ì •ë³´ ì €ì¥
            self.cache_manager.save_cache(cache_key, str(self.macro_dir), {
                'files': len(macro_files),
                'symbols': symbols
            })
        else:
            self.logger.log_info_error(f"âŒ ë§¤í¬ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
        
        return downloaded_files
    
    def ensure_data_available(
        self,
        data_type: str,
        symbols: List[str],
        **kwargs
    ) -> bool:
        """ë°ì´í„° ê°€ìš©ì„± í™•ì¸ ë° í•„ìš”ì‹œ ë‹¤ìš´ë¡œë“œ"""
        
        if data_type == "stock":
            target_dir = kwargs.get('target_dir', str(self.time_horizon_dir))
            
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ í™•ì¸ (ìºì‹œ ë§Œë£Œ ê²€ì‚¬ í¬í•¨)
            existing_files = []
            missing_symbols = []
            
            for symbol in symbols:
                pattern = f"{symbol}_*.csv"
                files = list(Path(target_dir).glob(pattern))
                
                if files and self.use_cached_data:
                    # ê°€ì¥ ìµœê·¼ íŒŒì¼ í™•ì¸
                    latest_file = max(files, key=lambda x: x.stat().st_mtime)
                    file_age_days = (datetime.now().timestamp() - latest_file.stat().st_mtime) / (24 * 3600)
                    
                    if file_age_days <= self.cache_days:
                        existing_files.append(symbol)
                        # ìºì‹œ ì‚¬ìš© (ë¡œê·¸ ìƒëµ)
                    else:
                        missing_symbols.append(symbol)
                        # ìºì‹œ ë§Œë£Œ
                else:
                    missing_symbols.append(symbol)
            
            if existing_files:
                # ê¸°ì¡´ ë°ì´í„° ì‚¬ìš© (ë¡œê·¸ ìƒëµ)
                pass
            
            if missing_symbols:
                self.logger.log_info(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ: {', '.join(missing_symbols)}")
                downloaded = self.download_stock_data(
                    missing_symbols,
                    interval=kwargs.get('interval', '1d'),
                    start_date=kwargs.get('start_date'),
                    end_date=kwargs.get('end_date'),
                    lookback_days=kwargs.get('lookback_days', 60),
                    target_dir=target_dir
                )
                return len(downloaded) == len(missing_symbols)
            
            return True
            
        elif data_type == "macro":
            # ë§¤í¬ë¡œ ë°ì´í„° í™•ì¸
            macro_files = list(self.macro_dir.glob("*.csv"))
            
            if macro_files and self.use_cached_data:
                # ê¸°ì¡´ ë§¤í¬ë¡œ ë°ì´í„° ì‚¬ìš© (ë¡œê·¸ ìƒëµ)
                return True
            else:
                downloaded = self.download_macro_data(symbols=symbols, **kwargs)
                return len(downloaded) > 0
        
        return False
    
    def get_data_summary(self) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„° ìƒíƒœ ìš”ì•½"""
        summary = {
            'time_horizon': self.time_horizon,
            'use_cached_data': self.use_cached_data,
            'directories': {
                'time_horizon': str(self.time_horizon_dir),
                'macro': str(self.macro_dir),
                'trader': str(self.trader_dir)
            },
            'files': {}
        }
        
        # ê° ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ìˆ˜ ê³„ì‚°
        for name, dir_path in summary['directories'].items():
            csv_files = list(Path(dir_path).glob("*.csv"))
            summary['files'][name] = {
                'count': len(csv_files),
                'size_mb': sum(f.stat().st_size for f in csv_files) / (1024 * 1024)
            }
        
        return summary


def main():
    """í…ŒìŠ¤íŠ¸ ë° ë…ë¦½ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="í†µí•© ë°ì´í„° ê´€ë¦¬ì")
    parser.add_argument("--config", default="config/config_default.json", help="ì„¤ì • íŒŒì¼")
    parser.add_argument("--time-horizon", default="swing", help="ì‹œê°„ ì§€í‰")
    parser.add_argument("--use-cached-data", action="store_true", help="ìºì‹œ ë°ì´í„° ì‚¬ìš©")
    parser.add_argument("--cache-days", type=int, default=1, help="ìºì‹œ ìœ íš¨ ê¸°ê°„(ì¼)")
    parser.add_argument("--symbols", nargs="+", help="ë‹¤ìš´ë¡œë“œí•  ì‹¬ë³¼ë“¤")
    parser.add_argument("--data-type", choices=["stock", "macro", "all"], default="all", help="ë°ì´í„° ìœ í˜•")
    parser.add_argument("--lookback-days", type=int, default=700, help="ê³¼ê±° ë°ì´í„° ì¼ìˆ˜")
    
    args = parser.parse_args()
    
    # ë°ì´í„° ê´€ë¦¬ì ì´ˆê¸°í™”
    manager = UnifiedDataManager(
        config_path=args.config,
        time_horizon=args.time_horizon,
        use_cached_data=args.use_cached_data,
        cache_days=args.cache_days
    )
    
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    if args.data_type in ["stock", "all"] and args.symbols:
        manager.logger.log_info("\nğŸ“Š ì£¼ì‹ ë°ì´í„° ë‹¤ìš´ë¡œë“œ...")
        manager.ensure_data_available("stock", args.symbols, lookback_days=args.lookback_days)
    
    if args.data_type in ["macro", "all"]:
        manager.logger.log_info("\nğŸ“ˆ ë§¤í¬ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ...")
        manager.ensure_data_available("macro", [])
    
    # ìš”ì•½ ì¶œë ¥
    manager.logger.log_info("\nğŸ“‹ ë°ì´í„° ìƒíƒœ ìš”ì•½:")
    summary = manager.get_data_summary()
    manager.logger.log_info(json.dumps(summary, indent=2))


if __name__ == "__main__":
    main()