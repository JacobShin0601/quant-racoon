#!/usr/bin/env python3
"""
í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ
- QuantAnalyst: ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ë¶„ì„
- FundamentalAnalyst: ì¬ë¬´ì§€í‘œ ê¸°ë°˜ ë¶„ì„
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import argparse
import json
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from actions.correlation import CorrelationAnalyzer
from actions.linear_regression import LinearRegressionAnalyzer
from actions.lasso_regression import LassoRegressionAnalyzer
from actions.random_forest import RandomForestAnalyzer
from actions.multi_layer_perceptron import MLPAnalyzer
from actions.bayesian_distribution import BayesianDistributionAnalyzer
from actions.financial_analysis import FinancialAnalyzer
from agent.helper import (
    Logger,
    load_config,
    load_and_preprocess_data,
    save_analysis_results,
    create_analysis_folder_structure,
    DEFAULT_CONFIG_PATH,
    DEFAULT_DATA_DIR,
)


class QuantAnalyst:
    """ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ì •ëŸ‰ ë¶„ì„ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        data_dir: str = DEFAULT_DATA_DIR,
        config_path: str = DEFAULT_CONFIG_PATH,
        return_type: str = "percentage",  # "percentage" or "log"
        top_features: int = 10,
        analysis_dir: str = "analysis",  # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    ):
        self.data_dir = data_dir
        self.config = load_config(config_path)
        self.return_type = return_type
        self.top_features = top_features
        self.analysis_dir = analysis_dir
        self.logger = Logger()
        self.analysis_start_time = datetime.now()
        self.execution_uuid = None  # UUID ì´ˆê¸°í™”

        # ë¶„ì„ í´ë” êµ¬ì¡° ìƒì„±
        create_analysis_folder_structure(analysis_dir)

        # ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”
        self.correlation_analyzer = CorrelationAnalyzer()
        self.linear_regression_analyzer = LinearRegressionAnalyzer()
        self.lasso_regression_analyzer = LassoRegressionAnalyzer()
        self.random_forest_analyzer = RandomForestAnalyzer()
        self.mlp_analyzer = MLPAnalyzer()
        self.bayesian_analyzer = BayesianDistributionAnalyzer()

        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = {}
        self.prepared_data = {}

    def prepare_data(
        self, data_dict: Dict[str, pd.DataFrame]
    ) -> Dict[str, pd.DataFrame]:
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ìˆ˜ìµë¥  ê³„ì‚° (ê¸°ìˆ ì  ì§€í‘œë§Œ ì‚¬ìš©)"""
        self.logger.log_info("ğŸ“Š ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ìˆ˜ìµë¥  ê³„ì‚° ì¤‘...")

        # ê¸°ìˆ ì  ì§€í‘œ ê´€ë ¨ ì»¬ëŸ¼ë“¤ (ì¬ë¬´ì§€í‘œ ì œì™¸)
        technical_columns = {
            "datetime", "date", "time", "timestamp", "open", "high", "low", "close", "volume",
            "sma_5", "sma_10", "sma_20", "sma_50", "sma_200",
            "ema_5", "ema_10", "ema_20", "ema_50", "ema_200",
            "rsi", "macd", "macd_signal", "macd_histogram",
            "bb_upper", "bb_middle", "bb_lower", "bb_width", "bb_position",
            "stoch_k", "stoch_d", "stoch_slow_k", "stoch_slow_d",
            "atr", "adx", "cci", "williams_r", "mfi", "obv",
        }

        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ (ì¬ë¬´ì§€í‘œ ë° ê¸°íƒ€)
        excluded_columns = {
            "datetime", "date", "time", "timestamp", "open", "high", "low", "close", "volume",
        }

        prepared_data = {}

        for symbol, data in data_dict.items():
            # ë°ì´í„° ë³µì‚¬
            df = data.copy()

            # ê¸°ì¡´ returns ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if "returns" in df.columns:
                # ê¸°ì¡´ returns ì»¬ëŸ¼ì„ targetìœ¼ë¡œ ì‚¬ìš©
                df["return"] = df["returns"]
                self.logger.log_info(f"  {symbol}: ê¸°ì¡´ returns ì»¬ëŸ¼ ì‚¬ìš©")
            else:
                # ìˆ˜ìµë¥  ê³„ì‚° (target ë³€ìˆ˜)
                if self.return_type == "log":
                    df["return"] = np.log(df["close"] / df["close"].shift(1))
                else:  # percentage
                    df["return"] = df["close"].pct_change() * 100
                self.logger.log_info(f"  {symbol}: ìƒˆë¡œìš´ ìˆ˜ìµë¥  ê³„ì‚°")

            # NaN ì œê±°
            df = df.dropna()

            # ê¸°ìˆ ì  ì§€í‘œ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ (ì¬ë¬´ì§€í‘œ ì œì™¸)
            feature_columns = []
            for col in df.columns:
                if col not in excluded_columns and col != "return" and col != "returns":
                    # ì¬ë¬´ì§€í‘œê°€ ì•„ë‹Œ ì»¬ëŸ¼ë§Œ ì„ íƒ (pe_ratio, market_cap ë“±ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼)
                    if not any(col.startswith(prefix) for prefix in [
                        "pe_", "market_", "enterprise_", "return_on_", "debt_", "current_",
                        "profit_", "operating_", "ebitda_", "revenue_", "earnings_",
                        "dividend_", "payout_", "book_", "cash_", "total_", "quarterly_",
                        "calculated_", "latest_", "beta", "fifty_", "two_hundred_",
                        "shares_", "held_", "institutional_", "short_", "float_"
                    ]):
                        feature_columns.append(col)

            # ìˆ«ìí˜• ë°ì´í„°ë§Œ ì„ íƒ
            numeric_columns = []
            for col in feature_columns:
                try:
                    pd.to_numeric(df[col], errors="raise")
                    numeric_columns.append(col)
                except (ValueError, TypeError):
                    self.logger.log_info(f"    {symbol}: {col} ì»¬ëŸ¼ ì œì™¸ (ìˆ«ìê°€ ì•„ë‹˜)")

            # ìˆ˜ìµë¥ ì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì´ë™
            columns_order = numeric_columns + ["return"]
            df = df[columns_order]

            # ëª¨ë“  ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

            # NaN ì œê±° (ìˆ«ì ë³€í™˜ í›„)
            df = df.dropna()

            prepared_data[symbol] = df
            self.logger.log_info(
                f"  {symbol}: {len(df)}ê°œ ë°ì´í„° í¬ì¸íŠ¸, {len(numeric_columns)}ê°œ ê¸°ìˆ ì  ì§€í‘œ"
            )

        self.prepared_data = prepared_data
        return prepared_data

    def run_correlation_analysis(self, symbol: str) -> Dict[str, Any]:
        """ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰"""
        self.logger.log_info(f"ğŸ” {symbol} ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰...")

        data = self.prepared_data[symbol]
        result = self.correlation_analyzer.analyze(
            data, target_column="return", top_n=self.top_features
        )

        self.analysis_results[symbol] = {"correlation": result}
        return result

    def run_linear_regression_analysis(
        self, symbol: str, top_features: int = 5
    ) -> Dict[str, Any]:
        """ì„ í˜•íšŒê·€ ë¶„ì„ ì‹¤í–‰"""
        self.logger.log_info(f"ğŸ“ˆ {symbol} ì„ í˜•íšŒê·€ ë¶„ì„ ì‹¤í–‰...")

        data = self.prepared_data[symbol]

        # ìƒìœ„ íŠ¹ì„± ì„ íƒ
        if (
            symbol in self.analysis_results
            and "correlation" in self.analysis_results[symbol]
        ):
            top_corr_features = self.analysis_results[symbol]["correlation"][
                "top_features"
            ]
            selected_features = top_corr_features[
                : min(top_features, len(top_corr_features))
            ]
        else:
            # ìƒê´€ê´€ê³„ ë¶„ì„ì´ ì—†ìœ¼ë©´ ëª¨ë“  íŠ¹ì„± ì‚¬ìš©
            feature_columns = [col for col in data.columns if col != "return"]
            selected_features = feature_columns[:top_features]

        result = self.linear_regression_analyzer.analyze(
            data, target_column="return", feature_columns=selected_features
        )

        if symbol not in self.analysis_results:
            self.analysis_results[symbol] = {}
        self.analysis_results[symbol]["linear_regression"] = result
        return result

    def run_lasso_regression_analysis(self, symbol: str) -> Dict[str, Any]:
        """Lasso íšŒê·€ ë¶„ì„ ì‹¤í–‰"""
        self.logger.log_info(f"ğŸ¯ {symbol} Lasso íšŒê·€ ë¶„ì„ ì‹¤í–‰...")

        data = self.prepared_data[symbol]
        feature_columns = [col for col in data.columns if col != "return"]

        result = self.lasso_regression_analyzer.analyze(
            data, target_column="return", feature_columns=feature_columns
        )

        if symbol not in self.analysis_results:
            self.analysis_results[symbol] = {}
        self.analysis_results[symbol]["lasso_regression"] = result
        return result

    def run_random_forest_analysis(self, symbol: str) -> Dict[str, Any]:
        """ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ì„ ì‹¤í–‰"""
        self.logger.log_info(f"ğŸŒ² {symbol} ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ì„ ì‹¤í–‰...")

        data = self.prepared_data[symbol]
        feature_columns = [col for col in data.columns if col != "return"]

        result = self.random_forest_analyzer.analyze(
            data, target_column="return", feature_columns=feature_columns
        )

        if symbol not in self.analysis_results:
            self.analysis_results[symbol] = {}
        self.analysis_results[symbol]["random_forest"] = result
        return result

    def run_mlp_analysis(self, symbol: str) -> Dict[str, Any]:
        """MLP ë¶„ì„ ì‹¤í–‰"""
        self.logger.log_info(f"ğŸ§  {symbol} MLP ë¶„ì„ ì‹¤í–‰...")

        data = self.prepared_data[symbol]
        feature_columns = [col for col in data.columns if col != "return"]

        result = self.mlp_analyzer.analyze(
            data, target_column="return", feature_columns=feature_columns
        )

        if symbol not in self.analysis_results:
            self.analysis_results[symbol] = {}
        self.analysis_results[symbol]["mlp"] = result
        return result

    def run_bayesian_distribution_analysis(self, symbol: str) -> Dict[str, Any]:
        """ë² ì´ì§€ì•ˆ ë¶„í¬ ë¶„ì„ ì‹¤í–‰"""
        self.logger.log_info(f"ğŸ”® {symbol} ë² ì´ì§€ì•ˆ ë¶„í¬ ë¶„ì„ ì‹¤í–‰...")

        data = self.prepared_data[symbol]

        # 1. ìˆ˜ìµë¥  ë¶„í¬ ë¶„ì„
        dist_result = self.bayesian_analyzer.analyze_return_distribution(
            data, target_column="return"
        )

        # 2. ë² ì´ì§€ì•ˆ íšŒê·€ ë¶„ì„
        reg_result = self.bayesian_analyzer.analyze_bayesian_regression(
            data, target_column="return"
        )

        # 3. ë³€ë™ì„± ë¶„ì„
        vol_result = self.bayesian_analyzer.analyze_volatility(
            data, target_column="return", window=20
        )

        if symbol not in self.analysis_results:
            self.analysis_results[symbol] = {}
        self.analysis_results[symbol]["bayesian_distribution"] = dist_result
        self.analysis_results[symbol]["bayesian_regression"] = reg_result
        self.analysis_results[symbol]["volatility"] = vol_result
        return {
            "distribution": dist_result,
            "regression": reg_result,
            "volatility": vol_result,
        }

    def run_full_analysis(self, symbols: List[str] = None) -> Dict[str, Any]:
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        # ë¡œê±° ì„¤ì •
        self.logger.setup_logger(
            strategy="quant_analysis", symbols=symbols or [], mode="analysis"
        )

        # ì¢…í•© ìš”ì•½ ë¡œê±° ì„¤ì •
        self.logger.setup_summary_logger(
            symbols=symbols or [], timestamp=self.analysis_start_time
        )

        self.logger.log_section("ğŸ¯ ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ì •ëŸ‰ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
        self.logger.log_info(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
        self.logger.log_info(f"ğŸ“Š ìˆ˜ìµë¥  íƒ€ì…: {self.return_type}")
        self.logger.log_info(f"ğŸ” ìƒìœ„ íŠ¹ì„± ìˆ˜: {self.top_features}")

        # ë°ì´í„° ë¡œë“œ
        self.logger.log_info("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
        data_dict = load_and_preprocess_data(self.data_dir, symbols)
        self.logger.log_success(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ ({len(data_dict)}ê°œ ì¢…ëª©)")

        # ë°ì´í„° ì „ì²˜ë¦¬
        prepared_data = self.prepare_data(data_dict)

        # ê° ì¢…ëª©ë³„ ë¶„ì„ ì‹¤í–‰
        for symbol in prepared_data.keys():
            self.logger.log_info(f"ğŸ”„ {symbol} ë¶„ì„ ì‹œì‘...")

            try:
                # 1. ìƒê´€ê´€ê³„ ë¶„ì„
                corr_result = self.run_correlation_analysis(symbol)

                # 2. ì„ í˜•íšŒê·€ ë¶„ì„
                lr_result = self.run_linear_regression_analysis(symbol)

                # 3. Lasso íšŒê·€ ë¶„ì„
                lasso_result = self.run_lasso_regression_analysis(symbol)

                # 4. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ì„
                rf_result = self.run_random_forest_analysis(symbol)

                # 5. MLP ë¶„ì„
                mlp_result = self.run_mlp_analysis(symbol)

                # 6. ë² ì´ì§€ì•ˆ ë¶„í¬ ë¶„ì„
                bayesian_result = self.run_bayesian_distribution_analysis(symbol)

                self.logger.log_success(f"âœ… {symbol} ë¶„ì„ ì™„ë£Œ")

            except Exception as e:
                self.logger.log_error(f"âŒ {symbol} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # ì¢…í•© ìš”ì•½ ìƒì„±
        self.generate_analysis_summary()

        return self.analysis_results

    def generate_analysis_summary(self):
        """ë¶„ì„ ê²°ê³¼ ì¢…í•© ìš”ì•½"""
        if not self.analysis_results:
            return

        self.logger.log_summary_section("ğŸ“Š ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ì •ëŸ‰ ë¶„ì„ ì¢…í•© ìš”ì•½ ë¦¬í¬íŠ¸")

        # ë¶„ì„ ì„¤ì •
        self.logger.log_summary_subsection("ğŸ“‹ ë¶„ì„ ì„¤ì •")
        self.logger.log_summary_info(f"ë¶„ì„ ìœ í˜•: ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜")
        self.logger.log_summary_info(f"ìˆ˜ìµë¥  íƒ€ì…: {self.return_type}")
        self.logger.log_summary_info(f"ìƒìœ„ íŠ¹ì„± ìˆ˜: {self.top_features}")
        self.logger.log_summary_info(f"ë¶„ì„ ì¢…ëª© ìˆ˜: {len(self.analysis_results)}")

        # ì¢…ëª©ë³„ ìš”ì•½
        self.logger.log_summary_subsection("ğŸ“ˆ ì¢…ëª©ë³„ ë¶„ì„ ìš”ì•½")

        for symbol, results in self.analysis_results.items():
            self.logger.log_summary_info(f"\n{symbol}:")

            if "correlation" in results:
                top_features = results["correlation"]["top_features"][:3]
                self.logger.log_summary_info(
                    f"  ìƒê´€ê´€ê³„ ìƒìœ„: {', '.join(top_features)}"
                )

            if "linear_regression" in results:
                r2 = results["linear_regression"]["r_squared"]
                self.logger.log_summary_info(f"  ì„ í˜•íšŒê·€ RÂ²: {r2:.4f}")

            if "random_forest" in results:
                rf_r2 = results["random_forest"]["r_squared"]
                self.logger.log_summary_info(f"  ëœë¤í¬ë ˆìŠ¤íŠ¸ RÂ²: {rf_r2:.4f}")

            if "mlp" in results:
                mlp_r2 = results["mlp"]["r_squared"]
                self.logger.log_summary_info(f"  MLP RÂ²: {mlp_r2:.4f}")

            if "bayesian_regression" in results:
                br_r2 = results["bayesian_regression"]["bayesian_ridge"]["metrics"][
                    "br_test_r2"
                ]
                ard_r2 = results["bayesian_regression"]["ard_regression"]["metrics"][
                    "ard_test_r2"
                ]
                self.logger.log_summary_info(f"  ë² ì´ì§€ì•ˆ Ridge RÂ²: {br_r2:.4f}")
                self.logger.log_summary_info(f"  ARD RÂ²: {ard_r2:.4f}")

            if "bayesian_distribution" in results:
                best_dist = results["bayesian_distribution"]["best_distribution"]
                var_95 = results["bayesian_distribution"]["var_95"]
                self.logger.log_summary_info(f"  ìµœì  ë¶„í¬: {best_dist}")
                self.logger.log_summary_info(f"  VaR 95%: {var_95:.4f}")

        # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        self.logger.log_summary_subsection("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

        model_performance = {}
        for symbol, results in self.analysis_results.items():
            for model_name, result in results.items():
                if "r_squared" in result:
                    if model_name not in model_performance:
                        model_performance[model_name] = []
                    model_performance[model_name].append(result["r_squared"])

        for model_name, r2_scores in model_performance.items():
            avg_r2 = np.mean(r2_scores)
            max_r2 = np.max(r2_scores)
            self.logger.log_summary_info(
                f"{model_name}: í‰ê·  RÂ² = {avg_r2:.4f}, ìµœê³  RÂ² = {max_r2:.4f}"
            )

        # ì¢…ë£Œ ë©”ì‹œì§€
        self.logger.log_summary_section("ğŸ‰ ë¶„ì„ ì™„ë£Œ")
        self.logger.log_summary_success(
            f"ì´ {len(self.analysis_results)}ê°œ ì¢…ëª© ë¶„ì„ ì™„ë£Œ"
        )
        self.logger.log_summary_info(f"ì¢…í•© ìš”ì•½ ë¡œê·¸: {self.logger.summary_log_file}")

    def save_results(self, output_path: str = None):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            uuid_suffix = f"_{self.execution_uuid}" if self.execution_uuid else ""
            output_path = f"quant_analysis_results_{timestamp}{uuid_suffix}.json"

        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        def make_serializable(obj):
            import numpy as np
            import pandas as pd

            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, pd.DataFrame):
                return obj.to_dict()
            elif isinstance(obj, pd.Series):
                return obj.to_dict()
            elif isinstance(obj, dict):
                return {k: make_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [make_serializable(v) for v in obj]
            elif hasattr(obj, "__name__") and not isinstance(obj, str):
                return str(obj.__name__)
            elif hasattr(obj, "__class__") and not isinstance(obj, str):
                return str(obj.__class__.__name__)
            else:
                try:
                    import json

                    json.dumps(obj)
                    return obj
                except Exception:
                    return str(obj)

        # NaN/infë¥¼ ì•ˆì „í•œ ê°’ìœ¼ë¡œ ë³€í™˜
        def clean_nan_inf(obj):
            import numpy as np
            import pandas as pd

            if isinstance(obj, float):
                if np.isnan(obj) or np.isinf(obj):
                    return None
                return obj
            elif isinstance(obj, dict):
                return {k: clean_nan_inf(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [clean_nan_inf(v) for v in obj]
            elif isinstance(obj, pd.DataFrame):
                return clean_nan_inf(obj.to_dict())
            elif isinstance(obj, pd.Series):
                return clean_nan_inf(obj.to_dict())
            else:
                return obj

        serializable_results = {}
        for symbol, results in self.analysis_results.items():
            serializable_results[symbol] = {}
            for model_name, result in results.items():
                serializable_results[symbol][model_name] = clean_nan_inf(
                    make_serializable(result)
                )

        # analysis í´ë”ì— ì €ì¥
        saved_path = save_analysis_results(
            serializable_results, 
            "quant_analysis", 
            output_path,
            self.analysis_dir
        )

        self.logger.log_success(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {saved_path}")
        return saved_path


class FundamentalAnalyst:
    """ì¬ë¬´ì§€í‘œ ê¸°ë°˜ ë¶„ì„ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        data_dir: str = DEFAULT_DATA_DIR,
        config_path: str = DEFAULT_CONFIG_PATH,
        return_type: str = "percentage",  # "percentage" or "log"
        top_features: int = 10,
        analysis_dir: str = "analysis",  # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    ):
        self.data_dir = data_dir
        self.config = load_config(config_path)
        self.return_type = return_type
        self.top_features = top_features
        self.analysis_dir = analysis_dir
        self.logger = Logger()
        self.analysis_start_time = datetime.now()
        self.execution_uuid = None  # UUID ì´ˆê¸°í™”

        # ë¶„ì„ í´ë” êµ¬ì¡° ìƒì„±
        create_analysis_folder_structure(analysis_dir)

        # ì¬ë¬´ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.financial_analyzer = FinancialAnalyzer()

        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = {}
        self.prepared_data = {}

    def prepare_data(
        self, data_dict: Dict[str, pd.DataFrame]
    ) -> Dict[str, pd.DataFrame]:
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ìˆ˜ìµë¥  ê³„ì‚° (ì¬ë¬´ì§€í‘œë§Œ ì‚¬ìš©)"""
        self.logger.log_info("ğŸ“Š ì¬ë¬´ì§€í‘œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ìˆ˜ìµë¥  ê³„ì‚° ì¤‘...")

        # ì¬ë¬´ì§€í‘œ ê´€ë ¨ ì»¬ëŸ¼ë“¤
        financial_columns = {
            "pe_ratio", "forward_pe", "peg_ratio", "price_to_book", "price_to_sales",
            "ev_to_ebitda", "ev_to_revenue", "price_to_cashflow", "price_to_free_cashflow",
            "return_on_equity", "return_on_assets", "return_on_capital", "return_on_invested_capital",
            "profit_margin", "operating_margin", "gross_margin", "ebitda_margin", "net_income_margin",
            "revenue_growth", "earnings_growth", "earnings_quarterly_growth", "revenue_quarterly_growth",
            "earnings_annual_growth", "revenue_annual_growth", "revenue_per_employee", "revenue_per_share",
            "debt_to_equity", "debt_to_assets", "current_ratio", "quick_ratio", "cash_ratio",
            "interest_coverage", "total_cash", "total_debt", "net_debt", "cash_per_share",
            "book_value", "tangible_book_value", "operating_cashflow", "free_cashflow",
            "free_cashflow_yield", "operating_cashflow_per_share", "free_cashflow_per_share",
            "cashflow_to_debt", "dividend_yield", "dividend_rate", "payout_ratio",
            "dividend_payout_ratio", "five_year_avg_dividend_yield", "forward_dividend_yield",
            "forward_dividend_rate", "earnings_ttm", "earnings_forward", "earnings_quarterly",
            "earnings_annual", "eps_ttm", "eps_forward", "eps_quarterly", "eps_annual",
            "total_revenue", "revenue_ttm", "revenue_forward", "revenue_quarterly", "revenue_annual",
            "gross_profits", "ebitda", "ebit", "net_income", "net_income_ttm",
            "shares_outstanding", "float_shares", "shares_short", "shares_short_prior_month",
            "short_ratio", "short_percent_of_float", "shares_percent_shares_out",
            "held_percent_insiders", "held_percent_institutions", "institutional_ownership",
            "beta", "fifty_two_week_change", "fifty_day_average", "two_hundred_day_average",
            "fifty_two_week_high", "fifty_two_week_low", "day_high", "day_low", "volume",
            "average_volume", "market_cap", "enterprise_value",
            # ë¶„ê¸°ë³„ ë°ì´í„°
            "quarterly_revenue", "quarterly_net_income", "quarterly_operating_income",
            "quarterly_ebitda", "quarterly_eps", "quarterly_gross_profit", "quarterly_ebit",
            "quarterly_operating_expense", "quarterly_research_development",
            "quarterly_selling_general_admin", "quarterly_total_assets", "quarterly_total_liabilities",
            "quarterly_total_equity", "quarterly_cash", "quarterly_debt", "quarterly_current_assets",
            "quarterly_current_liabilities", "quarterly_inventory", "quarterly_accounts_receivable",
            "quarterly_accounts_payable", "quarterly_short_term_debt", "quarterly_long_term_debt",
            "quarterly_goodwill", "quarterly_intangible_assets", "quarterly_property_plant_equipment",
            "quarterly_operating_cashflow", "quarterly_investing_cashflow", "quarterly_financing_cashflow",
            "quarterly_free_cashflow", "quarterly_capital_expenditure", "quarterly_dividends_paid",
            "quarterly_net_income_cashflow", "quarterly_depreciation", "quarterly_change_in_cash",
            "quarterly_change_in_receivables", "quarterly_change_in_inventory", "quarterly_change_in_payables",
            # ë°°ë‹¹ ë° ê¸°ì—… í–‰ë™ ë°ì´í„°
            "latest_dividend_amount", "dividend_frequency", "latest_split_ratio", "split_frequency",
            "latest_capital_gain",
            # ê³„ì‚°ëœ ì¬ë¬´ë¹„ìœ¨ë“¤
            "calculated_roe", "calculated_roa", "calculated_debt_to_assets", "calculated_current_ratio",
            "calculated_operating_margin", "calculated_net_margin", "calculated_ebitda_margin",
            "calculated_asset_turnover", "calculated_inventory_turnover", "calculated_receivables_turnover",
            "calculated_cashflow_to_debt", "calculated_fcf_yield", "calculated_dividend_payout"
        }

        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ (ê¸°ìˆ ì  ì§€í‘œ ë° ê¸°ë³¸ ë°ì´í„°)
        excluded_columns = {
            "datetime", "date", "time", "timestamp", "open", "high", "low", "close", "volume",
        }

        prepared_data = {}

        for symbol, data in data_dict.items():
            # ë°ì´í„° ë³µì‚¬
            df = data.copy()

            # ê¸°ì¡´ returns ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if "returns" in df.columns:
                # ê¸°ì¡´ returns ì»¬ëŸ¼ì„ targetìœ¼ë¡œ ì‚¬ìš©
                df["return"] = df["returns"]
                self.logger.log_info(f"  {symbol}: ê¸°ì¡´ returns ì»¬ëŸ¼ ì‚¬ìš©")
            else:
                # ìˆ˜ìµë¥  ê³„ì‚° (target ë³€ìˆ˜)
                if self.return_type == "log":
                    df["return"] = np.log(df["close"] / df["close"].shift(1))
                else:  # percentage
                    df["return"] = df["close"].pct_change() * 100
                self.logger.log_info(f"  {symbol}: ìƒˆë¡œìš´ ìˆ˜ìµë¥  ê³„ì‚°")

            # NaN ì œê±°
            df = df.dropna()

            # ì¬ë¬´ì§€í‘œ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ
            feature_columns = []
            for col in df.columns:
                if col not in excluded_columns and col != "return" and col != "returns":
                    # ì¬ë¬´ì§€í‘œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                    if col in financial_columns or any(col.startswith(prefix) for prefix in [
                        "pe_", "market_", "return_on_", "debt_", "current_",
                        "profit_", "operating_", "ebitda_", "revenue_", "earnings_",
                        "dividend_", "payout_", "book_", "cash_", "total_", "quarterly_",
                        "calculated_", "latest_", "beta", "fifty_", "two_hundred_",
                        "shares_", "held_", "institutional_", "short_", "float_"
                    ]):
                        feature_columns.append(col)

            # ìˆ«ìí˜• ë°ì´í„°ë§Œ ì„ íƒ
            numeric_columns = []
            for col in feature_columns:
                try:
                    pd.to_numeric(df[col], errors="raise")
                    numeric_columns.append(col)
                except (ValueError, TypeError):
                    self.logger.log_info(f"    {symbol}: {col} ì»¬ëŸ¼ ì œì™¸ (ìˆ«ìê°€ ì•„ë‹˜)")

            if len(numeric_columns) == 0:
                self.logger.log_info(f"  {symbol}: ì¬ë¬´ì§€í‘œ ì—†ìŒ â†’ ì¬ë¬´ë¶„ì„ ì œì™¸")
                continue

            # ìˆ˜ìµë¥ ì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì´ë™
            columns_order = numeric_columns + ["return"]
            df = df[columns_order]

            # ëª¨ë“  ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

            # NaN ì œê±° (ìˆ«ì ë³€í™˜ í›„)
            df = df.dropna()

            prepared_data[symbol] = df
            self.logger.log_info(
                f"  {symbol}: {len(df)}ê°œ ë°ì´í„° í¬ì¸íŠ¸, {len(numeric_columns)}ê°œ ì¬ë¬´ì§€í‘œ"
            )

        self.prepared_data = prepared_data
        return prepared_data

    def run_financial_analysis(self, symbol: str) -> Dict[str, Any]:
        """ì¬ë¬´ë¶„ì„ ì‹¤í–‰"""
        self.logger.log_info(f"ğŸ’° {symbol} ì¬ë¬´ë¶„ì„ ì‹¤í–‰...")

        data = self.prepared_data[symbol]
        result = self.financial_analyzer.analyze_comprehensive(
            data, target_column="return", symbol=symbol
        )

        if symbol not in self.analysis_results:
            self.analysis_results[symbol] = {}
        self.analysis_results[symbol]["financial_analysis"] = result
        return result

    def run_full_analysis(self, symbols: List[str] = None) -> Dict[str, Any]:
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        # ë¡œê±° ì„¤ì •
        self.logger.setup_logger(
            strategy="fundamental_analysis", symbols=symbols or [], mode="analysis"
        )

        # ì¢…í•© ìš”ì•½ ë¡œê±° ì„¤ì •
        self.logger.setup_summary_logger(
            symbols=symbols or [], timestamp=self.analysis_start_time
        )

        self.logger.log_section("ğŸ’° ì¬ë¬´ì§€í‘œ ê¸°ë°˜ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
        self.logger.log_info(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
        self.logger.log_info(f"ğŸ“Š ìˆ˜ìµë¥  íƒ€ì…: {self.return_type}")
        self.logger.log_info(f"ğŸ” ìƒìœ„ íŠ¹ì„± ìˆ˜: {self.top_features}")

        # ë°ì´í„° ë¡œë“œ
        self.logger.log_info("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
        data_dict = load_and_preprocess_data(self.data_dir, symbols)
        self.logger.log_success(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ ({len(data_dict)}ê°œ ì¢…ëª©)")

        # ë°ì´í„° ì „ì²˜ë¦¬
        prepared_data = self.prepare_data(data_dict)
        if not prepared_data:
            self.logger.log_warning("ì¬ë¬´ì§€í‘œê°€ ìˆëŠ” ì¢…ëª©ì´ ì—†ì–´ ì¬ë¬´ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}

        # ê° ì¢…ëª©ë³„ ë¶„ì„ ì‹¤í–‰
        for symbol in prepared_data.keys():
            self.logger.log_info(f"ğŸ”„ {symbol} ë¶„ì„ ì‹œì‘...")

            try:
                # ì¬ë¬´ë¶„ì„ ì‹¤í–‰
                financial_result = self.run_financial_analysis(symbol)
                self.logger.log_success(f"âœ… {symbol} ë¶„ì„ ì™„ë£Œ")

            except Exception as e:
                self.logger.log_error(f"âŒ {symbol} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # ì¢…í•© ìš”ì•½ ìƒì„±
        self.generate_analysis_summary()

        return self.analysis_results

    def generate_analysis_summary(self):
        """ë¶„ì„ ê²°ê³¼ ì¢…í•© ìš”ì•½"""
        if not self.analysis_results:
            return

        self.logger.log_summary_section("ğŸ’° ì¬ë¬´ì§€í‘œ ê¸°ë°˜ ë¶„ì„ ì¢…í•© ìš”ì•½ ë¦¬í¬íŠ¸")

        # ë¶„ì„ ì„¤ì •
        self.logger.log_summary_subsection("ğŸ“‹ ë¶„ì„ ì„¤ì •")
        self.logger.log_summary_info(f"ë¶„ì„ ìœ í˜•: ì¬ë¬´ì§€í‘œ ê¸°ë°˜")
        self.logger.log_summary_info(f"ìˆ˜ìµë¥  íƒ€ì…: {self.return_type}")
        self.logger.log_summary_info(f"ìƒìœ„ íŠ¹ì„± ìˆ˜: {self.top_features}")
        self.logger.log_summary_info(f"ë¶„ì„ ì¢…ëª© ìˆ˜: {len(self.analysis_results)}")

        # ì¢…ëª©ë³„ ìš”ì•½
        self.logger.log_summary_subsection("ğŸ“ˆ ì¢…ëª©ë³„ ë¶„ì„ ìš”ì•½")

        for symbol, results in self.analysis_results.items():
            self.logger.log_summary_info(f"\n{symbol}:")

            if "financial_analysis" in results:
                financial_result = results["financial_analysis"]
                
                # ì£¼ìš” ì¬ë¬´ì§€í‘œ ìš”ì•½
                if "key_metrics" in financial_result:
                    metrics = financial_result["key_metrics"]
                    self.logger.log_summary_info(f"  P/E ë¹„ìœ¨: {metrics.get('pe_ratio', 'N/A')}")
                    self.logger.log_summary_info(f"  ROE: {metrics.get('roe', 'N/A')}")
                    self.logger.log_summary_info(f"  ë¶€ì±„ë¹„ìœ¨: {metrics.get('debt_to_equity', 'N/A')}")
                    self.logger.log_summary_info(f"  ë°°ë‹¹ìˆ˜ìµë¥ : {metrics.get('dividend_yield', 'N/A')}")

                # ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼
                if "correlation_analysis" in financial_result:
                    corr_result = financial_result["correlation_analysis"]
                    if "top_features" in corr_result:
                        top_features = corr_result["top_features"][:3]
                        self.logger.log_summary_info(f"  ìƒê´€ê´€ê³„ ìƒìœ„: {', '.join(top_features)}")

                # ì˜ˆì¸¡ ëª¨ë¸ ê²°ê³¼
                if "prediction_models" in financial_result:
                    pred_result = financial_result["prediction_models"]
                    for model_name, model_result in pred_result.items():
                        if "r_squared" in model_result:
                            r2 = model_result["r_squared"]
                            self.logger.log_summary_info(f"  {model_name} RÂ²: {r2:.4f}")

        # ì¢…ë£Œ ë©”ì‹œì§€
        self.logger.log_summary_section("ğŸ‰ ë¶„ì„ ì™„ë£Œ")
        self.logger.log_summary_success(
            f"ì´ {len(self.analysis_results)}ê°œ ì¢…ëª© ë¶„ì„ ì™„ë£Œ"
        )
        self.logger.log_summary_info(f"ì¢…í•© ìš”ì•½ ë¡œê·¸: {self.logger.summary_log_file}")

    def save_results(self, output_path: str = None):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            uuid_suffix = f"_{self.execution_uuid}" if self.execution_uuid else ""
            output_path = f"fundamental_analysis_results_{timestamp}{uuid_suffix}.json"

        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        def make_serializable(obj):
            import numpy as np
            import pandas as pd

            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, pd.DataFrame):
                return obj.to_dict()
            elif isinstance(obj, pd.Series):
                return obj.to_dict()
            elif isinstance(obj, dict):
                return {k: make_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [make_serializable(v) for v in obj]
            elif hasattr(obj, "__name__") and not isinstance(obj, str):
                return str(obj.__name__)
            elif hasattr(obj, "__class__") and not isinstance(obj, str):
                return str(obj.__class__.__name__)
            else:
                try:
                    import json

                    json.dumps(obj)
                    return obj
                except Exception:
                    return str(obj)

        # NaN/infë¥¼ ì•ˆì „í•œ ê°’ìœ¼ë¡œ ë³€í™˜
        def clean_nan_inf(obj):
            import numpy as np
            import pandas as pd

            if isinstance(obj, float):
                if np.isnan(obj) or np.isinf(obj):
                    return None
                return obj
            elif isinstance(obj, dict):
                return {k: clean_nan_inf(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [clean_nan_inf(v) for v in obj]
            elif isinstance(obj, pd.DataFrame):
                return clean_nan_inf(obj.to_dict())
            elif isinstance(obj, pd.Series):
                return clean_nan_inf(obj.to_dict())
            else:
                return obj

        serializable_results = {}
        for symbol, results in self.analysis_results.items():
            serializable_results[symbol] = {}
            for model_name, result in results.items():
                serializable_results[symbol][model_name] = clean_nan_inf(
                    make_serializable(result)
                )

        # analysis í´ë”ì— ì €ì¥
        saved_path = save_analysis_results(
            serializable_results, 
            "fundamental_analysis", 
            output_path,
            self.analysis_dir
        )

        self.logger.log_success(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {saved_path}")
        return saved_path


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ")
    parser.add_argument("--data_dir", default="data", help="ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--symbols", nargs="+", help="ë¶„ì„í•  ì¢…ëª© ëª©ë¡")
    parser.add_argument(
        "--return_type",
        choices=["percentage", "log"],
        default="percentage",
        help="ìˆ˜ìµë¥  ê³„ì‚° ë°©ì‹",
    )
    parser.add_argument("--top_features", type=int, default=10, help="ìƒìœ„ íŠ¹ì„± ìˆ˜")
    parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--uuid", help="ì‹¤í–‰ UUID")
    parser.add_argument(
        "--analysis_type",
        choices=["quant", "fundamental", "both"],
        default="both",
        help="ë¶„ì„ ìœ í˜• (ê¸°ìˆ ì /ì¬ë¬´ì /ë‘˜ ë‹¤)",
    )

    args = parser.parse_args()

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    quant_analyst = QuantAnalyst(
        data_dir=args.data_dir,
        return_type=args.return_type,
        top_features=args.top_features,
    )
    
    fundamental_analyst = FundamentalAnalyst(
        data_dir=args.data_dir,
        return_type=args.return_type,
        top_features=args.top_features,
    )
    
    # UUID ì„¤ì •
    if args.uuid:
        quant_analyst.execution_uuid = args.uuid
        fundamental_analyst.execution_uuid = args.uuid
        print(f"ğŸ†” ë¶„ì„ UUID ì„¤ì •: {args.uuid}")

    results = {}

    # ë¶„ì„ ì‹¤í–‰
    if args.analysis_type in ["quant", "both"]:
        print("ğŸ¯ ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ë¶„ì„ ì‹œì‘...")
        quant_results = quant_analyst.run_full_analysis(symbols=args.symbols)
        quant_analyst.save_results()
        results["quant_analysis"] = quant_results

    if args.analysis_type in ["fundamental", "both"]:
        print("ğŸ’° ì¬ë¬´ì§€í‘œ ê¸°ë°˜ ë¶„ì„ ì‹œì‘...")
        fundamental_results = fundamental_analyst.run_full_analysis(symbols=args.symbols)
        fundamental_analyst.save_results()
        results["fundamental_analysis"] = fundamental_results

    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    if "quant_analysis" in results:
        print(f"   ê¸°ìˆ ì  ë¶„ì„: {len(results['quant_analysis'])}ê°œ ì¢…ëª©")
    if "fundamental_analysis" in results:
        print(f"   ì¬ë¬´ì  ë¶„ì„: {len(results['fundamental_analysis'])}ê°œ ì¢…ëª©")


if __name__ == "__main__":
    main()
