#!/usr/bin/env python3
"""
SPY ê°•í™”í•™ìŠµ í€€íŠ¸ë§¤ë§¤ ì‹¤í–‰ ì‹œìŠ¤í…œ

ì§€ì›í•˜ëŠ” ë°©ë²•ë¡ :
1. DQN (Deep Q-Network) - ì´ì‚° í–‰ë™ ê³µê°„
2. PPO (Proximal Policy Optimization) - ì—°ì† í–‰ë™ ê³µê°„

ì£¼ìš” ê¸°ëŠ¥:
1. SPY ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
2. ê°•í™”í•™ìŠµ ëª¨ë¸ í›ˆë ¨ (DQN/PPO)
3. ë°±í…ŒìŠ¤íŒ… ë° ì„±ê³¼ í‰ê°€
4. ê²°ê³¼ ì‹œê°í™” ë° ë¦¬í¬íŠ¸ ìƒì„±
5. ëª¨ë¸ ì €ì¥/ë¡œë“œ
"""

import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import argparse
import json
import warnings
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, project_root)

# DQN ëª¨ë“ˆ import
from src.actions.DQN import (
    TradingEnvironment as DQNTradingEnvironment,
    DQNAgent,
    RLTradingStrategy as DQNRLTradingStrategy,
    prepare_spy_data_for_rl,
    train_rl_agent as train_dqn_agent,
    backtest_rl_strategy as backtest_dqn_strategy,
)

# PPO (Advanced RL) ëª¨ë“ˆ import
from src.actions.advanced_rl import (
    ModernTradingEnvironment as PPOTradingEnvironment,
    PPOTrainer,
    ModernRLTradingStrategy as PPORLTradingStrategy,
    train_modern_rl_agent as train_ppo_agent,
)

# í•˜ì´ë¸Œë¦¬ë“œ RL ëª¨ë“ˆ import
from src.actions.hybrid_rl_strategy import (
    HybridTradingEnvironment,
    HybridPPOTrainer,
    HybridRLTradingStrategy,
    generate_traditional_signals,
    train_hybrid_rl_agent,
)

# ê¸°ì¡´ í”„ë ˆì„ì›Œí¬ import
from src.actions.calculate_index import StrategyParams
from src.actions.backtest_strategies import BacktestEngine
from src.agent.helper import load_config, Logger
import logging

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore")
plt.style.use("seaborn-v0_8")
sns.set_palette("husl")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# GPU/CPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class RLQuantLearner:
    """ê°•í™”í•™ìŠµ í€€íŠ¸ë§¤ë§¤ í•™ìŠµ ë° ì‹¤í–‰ ì‹œìŠ¤í…œ"""

    def __init__(self, config_path: str = "config/config_default.json"):
        """
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = load_config(config_path)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.data_dir = "data/macro"
        self.models_dir = "models/reinforcement_learning"
        self.results_dir = f"results/rl_trading_{self.timestamp}"
        self.log_dir = "log"

        # ë””ë ‰í† ë¦¬ ìƒì„±
        for directory in [self.models_dir, self.results_dir, self.log_dir]:
            os.makedirs(directory, exist_ok=True)

        # ê²°ê³¼ ì €ì¥ìš©
        self.training_results = {}
        self.backtest_results = {}
        self.current_method = None

        logger.info(f"ğŸš€ RLQuantLearner ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {self.models_dir}")
        logger.info(f"ğŸ“Š ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.results_dir}")

    def load_spy_data(self, symbol: str = "SPY") -> Optional[pd.DataFrame]:
        """SPY ë°ì´í„° ë¡œë“œ"""
        logger.info(f"ğŸ“‚ {symbol} ë°ì´í„° ë¡œë”© ì¤‘...")

        # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        data_files = []
        for file in os.listdir(self.data_dir):
            if (
                file.startswith(symbol) or file.startswith(symbol.lower())
            ) and file.endswith(".csv"):
                data_files.append(file)

        if not data_files:
            logger.error(f"âŒ {symbol} ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
        latest_file = sorted(data_files)[-1]
        file_path = os.path.join(self.data_dir, latest_file)

        logger.info(f"ğŸ“„ ì‚¬ìš©í•  ë°ì´í„° íŒŒì¼: {latest_file}")

        try:
            # ë°ì´í„° ì „ì²˜ë¦¬
            data = prepare_spy_data_for_rl(file_path)
            logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
            logger.info(f"ğŸ“… ê¸°ê°„: {data['datetime'].min()} ~ {data['datetime'].max()}")

            return data

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def split_data(
        self, data: pd.DataFrame, train_ratio: float = 0.8
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Train/Test ë°ì´í„° ë¶„í• """
        split_idx = int(len(data) * train_ratio)

        train_data = data.iloc[:split_idx].copy().reset_index(drop=True)
        test_data = data.iloc[split_idx:].copy().reset_index(drop=True)

        logger.info(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        logger.info(
            f"  ğŸ‹ï¸ Train: {len(train_data)}ê°œ ({train_data['datetime'].min()} ~ {train_data['datetime'].max()})"
        )
        logger.info(
            f"  ğŸ§ª Test: {len(test_data)}ê°œ ({test_data['datetime'].min()} ~ {test_data['datetime'].max()})"
        )

        return train_data, test_data

    def train_dqn_model(
        self, train_data: pd.DataFrame, episodes: int = 1000, symbol: str = "SPY"
    ) -> Tuple[DQNAgent, List[float]]:
        """DQN ëª¨ë¸ í›ˆë ¨"""
        logger.info("ğŸ¤– DQN ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

        model_save_path = os.path.join(
            self.models_dir, f"{symbol}_dqn_model_{self.timestamp}.pth"
        )

        # í›ˆë ¨ ì‹¤í–‰ (1ì¼~3ì£¼ ë³´ìœ  ì „ëµ)
        agent, episode_rewards = train_dqn_agent(
            data=train_data,
            episodes=episodes,
            model_save_path=model_save_path,
            min_holding_days=1,
            max_holding_days=21,
        )

        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        self.training_results = {
            "method": "DQN",
            "model_path": model_save_path,
            "episodes": episodes,
            "episode_rewards": episode_rewards,
            "final_epsilon": agent.epsilon,
            "loss_history": agent.loss_history,
            "train_data_points": len(train_data),
        }

        logger.info("âœ… DQN ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        return agent, episode_rewards

    def train_ppo_model(
        self, train_data: pd.DataFrame, episodes: int = 1000, symbol: str = "SPY"
    ) -> Tuple[PPOTrainer, List[float]]:
        """PPO ëª¨ë¸ í›ˆë ¨"""
        logger.info("ğŸ¤– PPO ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

        model_save_path = os.path.join(
            self.models_dir, f"{symbol}_ppo_model_{self.timestamp}.pth"
        )

        # í›ˆë ¨ ì‹¤í–‰ (ì—°ì† í–‰ë™ ê³µê°„)
        trainer, episode_rewards = train_ppo_agent(
            data=train_data,
            method="PPO",
            episodes=episodes,
            model_save_path=model_save_path,
            action_type="continuous",
        )

        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        self.training_results = {
            "method": "PPO",
            "model_path": model_save_path,
            "episodes": episodes,
            "episode_rewards": episode_rewards,
            "loss_history": trainer.loss_history,
            "train_data_points": len(train_data),
        }

        logger.info("âœ… PPO ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        return trainer, episode_rewards

    def train_hybrid_model(
        self, train_data: pd.DataFrame, episodes: int = 1000, symbol: str = "SPY"
    ) -> Tuple[HybridPPOTrainer, List[float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í›ˆë ¨ (ì „í†µ ì „ëµ + RL)"""
        logger.info("ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

        model_save_path = os.path.join(
            self.models_dir, f"{symbol}_hybrid_model_{self.timestamp}.pth"
        )

        # ì „í†µ ì‹ í˜¸ ìƒì„±
        traditional_signals = generate_traditional_signals(train_data)

        # í›ˆë ¨ ì‹¤í–‰
        trainer, episode_rewards = train_hybrid_rl_agent(
            data=train_data,
            traditional_signals=traditional_signals,
            episodes=episodes,
            model_save_path=model_save_path,
            action_type="continuous",
        )

        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        self.training_results = {
            "method": "HYBRID",
            "model_path": model_save_path,
            "episodes": episodes,
            "episode_rewards": episode_rewards,
            "loss_history": trainer.loss_history,
            "train_data_points": len(train_data),
        }

        logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        return trainer, episode_rewards

    def train_model(
        self,
        train_data: pd.DataFrame,
        episodes: int = 1000,
        method: str = "DQN",
        symbol: str = "SPY",
    ) -> Tuple[Any, List[float]]:
        """ê°•í™”í•™ìŠµ ëª¨ë¸ í›ˆë ¨ (í†µí•© ì¸í„°í˜ì´ìŠ¤)"""
        self.current_method = method

        if method.upper() == "DQN":
            return self.train_dqn_model(train_data, episodes, symbol)
        elif method.upper() == "PPO":
            return self.train_ppo_model(train_data, episodes, symbol)
        elif method.upper() == "HYBRID":
            return self.train_hybrid_model(train_data, episodes, symbol)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•ë¡ : {method}")

    def evaluate_dqn_model(
        self, agent: DQNAgent, test_data: pd.DataFrame, symbol: str = "SPY"
    ) -> Dict[str, Any]:
        """DQN ëª¨ë¸ í‰ê°€ (ë°±í…ŒìŠ¤íŒ…)"""
        logger.info("ğŸ“ˆ DQN ëª¨ë¸ ë°±í…ŒìŠ¤íŒ… ì‹œì‘")

        # ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
        backtest_results = backtest_dqn_strategy(test_data, agent)

        # ê¸°ì¡´ í”„ë ˆì„ì›Œí¬ì™€ ë¹„êµë¥¼ ìœ„í•œ Buy & Hold ê³„ì‚°
        initial_price = test_data.iloc[0]["close"]
        final_price = test_data.iloc[-1]["close"]
        buy_hold_return = (final_price - initial_price) / initial_price

        # ê²°ê³¼ ìš”ì•½
        results_summary = {
            "method": "DQN",
            "symbol": symbol,
            "test_period": f"{test_data['datetime'].min()} ~ {test_data['datetime'].max()}",
            "test_data_points": len(test_data),
            "strategy_return": backtest_results["strategy_return"],
            "buy_hold_return": buy_hold_return,
            "excess_return": backtest_results["excess_return"],
            "sharpe_ratio": backtest_results["sharpe_ratio"],
            "max_drawdown": backtest_results["max_drawdown"],
            "total_trades": backtest_results["total_trades"],
            "action_distribution": backtest_results["action_distribution"],
            "final_portfolio_value": backtest_results["final_portfolio_value"],
        }

        self.backtest_results = {**backtest_results, **results_summary}

        # ê²°ê³¼ ì¶œë ¥
        logger.info("ğŸ“Š DQN ë°±í…ŒìŠ¤íŒ… ê²°ê³¼:")
        logger.info(f"  ğŸ’° ì „ëµ ìˆ˜ìµë¥ : {results_summary['strategy_return']:.2%}")
        logger.info(f"  ğŸ“ˆ Buy & Hold ìˆ˜ìµë¥ : {results_summary['buy_hold_return']:.2%}")
        logger.info(f"  âš¡ ì´ˆê³¼ ìˆ˜ìµë¥ : {results_summary['excess_return']:.2%}")
        logger.info(f"  ğŸ“ ìƒ¤í”„ ë¹„ìœ¨: {results_summary['sharpe_ratio']:.3f}")
        logger.info(f"  ğŸ“‰ ìµœëŒ€ ë‚™í­: {results_summary['max_drawdown']:.2%}")
        logger.info(f"  ğŸ”„ ì´ ê±°ë˜ ìˆ˜: {results_summary['total_trades']}")

        return self.backtest_results

    def evaluate_ppo_model(
        self, trainer: PPOTrainer, test_data: pd.DataFrame, symbol: str = "SPY"
    ) -> Dict[str, Any]:
        """PPO ëª¨ë¸ í‰ê°€ (ë°±í…ŒìŠ¤íŒ…)"""
        logger.info("ğŸ“ˆ PPO ëª¨ë¸ ë°±í…ŒìŠ¤íŒ… ì‹œì‘")

        # PPO í™˜ê²½ìœ¼ë¡œ ë°±í…ŒìŠ¤íŒ…
        env = PPOTradingEnvironment(test_data, action_type="continuous")
        state = env.reset()

        portfolio_values = []
        actions_taken = []
        total_reward = 0
        done = False

        while not done:
            # í–‰ë™ ì„ íƒ (inference ëª¨ë“œ)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            action, _, _ = trainer.policy.get_action(state_tensor, training=False)

            # í–‰ë™ ì‹¤í–‰
            action_np = action.squeeze().detach().cpu().numpy()
            next_state, reward, done, info = env.step(action_np)

            # ë°ì´í„° ìˆ˜ì§‘
            portfolio_values.append(info["portfolio_value"])
            actions_taken.append(action_np[0])  # í¬ì§€ì…˜ í¬ê¸°ë§Œ ì €ì¥
            total_reward += reward

            state = next_state

        # ì„±ê³¼ ê³„ì‚°
        initial_value = 100000
        final_value = portfolio_values[-1] if portfolio_values else initial_value
        strategy_return = (final_value - initial_value) / initial_value

        # Buy & Hold ë¹„êµ
        initial_price = test_data.iloc[0]["close"]
        final_price = test_data.iloc[-1]["close"]
        buy_hold_return = (final_price - initial_price) / initial_price

        # ìƒ¤í”„ ë¹„ìœ¨ ê³„ì‚°
        returns = np.diff(portfolio_values) / portfolio_values[:-1]
        sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0

        # ìµœëŒ€ ë‚™í­ ê³„ì‚°
        peak = np.maximum.accumulate(portfolio_values)
        drawdown = (peak - portfolio_values) / peak
        max_drawdown = np.max(drawdown)

        # í–‰ë™ ë¶„í¬
        action_distribution = {}
        for action in actions_taken:
            if action < -0.3:
                action_distribution["short"] = action_distribution.get("short", 0) + 1
            elif action > 0.3:
                action_distribution["long"] = action_distribution.get("long", 0) + 1
            else:
                action_distribution["hold"] = action_distribution.get("hold", 0) + 1

        # ê²°ê³¼ ìš”ì•½
        results_summary = {
            "method": "PPO",
            "symbol": symbol,
            "test_period": f"{test_data['datetime'].min()} ~ {test_data['datetime'].max()}",
            "test_data_points": len(test_data),
            "strategy_return": strategy_return,
            "buy_hold_return": buy_hold_return,
            "excess_return": strategy_return - buy_hold_return,
            "sharpe_ratio": sharpe_ratio,
            "max_drawdown": max_drawdown,
            "total_trades": len([a for a in actions_taken if abs(a) > 0.1]),
            "action_distribution": action_distribution,
            "final_portfolio_value": final_value,
            "portfolio_values": portfolio_values,
        }

        self.backtest_results = results_summary

        # ê²°ê³¼ ì¶œë ¥
        logger.info("ğŸ“Š PPO ë°±í…ŒìŠ¤íŒ… ê²°ê³¼:")
        logger.info(f"  ğŸ’° ì „ëµ ìˆ˜ìµë¥ : {results_summary['strategy_return']:.2%}")
        logger.info(f"  ğŸ“ˆ Buy & Hold ìˆ˜ìµë¥ : {results_summary['buy_hold_return']:.2%}")
        logger.info(f"  âš¡ ì´ˆê³¼ ìˆ˜ìµë¥ : {results_summary['excess_return']:.2%}")
        logger.info(f"  ğŸ“ ìƒ¤í”„ ë¹„ìœ¨: {results_summary['sharpe_ratio']:.3f}")
        logger.info(f"  ğŸ“‰ ìµœëŒ€ ë‚™í­: {results_summary['max_drawdown']:.2%}")
        logger.info(f"  ğŸ”„ ì´ ê±°ë˜ ìˆ˜: {results_summary['total_trades']}")

        return self.backtest_results

    def evaluate_hybrid_model(
        self, trainer: HybridPPOTrainer, test_data: pd.DataFrame, symbol: str = "SPY"
    ) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í‰ê°€ (ë°±í…ŒìŠ¤íŒ…)"""
        logger.info("ğŸ“ˆ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë°±í…ŒìŠ¤íŒ… ì‹œì‘")

        # ì „í†µ ì‹ í˜¸ ìƒì„±
        traditional_signals = generate_traditional_signals(test_data)

        # í•˜ì´ë¸Œë¦¬ë“œ í™˜ê²½ìœ¼ë¡œ ë°±í…ŒìŠ¤íŒ…
        env = HybridTradingEnvironment(
            test_data, traditional_signals, action_type="continuous"
        )
        state = env.reset()

        portfolio_values = []
        actions_taken = []
        traditional_signals_used = []
        total_reward = 0
        done = False

        while not done:
            # í–‰ë™ ì„ íƒ (inference ëª¨ë“œ)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            action, _, _ = trainer.policy.get_action(state_tensor, training=False)

            # í–‰ë™ ì‹¤í–‰
            action_np = action.squeeze().detach().cpu().numpy()
            next_state, reward, done, info = env.step(action_np)

            # ë°ì´í„° ìˆ˜ì§‘
            portfolio_values.append(info["portfolio_value"])
            actions_taken.append(action_np[0])  # ì „í†µ ì‹ í˜¸ ê°€ì¤‘ì¹˜
            traditional_signals_used.append(info["traditional_signal"])
            total_reward += reward

            state = next_state

        # ì„±ê³¼ ê³„ì‚°
        initial_value = 100000
        final_value = portfolio_values[-1] if portfolio_values else initial_value
        strategy_return = (final_value - initial_value) / initial_value

        # Buy & Hold ë¹„êµ
        initial_price = test_data.iloc[0]["close"]
        final_price = test_data.iloc[-1]["close"]
        buy_hold_return = (final_price - initial_price) / initial_price

        # ìƒ¤í”„ ë¹„ìœ¨ ê³„ì‚°
        returns = np.diff(portfolio_values) / portfolio_values[:-1]
        sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0

        # ìµœëŒ€ ë‚™í­ ê³„ì‚°
        peak = np.maximum.accumulate(portfolio_values)
        drawdown = (peak - portfolio_values) / peak
        max_drawdown = np.max(drawdown)

        # í–‰ë™ ë¶„í¬
        action_distribution = {}
        for action in actions_taken:
            if action < -0.3:
                action_distribution["reduce_traditional"] = (
                    action_distribution.get("reduce_traditional", 0) + 1
                )
            elif action > 0.3:
                action_distribution["enhance_traditional"] = (
                    action_distribution.get("enhance_traditional", 0) + 1
                )
            else:
                action_distribution["neutral"] = (
                    action_distribution.get("neutral", 0) + 1
                )

        # ê²°ê³¼ ìš”ì•½
        results_summary = {
            "method": "HYBRID",
            "symbol": symbol,
            "test_period": f"{test_data['datetime'].min()} ~ {test_data['datetime'].max()}",
            "test_data_points": len(test_data),
            "strategy_return": strategy_return,
            "buy_hold_return": buy_hold_return,
            "excess_return": strategy_return - buy_hold_return,
            "sharpe_ratio": sharpe_ratio,
            "max_drawdown": max_drawdown,
            "total_trades": len([a for a in actions_taken if abs(a) > 0.1]),
            "action_distribution": action_distribution,
            "final_portfolio_value": final_value,
            "portfolio_values": portfolio_values,
            "traditional_signals_used": traditional_signals_used,
        }

        self.backtest_results = results_summary

        # ê²°ê³¼ ì¶œë ¥
        logger.info("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ë°±í…ŒìŠ¤íŒ… ê²°ê³¼:")
        logger.info(f"  ğŸ’° ì „ëµ ìˆ˜ìµë¥ : {results_summary['strategy_return']:.2%}")
        logger.info(f"  ğŸ“ˆ Buy & Hold ìˆ˜ìµë¥ : {results_summary['buy_hold_return']:.2%}")
        logger.info(f"  âš¡ ì´ˆê³¼ ìˆ˜ìµë¥ : {results_summary['excess_return']:.2%}")
        logger.info(f"  ğŸ“ ìƒ¤í”„ ë¹„ìœ¨: {results_summary['sharpe_ratio']:.3f}")
        logger.info(f"  ğŸ“‰ ìµœëŒ€ ë‚™í­: {results_summary['max_drawdown']:.2%}")
        logger.info(f"  ğŸ”„ ì´ ê±°ë˜ ìˆ˜: {results_summary['total_trades']}")

        return self.backtest_results

    def evaluate_model(
        self,
        model: Any,
        test_data: pd.DataFrame,
        method: str = "DQN",
        symbol: str = "SPY",
    ) -> Dict[str, Any]:
        """ëª¨ë¸ í‰ê°€ (í†µí•© ì¸í„°í˜ì´ìŠ¤)"""
        if method.upper() == "DQN":
            return self.evaluate_dqn_model(model, test_data, symbol)
        elif method.upper() == "PPO":
            return self.evaluate_ppo_model(model, test_data, symbol)
        elif method.upper() == "HYBRID":
            return self.evaluate_hybrid_model(model, test_data, symbol)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•ë¡ : {method}")

    def visualize_results(self, save_plots: bool = True):
        """ê²°ê³¼ ì‹œê°í™”"""
        logger.info("ğŸ“Š ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")

        method = self.training_results.get("method", "Unknown")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(
            f"SPY {method} ê°•í™”í•™ìŠµ í€€íŠ¸ë§¤ë§¤ ê²°ê³¼ - {self.timestamp}",
            fontsize=16,
            fontweight="bold",
        )

        # 1. í›ˆë ¨ ê³¼ì • (ì—í”¼ì†Œë“œë³„ ë³´ìƒ)
        if "episode_rewards" in self.training_results:
            axes[0, 0].plot(self.training_results["episode_rewards"], alpha=0.7)
            axes[0, 0].plot(
                pd.Series(self.training_results["episode_rewards"]).rolling(100).mean(),
                color="red",
                linewidth=2,
                label="100-Episode Average",
            )
            axes[0, 0].set_title(f"{method} í›ˆë ¨ ê³¼ì •: ì—í”¼ì†Œë“œë³„ ë³´ìƒ")
            axes[0, 0].set_xlabel("Episode")
            axes[0, 0].set_ylabel("Total Reward")
            axes[0, 0].legend()
            axes[0, 0].grid(True)

        # 2. í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë³€í™”
        if "portfolio_values" in self.backtest_results:
            portfolio_values = self.backtest_results["portfolio_values"]

            # Buy & Hold ë¹„êµì„  ì¶”ê°€
            initial_value = 100000  # ì´ˆê¸° ìë³¸
            buy_hold_values = [
                initial_value
                * (
                    1
                    + self.backtest_results["buy_hold_return"]
                    * i
                    / len(portfolio_values)
                )
                for i in range(len(portfolio_values))
            ]

            axes[0, 1].plot(portfolio_values, label=f"{method} ì „ëµ", linewidth=2)
            axes[0, 1].plot(buy_hold_values, label="Buy & Hold", linewidth=2, alpha=0.7)
            axes[0, 1].set_title("í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë³€í™”")
            axes[0, 1].set_xlabel("Time Steps")
            axes[0, 1].set_ylabel("Portfolio Value ($)")
            axes[0, 1].legend()
            axes[0, 1].grid(True)

        # 3. í–‰ë™ ë¶„í¬
        if "action_distribution" in self.backtest_results:
            action_dist = self.backtest_results["action_distribution"]

            if method == "DQN":
                action_labels = {0: "ë§¤ë„/í˜„ê¸ˆ", 1: "ë³´ìœ ", 2: "ë§¤ìˆ˜"}
                labels = [
                    action_labels.get(k, f"Action {k}") for k in action_dist.keys()
                ]
            else:  # PPO
                labels = list(action_dist.keys())

            values = list(action_dist.values())

            axes[1, 0].pie(values, labels=labels, autopct="%1.1f%%", startangle=90)
            axes[1, 0].set_title(f"{method} í–‰ë™ ë¶„í¬")

        # 4. ì†ì‹¤ í•¨ìˆ˜ (í•™ìŠµ ê³¼ì •)
        if "loss_history" in self.training_results:
            loss_history = self.training_results["loss_history"]
            if loss_history:
                axes[1, 1].plot(loss_history, alpha=0.7)
                axes[1, 1].plot(
                    pd.Series(loss_history).rolling(100).mean(),
                    color="red",
                    linewidth=2,
                    label="100-Step Average",
                )
                axes[1, 1].set_title(f"{method} í•™ìŠµ ê³¼ì •: ì†ì‹¤ í•¨ìˆ˜")
                axes[1, 1].set_xlabel("Training Steps")
                axes[1, 1].set_ylabel("Loss")
                axes[1, 1].legend()
                axes[1, 1].grid(True)

        plt.tight_layout()

        if save_plots:
            plot_path = os.path.join(
                self.results_dir,
                f"{method.lower()}_trading_results_{self.timestamp}.png",
            )
            plt.savefig(plot_path, dpi=300, bbox_inches="tight")
            logger.info(f"ğŸ“Š ì°¨íŠ¸ ì €ì¥: {plot_path}")

        plt.show()

    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")

        method = self.training_results.get("method", "Unknown")

        # ì „ì²´ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        all_results = {
            "timestamp": self.timestamp,
            "method": method,
            "training_results": self.training_results,
            "backtest_results": self.backtest_results,
            "config": self.config,
        }

        # JSON ì €ì¥
        results_path = os.path.join(
            self.results_dir, f"{method.lower()}_trading_results_{self.timestamp}.json"
        )
        with open(results_path, "w", encoding="utf-8") as f:
            # NumPy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸°
            def convert_numpy(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, dict):
                    return {key: convert_numpy(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy(item) for item in obj]
                else:
                    return obj

            json.dump(convert_numpy(all_results), f, indent=2, ensure_ascii=False)

        # ìš”ì•½ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
        report_path = os.path.join(
            self.results_dir, f"{method.lower()}_trading_report_{self.timestamp}.txt"
        )
        self._generate_text_report(report_path)

        logger.info(f"ğŸ“„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  ğŸ“Š JSON: {results_path}")
        logger.info(f"  ğŸ“ ë¦¬í¬íŠ¸: {report_path}")

    def _generate_text_report(self, report_path: str):
        """í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        method = self.training_results.get("method", "Unknown")

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write(f"SPY {method} ê°•í™”í•™ìŠµ í€€íŠ¸ë§¤ë§¤ ê²°ê³¼ ë¦¬í¬íŠ¸\n")
            f.write("=" * 80 + "\n")
            f.write(f"ìƒì„± ì‹œê°„: {self.timestamp}\n")
            f.write(f"ì‚¬ìš© ë°©ë²•ë¡ : {method}\n\n")

            # í›ˆë ¨ ê²°ê³¼
            if self.training_results:
                f.write("ğŸ¤– í›ˆë ¨ ê²°ê³¼\n")
                f.write("-" * 40 + "\n")
                f.write(
                    f"ì´ ì—í”¼ì†Œë“œ: {self.training_results.get('episodes', 'N/A')}\n"
                )
                if method == "DQN":
                    f.write(
                        f"ìµœì¢… Epsilon: {self.training_results.get('final_epsilon', 'N/A'):.4f}\n"
                    )
                f.write(
                    f"í›ˆë ¨ ë°ì´í„° í¬ì¸íŠ¸: {self.training_results.get('train_data_points', 'N/A')}\n"
                )
                f.write(
                    f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {self.training_results.get('model_path', 'N/A')}\n\n"
                )

            # ë°±í…ŒìŠ¤íŒ… ê²°ê³¼
            if self.backtest_results:
                f.write("ğŸ“ˆ ë°±í…ŒìŠ¤íŒ… ê²°ê³¼\n")
                f.write("-" * 40 + "\n")
                f.write(
                    f"í…ŒìŠ¤íŠ¸ ê¸°ê°„: {self.backtest_results.get('test_period', 'N/A')}\n"
                )
                f.write(
                    f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ì¸íŠ¸: {self.backtest_results.get('test_data_points', 'N/A')}\n\n"
                )

                f.write("ğŸ’° ì„±ê³¼ ì§€í‘œ\n")
                f.write(
                    f"  ì „ëµ ìˆ˜ìµë¥ : {self.backtest_results.get('strategy_return', 0):.2%}\n"
                )
                f.write(
                    f"  Buy & Hold ìˆ˜ìµë¥ : {self.backtest_results.get('buy_hold_return', 0):.2%}\n"
                )
                f.write(
                    f"  ì´ˆê³¼ ìˆ˜ìµë¥ : {self.backtest_results.get('excess_return', 0):.2%}\n"
                )
                f.write(
                    f"  ìƒ¤í”„ ë¹„ìœ¨: {self.backtest_results.get('sharpe_ratio', 0):.3f}\n"
                )
                f.write(
                    f"  ìµœëŒ€ ë‚™í­: {self.backtest_results.get('max_drawdown', 0):.2%}\n"
                )
                f.write(
                    f"  ì´ ê±°ë˜ ìˆ˜: {self.backtest_results.get('total_trades', 0)}\n"
                )
                f.write(
                    f"  ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ${self.backtest_results.get('final_portfolio_value', 0):,.2f}\n\n"
                )

                # í–‰ë™ ë¶„í¬
                if "action_distribution" in self.backtest_results:
                    f.write("ğŸ¯ í–‰ë™ ë¶„í¬\n")
                    if method == "DQN":
                        action_labels = {0: "ë§¤ë„/í˜„ê¸ˆ", 1: "ë³´ìœ ", 2: "ë§¤ìˆ˜"}
                        for action, count in self.backtest_results[
                            "action_distribution"
                        ].items():
                            label = action_labels.get(action, f"Action {action}")
                            f.write(f"  {label}: {count}íšŒ\n")
                    else:  # PPO
                        for action, count in self.backtest_results[
                            "action_distribution"
                        ].items():
                            f.write(f"  {action}: {count}íšŒ\n")

    def load_trained_model(self, model_path: str, method: str = "DQN") -> Optional[Any]:
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            if method.upper() == "DQN":
                strategy = DQNRLTradingStrategy(StrategyParams(), model_path)
                strategy.load_trained_model(model_path)
                logger.info(f"âœ… DQN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
                return strategy.agent
            elif method.upper() == "PPO":
                # PPO ëª¨ë¸ ë¡œë“œ ë¡œì§
                trainer = PPOTrainer(0, 0, action_type="continuous")  # ë”ë¯¸ ê°’
                trainer.load_model(model_path)
                logger.info(f"âœ… PPO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
                return trainer
            elif method.upper() == "HYBRID":
                # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë“œ ë¡œì§
                trainer = HybridPPOTrainer(0, 0, action_type="continuous")  # ë”ë¯¸ ê°’
                trainer.load_model(model_path)
                logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
                return trainer
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•ë¡ : {method}")
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def run_full_pipeline(
        self,
        episodes: int = 1000,
        train_ratio: float = 0.8,
        method: str = "DQN",
        symbol: str = "SPY",
    ) -> bool:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info(f"ğŸ¯ {method} ê°•í™”í•™ìŠµ í€€íŠ¸ë§¤ë§¤ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

        try:
            # 1. ë°ì´í„° ë¡œë“œ
            data = self.load_spy_data(symbol)
            if data is None:
                return False

            # 2. ë°ì´í„° ë¶„í• 
            train_data, test_data = self.split_data(data, train_ratio)

            # 3. ëª¨ë¸ í›ˆë ¨
            model, episode_rewards = self.train_model(
                train_data, episodes, method, symbol
            )

            # 4. ëª¨ë¸ í‰ê°€
            backtest_results = self.evaluate_model(model, test_data, method, symbol)

            # 5. ê²°ê³¼ ì‹œê°í™”
            self.visualize_results(save_plots=True)

            # 6. ê²°ê³¼ ì €ì¥
            self.save_results()

            logger.info(f"ğŸ‰ {method} ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            return True

        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="SPY ê°•í™”í•™ìŠµ í€€íŠ¸ë§¤ë§¤ ì‹œìŠ¤í…œ")
    parser.add_argument("--episodes", type=int, default=1000, help="í›ˆë ¨ ì—í”¼ì†Œë“œ ìˆ˜")
    parser.add_argument("--symbol", type=str, default="SPY", help="ê±°ë˜í•  ì¢…ëª©")
    parser.add_argument(
        "--train-ratio", type=float, default=0.8, help="í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="config/config_default.json",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["train", "evaluate", "full"],
        default="full",
        help="ì‹¤í–‰ ëª¨ë“œ",
    )
    parser.add_argument(
        "--method",
        type=str,
        choices=["DQN", "PPO", "HYBRID"],
        default="DQN",
        help="ê°•í™”í•™ìŠµ ë°©ë²•ë¡ ",
    )
    parser.add_argument(
        "--model-path", type=str, help="ë¡œë“œí•  ëª¨ë¸ ê²½ë¡œ (evaluate ëª¨ë“œìš©)"
    )

    args = parser.parse_args()

    # ë¡œê±° ì´ˆê¸°í™”
    learner = RLQuantLearner(args.config)

    if args.mode == "full":
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        success = learner.run_full_pipeline(
            episodes=args.episodes,
            train_ratio=args.train_ratio,
            method=args.method,
            symbol=args.symbol,
        )
        if success:
            print(f"âœ… {args.method} ê°•í™”í•™ìŠµ í€€íŠ¸ë§¤ë§¤ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ!")
        else:
            print("âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    elif args.mode == "train":
        # í›ˆë ¨ë§Œ ì‹¤í–‰
        data = learner.load_spy_data(args.symbol)
        if data is not None:
            train_data, _ = learner.split_data(data, args.train_ratio)
            model, episode_rewards = learner.train_model(
                train_data, args.episodes, args.method, args.symbol
            )
            print(f"âœ… {args.method} ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

    elif args.mode == "evaluate":
        # í‰ê°€ë§Œ ì‹¤í–‰
        if not args.model_path:
            print("âŒ evaluate ëª¨ë“œì—ì„œëŠ” --model-pathê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return

        model = learner.load_trained_model(args.model_path, args.method)
        if model:
            data = learner.load_spy_data(args.symbol)
            if data is not None:
                _, test_data = learner.split_data(data, args.train_ratio)
                backtest_results = learner.evaluate_model(
                    model, test_data, args.method, args.symbol
                )
                learner.visualize_results(save_plots=True)
                print(f"âœ… {args.method} ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
