#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª¨ë“ˆ
ê·¸ë¦¬ë“œ ì„œì¹˜, ë² ì´ì§€ì•ˆ ìµœì í™”, ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ë“± ë‹¤ì–‘í•œ ìµœì í™” ë°©ë²• ì œê³µ
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Callable
import itertools
import random
import time
import json
import os
from datetime import datetime
import warnings
from dataclasses import dataclass
import logging

warnings.filterwarnings("ignore")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class OptimizationResult:
    """ìµœì í™” ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„°í´ë˜ìŠ¤"""

    strategy_name: str
    best_params: Dict[str, Any]
    best_score: float
    all_results: List[Dict[str, Any]]
    optimization_method: str
    execution_time: float
    n_combinations_tested: int
    symbol: str = None


class HyperparameterOptimizer:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í´ë˜ìŠ¤"""

    def __init__(self, config_path: str = "config/config_research.json"):
        self.config = self._load_config(config_path)
        self.results = {}
        self.logger = logging.getLogger(__name__)

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.error(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            return {}
        except json.JSONDecodeError:
            self.logger.error(f"ì„¤ì • íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {config_path}")
            return {}

    def generate_parameter_combinations(
        self,
        param_ranges: Dict[str, List],
        max_combinations: int = 50,
        random_sampling: bool = True,
        sampling_ratio: float = 0.3,
    ) -> List[Dict[str, Any]]:
        """íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±"""

        # ëª¨ë“  ê°€ëŠ¥í•œ ì¡°í•© ìƒì„±
        param_names = list(param_ranges.keys())
        param_values = list(param_ranges.values())

        all_combinations = list(itertools.product(*param_values))
        total_combinations = len(all_combinations)

        self.logger.info(f"ì´ ê°€ëŠ¥í•œ ì¡°í•© ìˆ˜: {total_combinations}")

        if random_sampling and total_combinations > max_combinations:
            # ëœë¤ ìƒ˜í”Œë§
            sample_size = min(
                int(total_combinations * sampling_ratio), max_combinations
            )
            selected_combinations = random.sample(all_combinations, sample_size)
            self.logger.info(f"ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ {sample_size}ê°œ ì¡°í•© ì„ íƒ")
        else:
            # ì „ì²´ ì¡°í•© ë˜ëŠ” ìµœëŒ€ ì¡°í•© ìˆ˜ë§Œí¼
            selected_combinations = all_combinations[:max_combinations]
            self.logger.info(f"ì „ì²´ ì¡°í•© ì¤‘ {len(selected_combinations)}ê°œ ì‚¬ìš©")

        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        combinations = []
        for combination in selected_combinations:
            param_dict = dict(zip(param_names, combination))
            combinations.append(param_dict)

        return combinations

    def grid_search(
        self,
        strategy_name: str,
        param_ranges: Dict[str, List],
        evaluation_function: Callable,
        max_combinations: int = 50,
        random_sampling: bool = True,
        sampling_ratio: float = 0.3,
        timeout_per_combination: int = 300,
    ) -> OptimizationResult:
        """ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”"""

        self.logger.info(f"ğŸ” {strategy_name} ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹œì‘")
        start_time = time.time()

        # íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
        combinations = self.generate_parameter_combinations(
            param_ranges, max_combinations, random_sampling, sampling_ratio
        )

        best_score = float("-inf")
        best_params = None
        all_results = []

        for i, params in enumerate(combinations):
            try:
                self.logger.info(f"  í…ŒìŠ¤íŠ¸ {i+1}/{len(combinations)}: {params}")

                # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                start_eval = time.time()
                score = evaluation_function(params)
                eval_time = time.time() - start_eval

                result = {
                    "params": params,
                    "score": score,
                    "evaluation_time": eval_time,
                    "combination_index": i,
                }
                all_results.append(result)

                if score > best_score:
                    best_score = score
                    best_params = params
                    self.logger.info(f"    ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜: {score:.4f}")

            except Exception as e:
                self.logger.warning(
                    f"    âš ï¸ íŒŒë¼ë¯¸í„° ì¡°í•© {params} í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                )
                continue

        execution_time = time.time() - start_time

        result = OptimizationResult(
            strategy_name=strategy_name,
            best_params=best_params,
            best_score=best_score,
            all_results=all_results,
            optimization_method="grid_search",
            execution_time=execution_time,
            n_combinations_tested=len(all_results),
        )

        self.logger.info(f"âœ… {strategy_name} ê·¸ë¦¬ë“œ ì„œì¹˜ ì™„ë£Œ")
        self.logger.info(f"   ìµœê³  ì ìˆ˜: {best_score:.4f}")
        self.logger.info(f"   ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        self.logger.info(f"   ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")

        return result

    def bayesian_optimization(
        self,
        strategy_name: str,
        param_ranges: Dict[str, List],
        evaluation_function: Callable,
        n_trials: int = 100,
        n_startup_trials: int = 10,
    ) -> OptimizationResult:
        """ë² ì´ì§€ì•ˆ ìµœì í™” (Optuna ì‚¬ìš©)"""

        try:
            import optuna
        except ImportError:
            self.logger.error(
                "Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install optunaë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
            )
            return None

        self.logger.info(f"ğŸ” {strategy_name} ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘")
        start_time = time.time()

        def objective(trial):
            # íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
            params = {}
            for param_name, param_values in param_ranges.items():
                if isinstance(param_values[0], int):
                    params[param_name] = trial.suggest_int(
                        param_name, min(param_values), max(param_values)
                    )
                elif isinstance(param_values[0], float):
                    params[param_name] = trial.suggest_float(
                        param_name, min(param_values), max(param_values)
                    )
                else:
                    params[param_name] = trial.suggest_categorical(
                        param_name, param_values
                    )

            try:
                return evaluation_function(params)
            except Exception as e:
                self.logger.warning(f"ë² ì´ì§€ì•ˆ ìµœì í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return float("-inf")

        # Optuna ìŠ¤í„°ë”” ìƒì„±
        study = optuna.create_study(direction="maximize")
        study.optimize(objective, n_trials=n_trials, n_startup_trials=n_startup_trials)

        execution_time = time.time() - start_time

        # ê²°ê³¼ ë³€í™˜
        all_results = []
        for trial in study.trials:
            if trial.value is not None:
                result = {
                    "params": trial.params,
                    "score": trial.value,
                    "evaluation_time": trial.duration,
                    "combination_index": trial.number,
                }
                all_results.append(result)

        result = OptimizationResult(
            strategy_name=strategy_name,
            best_params=study.best_params,
            best_score=study.best_value,
            all_results=all_results,
            optimization_method="bayesian_optimization",
            execution_time=execution_time,
            n_combinations_tested=len(all_results),
        )

        self.logger.info(f"âœ… {strategy_name} ë² ì´ì§€ì•ˆ ìµœì í™” ì™„ë£Œ")
        self.logger.info(f"   ìµœê³  ì ìˆ˜: {study.best_value:.4f}")
        self.logger.info(f"   ìµœì  íŒŒë¼ë¯¸í„°: {study.best_params}")
        self.logger.info(f"   ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")

        return result

    def genetic_algorithm(
        self,
        strategy_name: str,
        param_ranges: Dict[str, List],
        evaluation_function: Callable,
        population_size: int = 50,
        generations: int = 20,
        mutation_rate: float = 0.1,
        crossover_rate: float = 0.8,
    ) -> OptimizationResult:
        """ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”"""

        self.logger.info(f"ğŸ” {strategy_name} ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ì‹œì‘")
        start_time = time.time()

        # ì´ˆê¸° ê°œì²´êµ° ìƒì„±
        population = self._generate_initial_population(param_ranges, population_size)
        best_score = float("-inf")
        best_params = None
        all_results = []

        for generation in range(generations):
            self.logger.info(f"  ì„¸ëŒ€ {generation + 1}/{generations}")

            # ì í•©ë„ í‰ê°€
            fitness_scores = []
            for individual in population:
                try:
                    score = evaluation_function(individual)
                    fitness_scores.append(score)

                    result = {
                        "params": individual,
                        "score": score,
                        "generation": generation,
                        "combination_index": len(all_results),
                    }
                    all_results.append(result)

                    if score > best_score:
                        best_score = score
                        best_params = individual
                        self.logger.info(f"    ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜: {score:.4f}")

                except Exception as e:
                    fitness_scores.append(float("-inf"))
                    self.logger.warning(f"    âš ï¸ ê°œì²´ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")

            # ì„ íƒ, êµì°¨, ëŒì—°ë³€ì´
            if generation < generations - 1:  # ë§ˆì§€ë§‰ ì„¸ëŒ€ê°€ ì•„ë‹ˆë©´
                population = self._evolve_population(
                    population,
                    fitness_scores,
                    param_ranges,
                    mutation_rate,
                    crossover_rate,
                )

        execution_time = time.time() - start_time

        result = OptimizationResult(
            strategy_name=strategy_name,
            best_params=best_params,
            best_score=best_score,
            all_results=all_results,
            optimization_method="genetic_algorithm",
            execution_time=execution_time,
            n_combinations_tested=len(all_results),
        )

        self.logger.info(f"âœ… {strategy_name} ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ì™„ë£Œ")
        self.logger.info(f"   ìµœê³  ì ìˆ˜: {best_score:.4f}")
        self.logger.info(f"   ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        self.logger.info(f"   ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")

        return result

    def _generate_initial_population(
        self, param_ranges: Dict[str, List], population_size: int
    ) -> List[Dict[str, Any]]:
        """ì´ˆê¸° ê°œì²´êµ° ìƒì„±"""
        population = []
        for _ in range(population_size):
            individual = {}
            for param_name, param_values in param_ranges.items():
                individual[param_name] = random.choice(param_values)
            population.append(individual)
        return population

    def _evolve_population(
        self,
        population: List[Dict[str, Any]],
        fitness_scores: List[float],
        param_ranges: Dict[str, List],
        mutation_rate: float,
        crossover_rate: float,
    ) -> List[Dict[str, Any]]:
        """ê°œì²´êµ° ì§„í™”"""
        new_population = []

        # ì—˜ë¦¬íŠ¸ ì„ íƒ (ìƒìœ„ 10% ë³´ì¡´)
        elite_size = max(1, len(population) // 10)
        elite_indices = np.argsort(fitness_scores)[-elite_size:]
        for idx in elite_indices:
            new_population.append(population[idx])

        # ë‚˜ë¨¸ì§€ ê°œì²´ë“¤ì€ ì„ íƒ, êµì°¨, ëŒì—°ë³€ì´ë¡œ ìƒì„±
        while len(new_population) < len(population):
            # í† ë„ˆë¨¼íŠ¸ ì„ íƒ
            parent1 = self._tournament_selection(population, fitness_scores)
            parent2 = self._tournament_selection(population, fitness_scores)

            # êµì°¨
            if random.random() < crossover_rate:
                child = self._crossover(parent1, parent2)
            else:
                child = parent1.copy()

            # ëŒì—°ë³€ì´
            if random.random() < mutation_rate:
                child = self._mutate(child, param_ranges)

            new_population.append(child)

        return new_population

    def _tournament_selection(
        self,
        population: List[Dict[str, Any]],
        fitness_scores: List[float],
        tournament_size: int = 3,
    ) -> Dict[str, Any]:
        """í† ë„ˆë¨¼íŠ¸ ì„ íƒ"""
        tournament_indices = random.sample(range(len(population)), tournament_size)
        tournament_fitness = [fitness_scores[i] for i in tournament_indices]
        winner_idx = tournament_indices[np.argmax(tournament_fitness)]
        return population[winner_idx]

    def _crossover(
        self, parent1: Dict[str, Any], parent2: Dict[str, Any]
    ) -> Dict[str, Any]:
        """êµì°¨ ì—°ì‚°"""
        child = {}
        for param_name in parent1.keys():
            if random.random() < 0.5:
                child[param_name] = parent1[param_name]
            else:
                child[param_name] = parent2[param_name]
        return child

    def _mutate(
        self, individual: Dict[str, Any], param_ranges: Dict[str, List]
    ) -> Dict[str, Any]:
        """ëŒì—°ë³€ì´ ì—°ì‚°"""
        mutated = individual.copy()
        param_name = random.choice(list(param_ranges.keys()))
        mutated[param_name] = random.choice(param_ranges[param_name])
        return mutated

    def optimize_strategy(
        self,
        strategy_name: str,
        param_ranges: Dict[str, List],
        evaluation_function: Callable,
        optimization_method: str = "grid_search",
        **kwargs,
    ) -> OptimizationResult:
        """ì „ëµ ìµœì í™” ì‹¤í–‰"""

        if optimization_method == "grid_search":
            return self.grid_search(
                strategy_name, param_ranges, evaluation_function, **kwargs
            )
        elif optimization_method == "bayesian_optimization":
            return self.bayesian_optimization(
                strategy_name, param_ranges, evaluation_function, **kwargs
            )
        elif optimization_method == "genetic_algorithm":
            return self.genetic_algorithm(
                strategy_name, param_ranges, evaluation_function, **kwargs
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìµœì í™” ë°©ë²•: {optimization_method}")

    def save_results(
        self, results: List[OptimizationResult], output_dir: str = "results"
    ):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSON í˜•íƒœë¡œ ì €ì¥
        results_dict = []
        for result in results:
            result_dict = {
                "strategy_name": result.strategy_name,
                "best_params": result.best_params,
                "best_score": result.best_score,
                "optimization_method": result.optimization_method,
                "execution_time": result.execution_time,
                "n_combinations_tested": result.n_combinations_tested,
                "symbol": result.symbol,
                "all_results": result.all_results,
            }
            results_dict.append(result_dict)

        json_path = os.path.join(output_dir, f"optimization_results_{timestamp}.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)

        # CSV í˜•íƒœë¡œ ì €ì¥ (ìš”ì•½)
        summary_data = []
        for result in results:
            summary_data.append(
                {
                    "strategy_name": result.strategy_name,
                    "best_score": result.best_score,
                    "optimization_method": result.optimization_method,
                    "execution_time": result.execution_time,
                    "n_combinations_tested": result.n_combinations_tested,
                    "symbol": result.symbol,
                    "best_params": str(result.best_params),
                }
            )

        df = pd.DataFrame(summary_data)
        csv_path = os.path.join(output_dir, f"optimization_summary_{timestamp}.csv")
        df.to_csv(csv_path, index=False, encoding="utf-8")

        self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        self.logger.info(f"  JSON: {json_path}")
        self.logger.info(f"  CSV: {csv_path}")

        return json_path, csv_path

    def generate_optimization_report(self, results: List[OptimizationResult]) -> str:
        """ìµœì í™” ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""

        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ğŸ”¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ ë¦¬í¬íŠ¸")
        report_lines.append("=" * 80)

        # ì „ì²´ ìš”ì•½
        total_time = sum(r.execution_time for r in results)
        total_combinations = sum(r.n_combinations_tested for r in results)

        report_lines.append(f"\nğŸ“Š ì „ì²´ ìš”ì•½:")
        report_lines.append(f"  ì´ ì „ëµ ìˆ˜: {len(results)}")
        report_lines.append(f"  ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        report_lines.append(f"  ì´ í…ŒìŠ¤íŠ¸ ì¡°í•© ìˆ˜: {total_combinations}")

        # ì „ëµë³„ ê²°ê³¼
        report_lines.append(f"\nğŸ† ì „ëµë³„ ìµœì í™” ê²°ê³¼:")
        report_lines.append("-" * 80)
        report_lines.append(
            f"{'ì „ëµëª…':<25} {'ìµœì í™”ë°©ë²•':<20} {'ìµœê³ ì ìˆ˜':<12} {'ì‹¤í–‰ì‹œê°„':<10} {'í…ŒìŠ¤íŠ¸ìˆ˜':<8}"
        )
        report_lines.append("-" * 80)

        # ì ìˆ˜ë³„ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: x.best_score, reverse=True)

        for result in sorted_results:
            report_lines.append(
                f"{result.strategy_name:<25} {result.optimization_method:<20} "
                f"{result.best_score:<12.4f} {result.execution_time:<10.2f} "
                f"{result.n_combinations_tested:<8}"
            )

        # ìµœê³  ì„±ê³¼ ì „ëµ ìƒì„¸
        if sorted_results:
            best_result = sorted_results[0]
            report_lines.append(f"\nğŸ¥‡ ìµœê³  ì„±ê³¼ ì „ëµ: {best_result.strategy_name}")
            report_lines.append("-" * 40)
            report_lines.append(f"ìµœì í™” ë°©ë²•: {best_result.optimization_method}")
            report_lines.append(f"ìµœê³  ì ìˆ˜: {best_result.best_score:.4f}")
            report_lines.append(f"ì‹¤í–‰ ì‹œê°„: {best_result.execution_time:.2f}ì´ˆ")
            report_lines.append(f"í…ŒìŠ¤íŠ¸ ì¡°í•© ìˆ˜: {best_result.n_combinations_tested}")
            report_lines.append(f"ìµœì  íŒŒë¼ë¯¸í„°:")
            for param, value in best_result.best_params.items():
                report_lines.append(f"  {param}: {value}")

        # ìµœì í™” ë°©ë²•ë³„ í†µê³„
        method_stats = {}
        for result in results:
            method = result.optimization_method
            if method not in method_stats:
                method_stats[method] = {
                    "count": 0,
                    "total_time": 0,
                    "avg_score": 0,
                    "scores": [],
                }
            method_stats[method]["count"] += 1
            method_stats[method]["total_time"] += result.execution_time
            method_stats[method]["scores"].append(result.best_score)

        report_lines.append(f"\nğŸ“ˆ ìµœì í™” ë°©ë²•ë³„ í†µê³„:")
        report_lines.append("-" * 50)
        for method, stats in method_stats.items():
            avg_score = np.mean(stats["scores"])
            avg_time = stats["total_time"] / stats["count"]
            report_lines.append(f"{method}:")
            report_lines.append(f"  ì „ëµ ìˆ˜: {stats['count']}")
            report_lines.append(f"  í‰ê·  ì ìˆ˜: {avg_score:.4f}")
            report_lines.append(f"  í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.2f}ì´ˆ")

        return "\n".join(report_lines)


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""

    # í…ŒìŠ¤íŠ¸ìš© í‰ê°€ í•¨ìˆ˜
    def test_evaluation_function(params):
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ì‹¤ì œë¡œëŠ” ì „ëµ í‰ê°€ í•¨ìˆ˜ê°€ ë“¤ì–´ê°)
        score = 0
        for param_name, value in params.items():
            if "period" in param_name:
                score += value * 0.1
            elif "threshold" in param_name:
                score += value * 0.2
        return score + random.random() * 0.1  # ì•½ê°„ì˜ ëœë¤ì„± ì¶”ê°€

    # ìµœì í™”ê¸° ì´ˆê¸°í™”
    optimizer = HyperparameterOptimizer()

    # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° ë²”ìœ„
    test_param_ranges = {
        "period": [10, 20, 30],
        "threshold": [0.1, 0.2, 0.3],
        "multiplier": [1.0, 1.5, 2.0],
    }

    # ê·¸ë¦¬ë“œ ì„œì¹˜ í…ŒìŠ¤íŠ¸
    result = optimizer.grid_search(
        "test_strategy",
        test_param_ranges,
        test_evaluation_function,
        max_combinations=10,
    )

    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
