"""
ì‹ ê²½ë§ ê¸°ë°˜ ì£¼ì‹ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ì •ë¦¬ëœ ë²„ì „)
- ë™ì  í”¼ì²˜ ìƒì„± ì§€ì›
- FeatureEngineeringPipeline í†µí•©
- NaN ì²˜ë¦¬ ê°œì„ 
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import joblib
import json
import os
import sys
import warnings
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Union
from sklearn.preprocessing import StandardScaler

# FeatureEngineeringPipeline ì„í¬íŠ¸
try:
    from .feature_engineering import FeatureEngineeringPipeline
except ImportError:
    try:
        sys.path.append(os.path.dirname(__file__))
        from feature_engineering import FeatureEngineeringPipeline
    except ImportError:
        FeatureEngineeringPipeline = None
        print(
            "FeatureEngineeringPipelineì„ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”¼ì²˜ ìƒì„± ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)


class SimpleStockPredictor(nn.Module):
    """ë‹¨ìˆœí•œ ì£¼ì‹ ì˜ˆì¸¡ ì‹ ê²½ë§"""

    def __init__(
        self,
        input_size: int,
        hidden_sizes: List[int] = [32, 16],
        dropout_rate: float = 0.2,
        output_size: int = 4,
    ):
        super().__init__()

        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend(
                [nn.Linear(prev_size, hidden_size), nn.ReLU(), nn.Dropout(dropout_rate)]
            )
            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, output_size))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


class StockDataset(Dataset):
    """ì£¼ì‹ ë°ì´í„°ì…‹"""

    def __init__(self, features: np.ndarray, targets: np.ndarray):
        self.features = torch.FloatTensor(features)
        self.targets = torch.FloatTensor(targets)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]


class StockPredictionNetwork:
    """ì‹ ê²½ë§ ê¸°ë°˜ ì£¼ì‹ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""

    def __init__(self, config: Dict):
        self.config = config
        self.neural_config = config.get("neural_network", {})

        # ì•™ìƒë¸” ì„¤ì •
        self.ensemble_config = self.neural_config.get(
            "ensemble",
            {
                "universal_weight": 0.7,
                "individual_weight": 0.3,
                "enable_individual_models": True,
            },
        )

        # ëª¨ë¸ë“¤
        self.universal_model = None
        self.individual_models = {}
        self.universal_scaler = StandardScaler()
        self.individual_scalers = {}

        # í”¼ì²˜ ì •ë³´
        self.feature_names = None
        self.target_columns = None

        # FeatureEngineeringPipeline ì´ˆê¸°í™”
        self.feature_pipeline = (
            FeatureEngineeringPipeline(config) if FeatureEngineeringPipeline else None
        )

        # í”¼ì²˜ ì •ë³´ ì €ì¥
        self.feature_info = {
            "universal_features": {},
            "individual_features": {},
            "macro_features": {},
            "feature_dimensions": {},
            "created_at": datetime.now().isoformat(),
        }

        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜
        self.universal_weight = self.ensemble_config.get("universal_weight", 0.7)
        self.individual_weight = self.ensemble_config.get("individual_weight", 0.3)
        self.enable_individual_models = self.ensemble_config.get(
            "enable_individual_models", True
        )

        logger.info(
            f"StockPredictionNetwork ì´ˆê¸°í™” ì™„ë£Œ - ì•™ìƒë¸” ëª¨ë“œ (Universal: {self.universal_weight}, Individual: {self.individual_weight})"
        )

    def _build_model(self, input_dim: int, output_size: int = 4) -> nn.Module:
        """ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¶•"""
        architecture = self.neural_config.get("architecture", {})
        hidden_sizes = architecture.get("hidden_layers", [32, 16])
        dropout_rate = architecture.get("dropout_rate", 0.2)

        model = SimpleStockPredictor(
            input_size=input_dim,
            hidden_sizes=hidden_sizes,
            dropout_rate=dropout_rate,
            output_size=output_size,
        )

        return model

    def create_features(
        self,
        stock_data: pd.DataFrame,
        symbol: str,
        market_regime: Dict,
        macro_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """ë™ì  í”¼ì²˜ ìƒì„±"""
        try:
            logger.info(f"ğŸ” {symbol} ë™ì  í”¼ì²˜ ìƒì„± ì‹œì‘...")
            logger.info(f"   - ì£¼ì‹ ë°ì´í„°: {stock_data.shape}")
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {len(stock_data.columns)}ê°œ")

            # FeatureEngineeringPipeline ì‚¬ìš©
            if self.feature_pipeline:
                logger.info(f"   ğŸ“Š FeatureEngineeringPipeline ì‚¬ìš©...")
                try:
                    all_features, feature_metadata = (
                        self.feature_pipeline.create_features(
                            stock_data,
                            symbol,
                            market_regime,
                            macro_data,
                            mode="individual",
                        )
                    )

                    # í”¼ì²˜ ì •ë³´ ì €ì¥
                    self.feature_info["individual_features"][symbol] = {
                        "total_features": len(all_features.columns),
                        "feature_names": list(all_features.columns),
                        "feature_stats": feature_metadata.get("feature_stats", {}),
                        "created_at": datetime.now().isoformat(),
                    }

                    logger.info(f"   âœ… ë™ì  í”¼ì²˜ ìƒì„± ì™„ë£Œ: {all_features.shape}")
                    logger.info(
                        f"   ğŸ“Š í”¼ì²˜ í†µê³„: {feature_metadata.get('feature_stats', {})}"
                    )

                    return all_features

                except Exception as e:
                    logger.error(f"   âŒ FeatureEngineeringPipeline ì‹¤íŒ¨: {e}")

            # ê¸°ë³¸ í”¼ì²˜ ìƒì„± (í˜¸í™˜ì„±)
            logger.info(f"   ğŸ“Š ê¸°ë³¸ í”¼ì²˜ ìƒì„±...")
            features = pd.DataFrame(index=stock_data.index)

            # ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œë“¤
            if all(
                col in stock_data.columns
                for col in ["open", "high", "low", "close", "volume"]
            ):
                features["dual_momentum"] = stock_data["close"].pct_change(
                    5
                ) - stock_data["close"].pct_change(20)
                features["volatility_breakout"] = (
                    stock_data["close"] - stock_data["close"].rolling(20).mean()
                ) / stock_data["close"].rolling(20).std()
                features["swing_ema"] = (
                    stock_data["close"].ewm(span=12).mean()
                    - stock_data["close"].ewm(span=26).mean()
                ) / stock_data["close"].ewm(span=26).mean()

            # ì‹œì¥ ì²´ì œ í”¼ì²˜
            regimes = ["BULLISH", "BEARISH", "SIDEWAYS", "VOLATILE"]
            current_regime = market_regime.get("current_regime", "SIDEWAYS")

            for regime in regimes:
                features[f"regime_{regime.lower()}"] = int(current_regime == regime)

            # NaN ì²˜ë¦¬
            features = features.fillna(method="ffill").fillna(method="bfill").fillna(0)

            logger.info(f"   âœ… ê¸°ë³¸ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {features.shape}")
            return features

        except Exception as e:
            logger.error(f"âŒ {symbol} í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame(index=stock_data.index)

    def prepare_training_data(
        self,
        features: pd.DataFrame,
        target: Union[pd.Series, pd.DataFrame, np.ndarray],
        lookback_days: int = 20,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        try:
            # í”¼ì²˜ ì •ê·œí™”
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)

            # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
            X, y = [], []

            for i in range(lookback_days, len(features_scaled)):
                X.append(features_scaled[i - lookback_days : i])
                if isinstance(target, pd.DataFrame):
                    y.append(target.iloc[i].values)
                else:
                    y.append(target[i])

            return np.array(X), np.array(y)

        except Exception as e:
            logger.error(f"í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return np.array([]), np.array([])

    def _train_universal_model(self, training_data: Dict) -> bool:
        """Universal ëª¨ë¸ í›ˆë ¨"""
        try:
            logger.info("ğŸ§  Universal ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

            # ëª¨ë“  ì¢…ëª©ì˜ í”¼ì²˜ í†µí•©
            all_features = []
            all_targets = []

            for symbol, data in training_data.items():
                features = data["features"]
                targets = data["targets"]

                if features is not None and targets is not None:
                    all_features.append(features)
                    all_targets.append(targets)

            if not all_features:
                logger.error("í›ˆë ¨í•  í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False

            # í”¼ì²˜ í†µí•©
            combined_features = pd.concat(all_features, axis=0)
            combined_targets = pd.concat(all_targets, axis=0)

            # í”¼ì²˜ ì •ê·œí™”
            self.universal_scaler.fit(combined_features)
            features_scaled = self.universal_scaler.transform(combined_features)

            # ëª¨ë¸ êµ¬ì¶•
            input_dim = features_scaled.shape[1]
            output_size = (
                combined_targets.shape[1] if len(combined_targets.shape) > 1 else 1
            )

            self.universal_model = self._build_model(input_dim, output_size)
            self.feature_names = list(combined_features.columns)
            self.target_columns = (
                list(combined_targets.columns)
                if hasattr(combined_targets, "columns")
                else None
            )

            # í›ˆë ¨ ì„¤ì •
            epochs = self.neural_config.get("epochs", 200)
            batch_size = self.neural_config.get("batch_size", 32)
            learning_rate = self.neural_config.get("learning_rate", 0.001)
            early_stopping_patience = self.neural_config.get(
                "early_stopping_patience", 20
            )

            # ë°ì´í„°ì…‹ ìƒì„±
            dataset = StockDataset(features_scaled, combined_targets.values)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

            # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
            optimizer = optim.Adam(self.universal_model.parameters(), lr=learning_rate)
            criterion = nn.MSELoss()

            # í›ˆë ¨ ë£¨í”„
            best_loss = float("inf")
            patience_counter = 0
            actual_epochs = 0

            for epoch in range(epochs):
                self.universal_model.train()
                total_loss = 0

                for batch_features, batch_targets in dataloader:
                    optimizer.zero_grad()
                    predictions = self.universal_model(batch_features)
                    loss = criterion(predictions, batch_targets)
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()

                avg_loss = total_loss / len(dataloader)

                # Early stopping
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience_counter = 0
                else:
                    patience_counter += 1

                if patience_counter >= early_stopping_patience:
                    logger.info(f"Early stopping at epoch {epoch}")
                    break

                actual_epochs = epoch + 1

            logger.info(
                f"âœ… Universal ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {actual_epochs} epochs, ìµœì¢… ì†ì‹¤: {best_loss:.6f}"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ Universal ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return False

    def _train_individual_models(self, training_data: Dict) -> bool:
        """ê°œë³„ ëª¨ë¸ í›ˆë ¨"""
        try:
            logger.info("ğŸ§  ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

            from tqdm import tqdm

            symbol_iter = tqdm(training_data.items(), desc="ê°œë³„ ëª¨ë¸ í›ˆë ¨")

            for symbol, data in symbol_iter:
                features = data["features"]
                targets = data["targets"]

                if features is None or targets is None:
                    continue

                try:
                    # í”¼ì²˜ ì •ê·œí™”
                    scaler = StandardScaler()
                    features_scaled = scaler.fit_transform(features)

                    # ëª¨ë¸ êµ¬ì¶•
                    input_dim = features_scaled.shape[1]
                    output_size = targets.shape[1] if len(targets.shape) > 1 else 1

                    model = self._build_model(input_dim, output_size)

                    # í›ˆë ¨ ì„¤ì •
                    epochs = self.neural_config.get("epochs", 200)
                    batch_size = self.neural_config.get("batch_size", 32)
                    learning_rate = self.neural_config.get("learning_rate", 0.001)
                    early_stopping_patience = self.neural_config.get(
                        "early_stopping_patience", 20
                    )

                    # ë°ì´í„°ì…‹ ìƒì„±
                    dataset = StockDataset(features_scaled, targets.values)
                    dataloader = DataLoader(
                        dataset, batch_size=batch_size, shuffle=True
                    )

                    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
                    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
                    criterion = nn.MSELoss()

                    # í›ˆë ¨ ë£¨í”„
                    best_loss = float("inf")
                    patience_counter = 0
                    actual_epochs = 0

                    for epoch in range(epochs):
                        model.train()
                        total_loss = 0

                        for batch_features, batch_targets in dataloader:
                            optimizer.zero_grad()
                            predictions = model(batch_features)
                            loss = criterion(predictions, batch_targets)
                            loss.backward()
                            optimizer.step()
                            total_loss += loss.item()

                        avg_loss = total_loss / len(dataloader)

                        # Early stopping
                        if avg_loss < best_loss:
                            best_loss = avg_loss
                            patience_counter = 0
                        else:
                            patience_counter += 1

                        if patience_counter >= early_stopping_patience:
                            break

                        actual_epochs = epoch + 1

                    # ëª¨ë¸ ì €ì¥
                    self.individual_models[symbol] = model
                    self.individual_scalers[symbol] = scaler

                    symbol_iter.set_postfix(
                        {
                            "symbol": symbol,
                            "final_train": f"{best_loss:.6f}",
                            "epochs": actual_epochs,
                        }
                    )

                except Exception as e:
                    logger.error(f"âŒ {symbol} ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
                    continue

            logger.info(f"âœ… ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(self.individual_models)}ê°œ ëª¨ë¸")
            return True

        except Exception as e:
            logger.error(f"âŒ ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return False

    def fit(self, training_data: Dict) -> bool:
        """ëª¨ë¸ í›ˆë ¨"""
        try:
            logger.info("ğŸš€ ì‹ ê²½ë§ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

            # Universal ëª¨ë¸ í›ˆë ¨
            if not self._train_universal_model(training_data):
                return False

            # ê°œë³„ ëª¨ë¸ í›ˆë ¨
            if self.enable_individual_models:
                if not self._train_individual_models(training_data):
                    logger.warning("ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨, Universal ëª¨ë¸ë§Œ ì‚¬ìš©")

            logger.info("âœ… ì‹ ê²½ë§ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return False

    def predict(
        self, features: pd.DataFrame, symbol: str
    ) -> Union[float, Dict[str, float]]:
        """ì˜ˆì¸¡"""
        try:
            if self.universal_model is None:
                logger.error("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return 0.0

            # í”¼ì²˜ ì •ê·œí™”
            features_scaled = self.universal_scaler.transform(features)

            # ì˜ˆì¸¡
            self.universal_model.eval()
            with torch.no_grad():
                predictions = self.universal_model(torch.FloatTensor(features_scaled))
                latest_pred = predictions[-1].numpy()

            # ê²°ê³¼ ë°˜í™˜
            if self.target_columns:
                return {
                    col: float(val)
                    for col, val in zip(self.target_columns, latest_pred)
                }
            else:
                return {f"target_{i}": float(val) for i, val in enumerate(latest_pred)}

        except Exception as e:
            logger.error(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return 0.0

    def save_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ì €ì¥"""
        try:
            # Universal ëª¨ë¸ ì €ì¥
            torch.save(
                self.universal_model.state_dict(), f"{filepath}_pytorch_universal.pth"
            )

            # í”¼ì²˜ ì •ë³´ ì €ì¥
            if self.feature_info:
                feature_info_path = f"{filepath}_feature_info.json"
                with open(feature_info_path, "w", encoding="utf-8") as f:
                    json.dump(self.feature_info, f, indent=2, ensure_ascii=False)
                logger.info(f"í”¼ì²˜ ì •ë³´ ì €ì¥ ì™„ë£Œ: {feature_info_path}")

            # ê°œë³„ ëª¨ë¸ ì €ì¥
            for symbol, model in self.individual_models.items():
                torch.save(
                    model.state_dict(), f"{filepath}_pytorch_individual_{symbol}.pth"
                )

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            model_data = {
                "feature_names": self.feature_names,
                "target_columns": self.target_columns,
                "config": self.config,
                "is_fitted": self.enable_individual_models,
                "feature_info": self.feature_info,
            }

            joblib.dump(model_data, f"{filepath}_meta.pkl")
            logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            return True

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def load_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            model_data = joblib.load(f"{filepath}_meta.pkl")
            self.feature_names = model_data["feature_names"]
            self.target_columns = model_data["target_columns"]
            self.enable_individual_models = model_data["is_fitted"]

            # í”¼ì²˜ ì •ë³´ ë¡œë“œ
            if "feature_info" in model_data:
                self.feature_info = model_data["feature_info"]
                logger.info("í”¼ì²˜ ì •ë³´ ë¡œë“œ ì™„ë£Œ")

            # PyTorch ëª¨ë¸ ë¡œë“œ
            if self.enable_individual_models:
                # Universal ëª¨ë¸ ë¡œë“œ
                universal_state_dict = torch.load(
                    f"{filepath}_pytorch_universal.pth",
                    map_location=torch.device("cpu"),
                )
                actual_input_dim = universal_state_dict["network.0.weight"].shape[1]
                actual_output_dim = universal_state_dict["network.6.weight"].shape[0]

                self.universal_model = self._build_model(
                    actual_input_dim, actual_output_dim
                )
                self.universal_model.load_state_dict(universal_state_dict)

                # ê°œë³„ ëª¨ë¸ ë¡œë“œ
                for symbol in self.individual_scalers.keys():
                    individual_state_dict = torch.load(
                        f"{filepath}_pytorch_individual_{symbol}.pth",
                        map_location=torch.device("cpu"),
                    )
                    model = self._build_model(actual_input_dim, actual_output_dim)
                    model.load_state_dict(individual_state_dict)
                    self.individual_models[symbol] = model

            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
            return True

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="ì‹ ê²½ë§ ê¸°ë°˜ ì£¼ì‹ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    parser.add_argument("--train", action="store_true", help="ëª¨ë¸ í›ˆë ¨")
    parser.add_argument("--force", action="store_true", help="ê°•ì œ ì¬í›ˆë ¨")
    parser.add_argument("--data-dir", default="data/trader", help="ë°ì´í„° ë””ë ‰í† ë¦¬")

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    config_path = "config/config_trader.json"
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)
    else:
        config = {
            "neural_network": {
                "epochs": 200,
                "batch_size": 32,
                "learning_rate": 0.001,
                "early_stopping_patience": 20,
                "architecture": {"hidden_layers": [32, 16], "dropout_rate": 0.2},
                "ensemble": {
                    "universal_weight": 0.7,
                    "individual_weight": 0.3,
                    "enable_individual_models": True,
                },
            }
        }

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = StockPredictionNetwork(config)

    if args.train:
        # ë°ì´í„° ë¡œë“œ ë° í›ˆë ¨
        training_data = {}

        # ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ
        data_files = [f for f in os.listdir(args.data_dir) if f.endswith(".csv")]

        for file in data_files:
            symbol = file.split("_")[0]
            filepath = os.path.join(args.data_dir, file)

            try:
                df = pd.read_csv(filepath)
                df = df.fillna(method="ffill").fillna(method="bfill")

                # í”¼ì²˜ ìƒì„±
                market_regime = {"current_regime": "SIDEWAYS"}
                features = model.create_features(df, symbol, market_regime)

                # íƒ€ê²Ÿ ìƒì„± (22ì¼, 66ì¼ ìˆ˜ìµë¥  + í‘œì¤€í¸ì°¨)
                close = df["close"]
                target_22d = close.pct_change(22).shift(-22)
                target_66d = close.pct_change(66).shift(-66)

                # í‘œì¤€í¸ì°¨ ê³„ì‚°
                rolling_std_22d = (
                    close.pct_change(22)
                    .rolling(window=22, min_periods=1)
                    .std()
                    .shift(-22)
                )
                rolling_std_66d = (
                    close.pct_change(66)
                    .rolling(window=66, min_periods=1)
                    .std()
                    .shift(-66)
                )

                # NaN ì²˜ë¦¬
                target_22d = target_22d.fillna(method="ffill").fillna(method="bfill")
                target_66d = target_66d.fillna(method="ffill").fillna(method="bfill")
                rolling_std_22d = rolling_std_22d.fillna(method="ffill").fillna(
                    method="bfill"
                )
                rolling_std_66d = rolling_std_66d.fillna(method="ffill").fillna(
                    method="bfill"
                )

                targets = pd.DataFrame(
                    {
                        "target_22d": target_22d,
                        "sigma_22d": rolling_std_22d,
                        "target_66d": target_66d,
                        "sigma_66d": rolling_std_66d,
                    }
                )

                training_data[symbol] = {"features": features, "targets": targets}

                print(f"âœ… {symbol} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")
                print(
                    f"   ğŸ“Š í”¼ì²˜ ì°¨ì›: {features.shape if features is not None else 'None'}"
                )

            except Exception as e:
                print(f"âŒ {symbol} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        print(f"ğŸ“Š ì´ {len(training_data)}ê°œ ì¢…ëª© ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

        # ëª¨ë¸ í›ˆë ¨
        if model.fit(training_data):
            # ëª¨ë¸ ì €ì¥
            model_path = "models/trader/neural_predictor"
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            model.save_model(model_path)

            # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
            if training_data:
                symbol = list(training_data.keys())[0]
                features = training_data[symbol]["features"]
                prediction = model.predict(features.tail(1), symbol)
                print(f"ğŸ“ˆ {symbol} ì˜ˆì¸¡ ê²°ê³¼: {prediction}")

    else:
        # ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡
        model_path = "models/trader/neural_predictor"
        if model.load_model(model_path):
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
