"""
ì‹ ê²½ë§ ê¸°ë°˜ ì£¼ì‹ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì€ feature_engineering.py ì‚¬ìš©
- ì‹ ê²½ë§ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ì— ì§‘ì¤‘
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import json
import os
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import logging
import warnings

# tqdm import (ì§„í–‰ë°” í‘œì‹œìš©)
try:
    from tqdm import trange
except ImportError:
    # tqdmì´ ì—†ìœ¼ë©´ ê¸°ë³¸ range ì‚¬ìš©
    trange = range

# feature_engineering.pyì—ì„œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê¸°ëŠ¥ import
try:
    from .feature_engineering import FeatureEngineeringPipeline
except ImportError:
    # ìƒëŒ€ ì„í¬íŠ¸ê°€ ì‹¤íŒ¨í•˜ë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ì‹œë„
    import sys
    import os

    sys.path.append(os.path.dirname(__file__))
    try:
        from feature_engineering import FeatureEngineeringPipeline
    except ImportError:
        FeatureEngineeringPipeline = None
        logger.warning(
            "FeatureEngineeringPipelineì„ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”¼ì²˜ ìƒì„± ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )

# FeatureEngineeringPipeline import ì‹œë„
try:
    from ..agent.enhancements.feature_engineering_pipeline import (
        FeatureEngineeringPipeline,
    )
except ImportError:
    FeatureEngineeringPipeline = None

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)


class EnsembleWeightLearner(nn.Module):
    """
    ì•™ìƒë¸” ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ëŠ” ë©”íƒ€-í•™ìŠµ ì‹ ê²½ë§

    ì…ë ¥: ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’, ì‹œì¥ ìƒí™©, ì¢…ëª© íŠ¹ì„±
    ì¶œë ¥: ìµœì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜
    """

    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16]):
        super(EnsembleWeightLearner, self).__init__()

        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend(
                [nn.Linear(prev_size, hidden_size), nn.ReLU(), nn.Dropout(0.2)]
            )
            prev_size = hidden_size

        # ì¶œë ¥ì¸µ: 2ê°œ ê°€ì¤‘ì¹˜ (universal, individual)
        layers.append(nn.Linear(prev_size, 2))
        layers.append(nn.Softmax(dim=1))  # ê°€ì¤‘ì¹˜ í•©ì´ 1ì´ ë˜ë„ë¡

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


class EnsembleWeightDataset(Dataset):
    """
    ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹
    """

    def __init__(self, ensemble_inputs: np.ndarray, optimal_weights: np.ndarray):
        self.ensemble_inputs = torch.FloatTensor(ensemble_inputs)
        self.optimal_weights = torch.FloatTensor(optimal_weights)

    def __len__(self):
        return len(self.ensemble_inputs)

    def __getitem__(self, idx):
        return self.ensemble_inputs[idx], self.optimal_weights[idx]


class SimpleStockPredictor(nn.Module):
    """
    ê°„ë‹¨í•œ ì£¼ì‹ ì˜ˆì¸¡ ì‹ ê²½ë§
    """

    def __init__(
        self,
        input_size: int,
        hidden_sizes: List[int] = [32, 16],
        dropout_rate: float = 0.2,
        output_size: int = 1,  # ë©€í‹°íƒ€ê²Ÿì„ ìœ„í•´ ì¶œë ¥ í¬ê¸° íŒŒë¼ë¯¸í„° ì¶”ê°€
    ):
        super(SimpleStockPredictor, self).__init__()

        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend(
                [nn.Linear(prev_size, hidden_size), nn.ReLU(), nn.Dropout(dropout_rate)]
            )
            prev_size = hidden_size

        # ì¶œë ¥ì¸µì„ ë™ì ìœ¼ë¡œ ì„¤ì •
        layers.append(nn.Linear(prev_size, output_size))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


class StockDataset(Dataset):
    """
    ì£¼ì‹ ë°ì´í„°ë¥¼ ìœ„í•œ PyTorch Dataset
    """

    def __init__(self, features: np.ndarray, targets: np.ndarray):
        self.features = torch.FloatTensor(features)
        self.targets = torch.FloatTensor(targets)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]


class StockPredictionNetwork:
    """
    ì‹ ê²½ë§ ê¸°ë°˜ ê°œë³„ ì¢…ëª© ì˜ˆì¸¡ê¸°

    Features:
    - 18ê°œ ê¸°ì¡´ ìŠ¤ìœ™ ì „ëµ ì‹ í˜¸
    - ì‹œì¥ ì²´ì œ ì •ë³´ (HMM ê²°ê³¼)
    - ë§¤í¬ë¡œ í™˜ê²½ í”¼ì²˜
    - ê¸°ìˆ ì  ì§€í‘œë“¤
    """

    def __init__(self, config: Dict):
        """
        ì‹ ê²½ë§ ê¸°ë°˜ ì£¼ì‹ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ì•™ìƒë¸” ë°©ì‹)

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.neural_config = config.get("neural_network", {})

        # Train-test ë¶„í•  ì„¤ì •
        self.train_ratio = self.neural_config.get("train_ratio", 0.8)
        logger.info(f"ğŸ“Š Train-test ë¶„í•  ë¹„ìœ¨: {self.train_ratio:.1%}")

        # ì•™ìƒë¸” ì„¤ì •
        self.ensemble_config = self.neural_config.get(
            "ensemble",
            {
                "universal_weight": 0.7,
                "individual_weight": 0.3,
                "enable_individual_models": True,
                "enable_weight_learning": True,  # ê°€ì¤‘ì¹˜ í•™ìŠµ í™œì„±í™”
                "weight_learning_config": {
                    "learning_rate": 0.001,
                    "epochs": 100,
                    "batch_size": 32,
                    "validation_split": 0.2,
                    "min_samples_for_weight_learning": 50,
                },
            },
        )

        # ëª¨ë¸ë“¤
        self.universal_model = None  # í†µí•© ëª¨ë¸
        self.individual_models = {}  # ì¢…ëª©ë³„ ëª¨ë¸ë“¤
        self.universal_scaler = StandardScaler()
        self.individual_scalers = {}

        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸°
        self.weight_learner = None
        self.weight_learner_scaler = StandardScaler()
        self.enable_weight_learning = self.ensemble_config.get(
            "enable_weight_learning", True
        )
        self.weight_learning_config = self.ensemble_config.get(
            "weight_learning_config", {}
        )

        # í”¼ì²˜ ì •ë³´
        self.feature_names = None
        self.target_columns = None

        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        if FeatureEngineeringPipeline:
            try:
                self.feature_pipeline = FeatureEngineeringPipeline(config)
                logger.info("âœ… FeatureEngineeringPipeline ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"FeatureEngineeringPipeline ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.feature_pipeline = None
        else:
            self.feature_pipeline = None
            logger.info("â„¹ï¸ ê¸°ë³¸ í”¼ì²˜ ìƒì„± ë°©ì‹ ì‚¬ìš©")

        # í”¼ì²˜ ì •ë³´ ì €ì¥
        self.feature_info = {
            "universal_features": {},
            "individual_features": {},
            "macro_features": {},
            "feature_dimensions": {},
            "created_at": datetime.now().isoformat(),
        }

        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’, í•™ìŠµ í›„ ì—…ë°ì´íŠ¸ë¨)
        self.universal_weight = self.ensemble_config.get("universal_weight", 0.7)
        self.individual_weight = self.ensemble_config.get("individual_weight", 0.3)
        self.enable_individual_models = self.ensemble_config.get(
            "enable_individual_models", True
        )

        # ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ì €ì¥
        self.weight_training_data = {
            "ensemble_inputs": [],
            "optimal_weights": [],
            "performance_metrics": [],
        }

        logger.info(
            f"StockPredictionNetwork ì´ˆê¸°í™” ì™„ë£Œ - ì•™ìƒë¸” ëª¨ë“œ (Universal: {self.universal_weight}, Individual: {self.individual_weight})"
        )
        if self.enable_weight_learning:
            logger.info("âœ… ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµ í™œì„±í™”")

    def _build_model(self, input_dim: int, output_size: int = 1) -> nn.Module:
        """
        ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¶•

        Args:
            input_dim: ì…ë ¥ ì°¨ì›
            output_size: ì¶œë ¥ ì°¨ì› (ë©€í‹°íƒ€ê²Ÿ ê°œìˆ˜)

        Returns:
            PyTorch ëª¨ë¸
        """
        architecture = self.neural_config.get("architecture", {})
        hidden_sizes = architecture.get("hidden_layers", [32, 16])
        dropout_rate = architecture.get("dropout_rate", 0.2)

        model = SimpleStockPredictor(
            input_size=input_dim,
            hidden_sizes=hidden_sizes,
            dropout_rate=dropout_rate,
            output_size=output_size,
        )

        return model

    def extract_swing_features(
        self, stock_data: pd.DataFrame, symbol: str
    ) -> pd.DataFrame:
        """
        ê¸°ì¡´ ìŠ¤ìœ™ ì „ëµ í”¼ì²˜ ì¶”ì¶œ (í˜¸í™˜ì„± ìœ ì§€)

        Args:
            stock_data: ì£¼ì‹ ë°ì´í„° (OHLCV)
            symbol: ì¢…ëª© ì½”ë“œ

        Returns:
            ìŠ¤ìœ™ ì „ëµ í”¼ì²˜ ë°ì´í„°í”„ë ˆì„
        """
        # feature_engineering.py ì‚¬ìš©
        if self.feature_pipeline:
            try:
                features, _ = self.feature_pipeline.create_dynamic_features(
                    stock_data, symbol, {}, None, "individual"
                )
                return features
            except Exception as e:
                logger.warning(f"FeatureEngineeringPipeline ì‚¬ìš© ì‹¤íŒ¨: {e}")

        # ê¸°ë³¸ í”¼ì²˜ ìƒì„± (fallback)
        return self._create_basic_swing_features(stock_data, symbol)

    def _create_basic_swing_features(
        self, stock_data: pd.DataFrame, symbol: str
    ) -> pd.DataFrame:
        """ê¸°ë³¸ ìŠ¤ìœ™ í”¼ì²˜ ìƒì„± (fallback)"""
        features = pd.DataFrame(index=stock_data.index)

        try:
            # ê¸°ë³¸ ê°€ê²© ë°ì´í„° í™•ì¸
            required_cols = ["open", "high", "low", "close", "volume"]
            for col in required_cols:
                if col not in stock_data.columns:
                    logger.warning(f"{symbol}: {col} ì»¬ëŸ¼ ì—†ìŒ")
                    stock_data[col] = stock_data.get("close", 100)  # ê¸°ë³¸ê°’

            # í™•ì¥ëœ ê¸°ë³¸ í”¼ì²˜ë“¤ ìƒì„±
            features["dual_momentum"] = self._calculate_dual_momentum(stock_data)
            features["volatility_breakout"] = self._calculate_volatility_breakout(stock_data)
            features["swing_ema"] = self._calculate_swing_ema(stock_data)
            features["swing_rsi"] = self._calculate_swing_rsi(stock_data)
            
            # ì¶”ê°€ ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜ë“¤
            features["swing_donchian"] = self._calculate_swing_donchian(stock_data)
            features["stoch_donchian"] = self._calculate_stoch_donchian(stock_data)
            features["whipsaw_prevention"] = self._calculate_whipsaw_prevention(stock_data)
            features["donchian_rsi_whipsaw"] = self._calculate_donchian_rsi_whipsaw(stock_data)
            features["volatility_filtered_breakout"] = self._calculate_volatility_filtered_breakout(stock_data)
            features["multi_timeframe_whipsaw"] = self._calculate_multi_timeframe_whipsaw(stock_data)
            features["adaptive_whipsaw"] = self._calculate_adaptive_whipsaw(stock_data)
            features["cci_bollinger"] = self._calculate_cci_bollinger(stock_data)
            features["mean_reversion"] = self._calculate_mean_reversion(stock_data)
            features["swing_breakout"] = self._calculate_swing_breakout(stock_data)
            features["swing_pullback_entry"] = self._calculate_swing_pullback_entry(stock_data)
            features["swing_candle_pattern"] = self._calculate_swing_candle_pattern(stock_data)
            features["swing_bollinger_band"] = self._calculate_swing_bollinger_band(stock_data)
            features["swing_macd"] = self._calculate_swing_macd(stock_data)
            
            # ì¶”ê°€ ê°€ê²© ê¸°ë°˜ í”¼ì²˜ë“¤
            features["atr_normalized"] = self._calculate_atr(stock_data) / stock_data["close"]
            features["rsi_14"] = self._calculate_rsi(stock_data, 14) / 100 - 0.5  # -0.5 ~ 0.5 ì •ê·œí™”
            features["macd_signal"] = self._calculate_macd(stock_data) / stock_data["close"]
            
            # ìƒˆë¡œìš´ ê³ ê¸‰ í”¼ì²˜ë“¤
            features["volume_price_trend"] = self._calculate_volume_price_trend(stock_data)
            features["price_volume_oscillator"] = self._calculate_price_volume_oscillator(stock_data)
            features["trend_strength"] = self._calculate_trend_strength(stock_data)
            features["volatility_regime"] = self._calculate_volatility_regime(stock_data)
            features["support_resistance"] = self._calculate_support_resistance_strength(stock_data)
            features["momentum_divergence"] = self._calculate_momentum_divergence(stock_data)
            features["volatility_skew"] = self._calculate_volatility_skew(stock_data)
            features["market_microstructure"] = self._calculate_market_microstructure(stock_data)

            # NaN ì²˜ë¦¬
            features = features.fillna(method="ffill").fillna(method="bfill").fillna(0)

        except Exception as e:
            logger.error(f"ê¸°ë³¸ ìŠ¤ìœ™ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")

        return features

    # ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë©”ì„œë“œë“¤ (feature_engineering.pyì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë“¤ë§Œ ìœ ì§€)
    def _calculate_dual_momentum(self, data: pd.DataFrame) -> pd.Series:
        """ë“€ì–¼ ëª¨ë©˜í…€ ê³„ì‚°"""
        close = data["close"]
        momentum_short = close.pct_change(5)
        momentum_long = close.pct_change(20)
        return momentum_short - momentum_long

    def _calculate_volatility_breakout(self, data: pd.DataFrame) -> pd.Series:
        """ë³€ë™ì„± ë¸Œë ˆì´í¬ì•„ì›ƒ ê³„ì‚°"""
        close = data["close"]
        volatility = close.pct_change().rolling(20).std()
        return (close - close.rolling(20).mean()) / (
            volatility * close.rolling(20).mean()
        )

    def _calculate_swing_ema(self, data: pd.DataFrame) -> pd.Series:
        """ìŠ¤ìœ™ EMA ê³„ì‚°"""
        close = data["close"]
        ema_short = close.ewm(span=12).mean()
        ema_long = close.ewm(span=26).mean()
        return (ema_short - ema_long) / ema_long

    def _calculate_swing_rsi(self, data: pd.DataFrame) -> pd.Series:
        """ìŠ¤ìœ™ RSI ê³„ì‚°"""
        close = data["close"]
        delta = close.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return (rsi - 50) / 50  # -1 ~ 1 ì •ê·œí™”

    def _calculate_swing_donchian(self, data: pd.DataFrame) -> pd.Series:
        """Swing Donchian ê³„ì‚°"""
        try:
            period = 20
            donchian_high = data["high"].rolling(period).max()
            donchian_low = data["low"].rolling(period).min()
            donchian_mid = (donchian_high + donchian_low) / 2
            return (data["close"] - donchian_mid) / (donchian_high - donchian_low)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_stoch_donchian(self, data: pd.DataFrame) -> pd.Series:
        """Stochastic Donchian ê³„ì‚°"""
        try:
            period = 14
            lowest_low = data["low"].rolling(period).min()
            highest_high = data["high"].rolling(period).max()
            stoch = (data["close"] - lowest_low) / (highest_high - lowest_low)
            return (stoch - 0.5) * 2  # -1 ~ 1 ì •ê·œí™”
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_whipsaw_prevention(self, data: pd.DataFrame) -> pd.Series:
        """Whipsaw Prevention ê³„ì‚°"""
        try:
            ma = data["close"].rolling(20).mean()
            volatility = data["close"].rolling(20).std()
            signal = np.where(
                data["close"] > ma + volatility,
                1,
                np.where(data["close"] < ma - volatility, -1, 0),
            )
            return pd.Series(signal, index=data.index)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_donchian_rsi_whipsaw(self, data: pd.DataFrame) -> pd.Series:
        """Donchian RSI Whipsaw ê³„ì‚°"""
        try:
            donchian = self._calculate_swing_donchian(data)
            rsi = self._calculate_swing_rsi(data)
            return (donchian + rsi) / 2
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_volatility_filtered_breakout(self, data: pd.DataFrame) -> pd.Series:
        """Volatility Filtered Breakout ê³„ì‚°"""
        try:
            volatility = data["close"].rolling(20).std()
            vol_threshold = volatility.rolling(60).quantile(0.3)  # ë‚®ì€ ë³€ë™ì„± ê¸°ì¤€
            breakout = self._calculate_volatility_breakout(data)
            return np.where(volatility < vol_threshold, breakout, 0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_multi_timeframe_whipsaw(self, data: pd.DataFrame) -> pd.Series:
        """Multi Timeframe Whipsaw ê³„ì‚°"""
        try:
            short_trend = data["close"].rolling(5).mean()
            medium_trend = data["close"].rolling(20).mean()
            long_trend = data["close"].rolling(50).mean()

            short_signal = np.where(data["close"] > short_trend, 1, -1)
            medium_signal = np.where(short_trend > medium_trend, 1, -1)
            long_signal = np.where(medium_trend > long_trend, 1, -1)

            return (short_signal + medium_signal + long_signal) / 3
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_adaptive_whipsaw(self, data: pd.DataFrame) -> pd.Series:
        """Adaptive Whipsaw ê³„ì‚°"""
        try:
            volatility = data["close"].rolling(20).std()
            adaptive_period = np.clip(20 / (volatility + 0.01), 10, 50).astype(int)

            signals = []
            for i, period in enumerate(adaptive_period):
                if i < period:
                    signals.append(0)
                else:
                    ma = data["close"].iloc[i - period : i].mean()
                    signal = 1 if data["close"].iloc[i] > ma else -1
                    signals.append(signal)

            return pd.Series(signals, index=data.index)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_cci_bollinger(self, data: pd.DataFrame) -> pd.Series:
        """CCI Bollinger ê³„ì‚°"""
        try:
            # CCI ê³„ì‚°
            tp = (data["high"] + data["low"] + data["close"]) / 3
            ma = tp.rolling(20).mean()
            mad = tp.rolling(20).apply(lambda x: np.abs(x - x.mean()).mean())
            cci = (tp - ma) / (0.015 * mad)

            # ì •ê·œí™” (-1 ~ 1)
            return np.clip(cci / 100, -1, 1)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_mean_reversion(self, data: pd.DataFrame) -> pd.Series:
        """Mean Reversion ê³„ì‚°"""
        try:
            ma = data["close"].rolling(20).mean()
            std = data["close"].rolling(20).std()
            z_score = (data["close"] - ma) / std
            return -z_score  # í‰ê·  íšŒê·€ ì‹ í˜¸ (í˜„ì¬ê°€ê°€ ë†’ìœ¼ë©´ ìŒì˜ ì‹ í˜¸)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_swing_breakout(self, data: pd.DataFrame) -> pd.Series:
        """Swing Breakout ê³„ì‚°"""
        try:
            period = 20
            resistance = data["high"].rolling(period).max()
            support = data["low"].rolling(period).min()

            breakout_up = data["close"] > resistance.shift(1)
            breakout_down = data["close"] < support.shift(1)

            return np.where(breakout_up, 1, np.where(breakout_down, -1, 0))
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_swing_pullback_entry(self, data: pd.DataFrame) -> pd.Series:
        """Swing Pullback Entry ê³„ì‚°"""
        try:
            ma = data["close"].rolling(50).mean()
            recent_high = data["high"].rolling(10).max()

            uptrend = data["close"] > ma
            pullback = (data["close"] / recent_high - 1) < -0.05  # 5% í•˜ë½

            return np.where(uptrend & pullback, 1, 0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_swing_candle_pattern(self, data: pd.DataFrame) -> pd.Series:
        """Swing Candle Pattern ê³„ì‚° (ë‹¨ìˆœ ë²„ì „)"""
        try:
            body = abs(data["close"] - data["open"])
            range_size = data["high"] - data["low"]

            # ë„ì§€ íŒ¨í„´ (ì‘ì€ ëª¸í†µ)
            doji = body < (range_size * 0.1)

            # ë§ì¹˜ íŒ¨í„´ (ì•„ë˜ê¼¬ë¦¬ê°€ ê¸´ íŒ¨í„´)
            hammer = (data["close"] > data["open"]) & (
                (data["open"] - data["low"]) > body * 2
            )

            return np.where(hammer, 1, np.where(doji, 0, -1))
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_swing_bollinger_band(self, data: pd.DataFrame) -> pd.Series:
        """Swing Bollinger Band ê³„ì‚°"""
        try:
            ma = data["close"].rolling(20).mean()
            std = data["close"].rolling(20).std()
            upper = ma + (std * 2)
            lower = ma - (std * 2)

            bb_position = (data["close"] - lower) / (upper - lower)
            return (bb_position - 0.5) * 2  # -1 ~ 1 ì •ê·œí™”
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_swing_macd(self, data: pd.DataFrame) -> pd.Series:
        """Swing MACD ê³„ì‚°"""
        try:
            ema_12 = data["close"].ewm(span=12).mean()
            ema_26 = data["close"].ewm(span=26).mean()
            macd = ema_12 - ema_26
            signal = macd.ewm(span=9).mean()
            histogram = macd - signal

            return histogram / data["close"] * 100  # ì •ê·œí™”
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_atr(self, data: pd.DataFrame, period: int = 14) -> pd.Series:
        """ATR (Average True Range) ê³„ì‚°"""
        high = data["high"]
        low = data["low"]
        close = data["close"]

        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())

        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(window=period).mean()

        return atr

    def _calculate_rsi(self, data: pd.DataFrame, period: int = 14) -> pd.Series:
        """RSI (Relative Strength Index) ê³„ì‚°"""
        delta = data["close"].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi

    def _calculate_macd(
        self, data: pd.DataFrame, fast: int = 12, slow: int = 26, signal: int = 9
    ) -> pd.Series:
        """MACD (Moving Average Convergence Divergence) ê³„ì‚°"""
        ema_fast = data["close"].ewm(span=fast).mean()
        ema_slow = data["close"].ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        return macd

    def _calculate_bollinger_bands(
        self, data: pd.DataFrame, period: int = 20, std: int = 2
    ) -> Tuple[pd.Series, pd.Series]:
        """ë³¼ë¦°ì € ë°´ë“œ ê³„ì‚°"""
        sma = data["close"].rolling(window=period).mean()
        std_dev = data["close"].rolling(window=period).std()
        upper_band = sma + (std_dev * std)
        lower_band = sma - (std_dev * std)
        return upper_band, lower_band

    def _calculate_volume_price_trend(self, data: pd.DataFrame) -> pd.Series:
        """ë³¼ë¥¨-ê°€ê²© ì¶”ì„¸ ê³„ì‚°"""
        try:
            price_change = data["close"].pct_change()
            volume_norm = data["volume"] / data["volume"].rolling(20).mean()
            vpt = (price_change * volume_norm).rolling(10).sum()
            return vpt.fillna(0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_price_volume_oscillator(self, data: pd.DataFrame) -> pd.Series:
        """ê°€ê²©-ë³¼ë¥¨ ì˜¤ì‹¤ë ˆì´í„°"""
        try:
            volume_sma_short = data["volume"].rolling(12).mean()
            volume_sma_long = data["volume"].rolling(26).mean()
            pvo = (volume_sma_short - volume_sma_long) / volume_sma_long
            return pvo.fillna(0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_trend_strength(self, data: pd.DataFrame) -> pd.Series:
        """ì¶”ì„¸ ê°•ë„ ê³„ì‚°"""
        try:
            close = data["close"]
            ma_5 = close.rolling(5).mean()
            ma_10 = close.rolling(10).mean()
            ma_20 = close.rolling(20).mean()
            ma_50 = close.rolling(50).mean()
            
            # ì´ë™í‰ê· ì„  ì •ë ¬ë„ ê³„ì‚°
            trend_alignment = (
                (ma_5 > ma_10).astype(int) +
                (ma_10 > ma_20).astype(int) +
                (ma_20 > ma_50).astype(int)
            ) / 3.0 - 0.5  # -0.5 ~ 0.5 ì •ê·œí™”
            
            return trend_alignment.fillna(0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_volatility_regime(self, data: pd.DataFrame) -> pd.Series:
        """ë³€ë™ì„± ì²´ì œ ë¶„ë¥˜"""
        try:
            returns = data["close"].pct_change()
            volatility = returns.rolling(20).std()
            vol_percentile = volatility.rolling(252).rank(pct=True)
            
            # ë³€ë™ì„± ì²´ì œ (0: ë‚®ìŒ, 1: ë†’ìŒ)
            vol_regime = (vol_percentile > 0.7).astype(float) - (vol_percentile < 0.3).astype(float)
            return vol_regime.fillna(0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_support_resistance_strength(self, data: pd.DataFrame) -> pd.Series:
        """ì§€ì§€/ì €í•­ ê°•ë„ ê³„ì‚°"""
        try:
            close = data["close"]
            high = data["high"]
            low = data["low"]
            
            # ìµœê·¼ 20ì¼ ê³ ì /ì €ì  ëŒ€ë¹„ í˜„ì¬ ìœ„ì¹˜
            period_high = high.rolling(20).max()
            period_low = low.rolling(20).min()
            
            # ê³ ì /ì €ì  ê·¼ì ‘ë„ (-1: ì €ì  ê·¼ì²˜, 1: ê³ ì  ê·¼ì²˜)
            position = (close - period_low) / (period_high - period_low) * 2 - 1
            return position.fillna(0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_momentum_divergence(self, data: pd.DataFrame) -> pd.Series:
        """ëª¨ë©˜í…€ ë‹¤ì´ë²„ì „ìŠ¤ ê°ì§€"""
        try:
            close = data["close"]
            rsi = self._calculate_rsi(data, 14)
            
            # ê°€ê²©ê³¼ RSIì˜ ìƒê´€ê´€ê³„ ë³€í™”ë¡œ ë‹¤ì´ë²„ì „ìŠ¤ ê°ì§€
            price_momentum = close.pct_change(5)
            rsi_momentum = rsi.diff(5)
            
            # ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ë‹¤ì´ë²„ì „ìŠ¤ ì‹ í˜¸
            correlation = price_momentum.rolling(10).corr(rsi_momentum)
            divergence = (1 - correlation).fillna(0)  # ë‚®ì€ ìƒê´€ê´€ê³„ = ë‹¤ì´ë²„ì „ìŠ¤
            
            return divergence
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_volatility_skew(self, data: pd.DataFrame) -> pd.Series:
        """ë³€ë™ì„± ë¹„ëŒ€ì¹­ì„± (ìŠ¤í) ê³„ì‚°"""
        try:
            returns = data["close"].pct_change()
            
            # ë¡¤ë§ ìŠ¤í ê³„ì‚°
            skewness = returns.rolling(20).skew()
            
            # ì •ê·œí™” (-1 ~ 1)
            normalized_skew = np.tanh(skewness / 2)
            return normalized_skew.fillna(0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def _calculate_market_microstructure(self, data: pd.DataFrame) -> pd.Series:
        """ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° ì§€í‘œ"""
        try:
            high = data["high"]
            low = data["low"]
            close = data["close"]
            open_price = data["open"]
            
            # ì¼ì¤‘ ê°€ê²© íš¨ìœ¨ì„± ì¸¡ì •
            intraday_range = (high - low) / close
            gap = abs(open_price - close.shift(1)) / close.shift(1)
            
            # ë¯¸ì‹œêµ¬ì¡° ì‹ í˜¸ (ê°­ ëŒ€ë¹„ ì¼ì¤‘ ë ˆì¸ì§€)
            microstructure = (intraday_range / (gap + 0.001)).rolling(5).mean()
            
            # ë¡œê·¸ ë³€í™˜ í›„ ì •ê·œí™”
            microstructure_norm = np.tanh(np.log1p(microstructure))
            return microstructure_norm.fillna(0)
        except:
            return pd.Series([0.0] * len(data), index=data.index)

    def create_features(
        self,
        stock_data: pd.DataFrame,
        symbol: str,
        market_regime: Dict,
        macro_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """
        ì „ì²´ í”¼ì²˜ ìƒì„± (feature_engineering.py ì‚¬ìš©)

        Args:
            stock_data: ì£¼ì‹ ë°ì´í„°
            symbol: ì¢…ëª© ì½”ë“œ
            market_regime: HMMì—ì„œ ì˜ˆì¸¡í•œ ì‹œì¥ ì²´ì œ
            macro_data: ì „ì²´ ë§¤í¬ë¡œ ë°ì´í„° (ì„ íƒì‚¬í•­)

        Returns:
            í†µí•© í”¼ì²˜ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info(f"ğŸ” {symbol} í”¼ì²˜ ìƒì„± ì‹œì‘...")
            logger.info(f"   - ì£¼ì‹ ë°ì´í„°: {stock_data.shape}")
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ ìˆ˜: {len(stock_data.columns)}ê°œ")
            logger.info(
                f"   - ë§¤í¬ë¡œ ë°ì´í„°: {macro_data.shape if macro_data is not None else 'None'}"
            )

            # feature_engineering.pyì˜ ë™ì  í”¼ì²˜ ìƒì„± ì‚¬ìš©
            if self.feature_pipeline:
                logger.info(f"   ğŸ”§ {symbol} ë™ì  í”¼ì²˜ ìƒì„± ì¤‘...")
                try:
                    # individual ëª¨ë“œì—ì„œëŠ” ë§¤í¬ë¡œ ë°ì´í„° ì œì™¸ (ì €ì¥ëœ ëª¨ë¸ê³¼ ì¼ì¹˜)
                    features, metadata = self.feature_pipeline.create_dynamic_features(
                        stock_data=stock_data,
                        symbol=symbol,
                        market_regime=market_regime,
                        macro_data=None,  # ê°œë³„ ëª¨ë¸ì€ ë§¤í¬ë¡œ í”¼ì²˜ ì œì™¸
                        mode="individual",
                    )

                    # í”¼ì²˜ ì¹´í…Œê³ ë¦¬ ìš”ì•½
                    feature_categories = metadata.get("feature_categories", {})
                    total_features = (
                        sum(feature_categories.values())
                        if feature_categories
                        else len(features.columns)
                    )
                    category_summary = " | ".join(
                        [f"{k}:{v}" for k, v in feature_categories.items() if v > 0]
                    )

                    logger.info(f"   âœ… ë™ì  í”¼ì²˜ ìƒì„± ì™„ë£Œ: {features.shape}")
                    logger.info(
                        f"   ğŸ“Š ë³µì¡ë„: {metadata.get('data_complexity', 'unknown')} | ì¹´í…Œê³ ë¦¬: {category_summary}"
                    )

                    # í”¼ì²˜ ì •ë³´ ì €ì¥
                    self.feature_info["individual_features"][symbol] = {
                        "total_features": len(features.columns),
                        "feature_names": list(features.columns),
                        "data_complexity": metadata.get("data_complexity", "unknown"),
                        "created_at": datetime.now().isoformat(),
                    }

                    return features

                except Exception as e:
                    logger.error(f"   âŒ ë™ì  í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
                    logger.info("   ğŸ”„ ê¸°ë³¸ í”¼ì²˜ ìƒì„± ë°©ì‹ìœ¼ë¡œ ì „í™˜...")

            # ê¸°ë³¸ í”¼ì²˜ ìƒì„± ë°©ì‹ (fallback)
            logger.info(f"   ğŸ“Š ê¸°ë³¸ í”¼ì²˜ ìƒì„± ì¤‘...")
            return self._create_basic_features(
                stock_data, symbol, market_regime, macro_data
            )

        except Exception as e:
            logger.error(f"í”¼ì²˜ ìƒì„± ì‹¤íŒ¨ ({symbol}): {e}")
            return pd.DataFrame(index=stock_data.index)

    def _create_basic_features(
        self,
        stock_data: pd.DataFrame,
        symbol: str,
        market_regime: Dict,
        macro_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """ê¸°ë³¸ í”¼ì²˜ ìƒì„± (fallback)"""
        features = pd.DataFrame(index=stock_data.index)

        try:
            # ê¸°ë³¸ ìŠ¤ìœ™ í”¼ì²˜
            swing_features = self.extract_swing_features(stock_data, symbol)
            features = pd.concat([features, swing_features], axis=1)

            # ì‹œì¥ ì²´ì œ í”¼ì²˜
            regime_features = pd.DataFrame(index=stock_data.index)
            regimes = ["TRENDING_UP", "TRENDING_DOWN", "SIDEWAYS", "VOLATILE"]
            current_regime = market_regime.get("current_regime", "SIDEWAYS")

            for regime in regimes:
                regime_features[f"regime_{regime.lower()}"] = pd.Series(
                    int(current_regime == regime), index=stock_data.index
                )

            features = pd.concat([features, regime_features], axis=1)

            # NaN ì²˜ë¦¬
            features = features.fillna(method="ffill").fillna(method="bfill").fillna(0)

        except Exception as e:
            logger.error(f"ê¸°ë³¸ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")

        return features

    def prepare_training_data(
        self,
        features: pd.DataFrame,
        target: Union[pd.Series, pd.DataFrame, np.ndarray],
        lookback_days: int = 20,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì‹œê³„ì—´ ìœˆë„ìš° ë°ì´í„° ì¤€ë¹„

        Args:
            features: í”¼ì²˜ ë°ì´í„°
            target: íƒ€ê²Ÿ ë°ì´í„° (ë‹¨ì¼ ë˜ëŠ” ë©€í‹°íƒ€ê²Ÿ)
            lookback_days: ìœˆë„ìš° í¬ê¸°

        Returns:
            (X, y) íŠœí”Œ
        """
        try:
            # íƒ€ê²Ÿì„ numpy arrayë¡œ ë³€í™˜
            if isinstance(target, pd.Series):
                target_array = target.values
            elif isinstance(target, pd.DataFrame):
                target_array = target.values
            else:
                target_array = target

            # 1Dì¸ ê²½ìš° 2Dë¡œ ë³€í™˜
            if len(target_array.shape) == 1:
                target_array = target_array.reshape(-1, 1)

            # í”¼ì²˜ë¥¼ numpy arrayë¡œ ë³€í™˜
            if isinstance(features, pd.DataFrame):
                features_array = features.values
            else:
                features_array = features

            # ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
            try:
                # í”¼ì²˜ ë°ì´í„°ë¥¼ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
                if isinstance(features_array, np.ndarray):
                    # ë¬¸ìì—´ì´ë‚˜ ê°ì²´ íƒ€ì…ì´ ìˆëŠ”ì§€ í™•ì¸
                    if features_array.dtype.kind in [
                        "O",
                        "U",
                        "S",
                    ]:  # object, unicode, bytes
                        logger.warning(
                            "í”¼ì²˜ì— ë¬¸ìì—´ ë°ì´í„°ê°€ ë°œê²¬ë˜ì–´ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤"
                        )
                        # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                        numeric_features = []
                        for i in range(features_array.shape[1]):
                            try:
                                numeric_features.append(
                                    pd.to_numeric(features_array[:, i], errors="coerce")
                                )
                            except:
                                # ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ì›€
                                numeric_features.append(
                                    np.zeros(features_array.shape[0])
                                )
                        features_array = np.column_stack(numeric_features)

                # íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
                if isinstance(target_array, np.ndarray):
                    if target_array.dtype.kind in ["O", "U", "S"]:
                        logger.warning(
                            "íƒ€ê²Ÿì— ë¬¸ìì—´ ë°ì´í„°ê°€ ë°œê²¬ë˜ì–´ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤"
                        )
                        target_array = pd.to_numeric(
                            target_array.flatten(), errors="coerce"
                        ).reshape(target_array.shape)

                # NaN ê²€ì¦ ë° ì²˜ë¦¬
                if np.isnan(features_array).any():
                    logger.warning("í”¼ì²˜ì— NaNì´ ë°œê²¬ë˜ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤")
                    # NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°
                    features_array = np.nan_to_num(features_array, nan=0.0)

                if np.isnan(target_array).any():
                    logger.warning("íƒ€ê²Ÿì— NaNì´ ë°œê²¬ë˜ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤")
                    # NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°
                    target_array = np.nan_to_num(target_array, nan=0.0)

            except Exception as e:
                logger.error(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëª¨ë“  ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
                features_array = np.zeros_like(features_array, dtype=float)
                target_array = np.zeros_like(target_array, dtype=float)

            # ìœˆë„ìš° ë°ì´í„° ìƒì„±
            X, y = [], []

            for i in range(lookback_days, len(features_array)):
                # ì…ë ¥ ìœˆë„ìš°
                X.append(features_array[i - lookback_days : i].flatten())
                # íƒ€ê²Ÿ (í˜„ì¬ ì‹œì ì˜ ë¯¸ë˜ ìˆ˜ìµë¥ )
                y.append(target_array[i])

            return np.array(X), np.array(y)

        except Exception as e:
            logger.error(f"ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return np.array([]), np.array([])

    def _prepare_prediction_data(
        self, features: pd.DataFrame, lookback_days: int = 20
    ) -> np.ndarray:
        """
        ì˜ˆì¸¡ìš© ìœˆë„ìš° ë°ì´í„° ìƒì„± (íƒ€ê²Ÿ ë°ì´í„° ì—†ì´)

        Args:
            features: í”¼ì²˜ ë°ì´í„°
            lookback_days: ë£©ë°± ê¸°ê°„

        Returns:
            ìœˆë„ìš° í˜•íƒœì˜ í”¼ì²˜ ë°°ì—´
        """
        try:
            # í”¼ì²˜ë¥¼ numpy arrayë¡œ ë³€í™˜
            if isinstance(features, pd.DataFrame):
                features_array = features.values
            else:
                features_array = features

            # ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
            if features_array.dtype.kind in ["O", "U", "S"]:  # object, unicode, bytes
                logger.warning("í”¼ì²˜ì— ë¬¸ìì—´ ë°ì´í„°ê°€ ë°œê²¬ë˜ì–´ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤")
                numeric_features = []
                for i in range(features_array.shape[1]):
                    try:
                        numeric_features.append(
                            pd.to_numeric(features_array[:, i], errors="coerce")
                        )
                    except:
                        numeric_features.append(np.zeros(features_array.shape[0]))
                features_array = np.column_stack(numeric_features)

            # NaN ì²˜ë¦¬
            if np.isnan(features_array).any():
                logger.warning("í”¼ì²˜ì— NaNì´ ë°œê²¬ë˜ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤")
                features_array = np.nan_to_num(features_array, nan=0.0)

            # ë°ì´í„° ê¸¸ì´ í™•ì¸
            if len(features_array) < lookback_days:
                logger.warning(
                    f"ë°ì´í„° ê¸¸ì´({len(features_array)})ê°€ ë£©ë°± ê¸°ê°„({lookback_days})ë³´ë‹¤ ì§§ìŠµë‹ˆë‹¤"
                )
                # ë¶€ì¡±í•œ ë°ì´í„°ëŠ” ì²« ë²ˆì§¸ í–‰ìœ¼ë¡œ íŒ¨ë”©
                padding_rows = lookback_days - len(features_array)
                if len(features_array) > 0:
                    padding = np.repeat(features_array[0:1], padding_rows, axis=0)
                    features_array = np.vstack([padding, features_array])
                else:
                    logger.error("í”¼ì²˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                    return np.array([])

            # ìµœê·¼ lookback_days ë§Œí¼ì˜ ë°ì´í„°ë¡œ ìœˆë„ìš° ìƒì„±
            if len(features_array) >= lookback_days:
                # ê°€ì¥ ìµœê·¼ ë°ì´í„°ì˜ ìœˆë„ìš°ë§Œ ìƒì„± (ì˜ˆì¸¡ìš©)
                window = features_array[-lookback_days:].flatten()
                return np.array([window])  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            else:
                logger.error(
                    f"ìœˆë„ìš° ìƒì„± ë¶ˆê°€: ë°ì´í„° ê¸¸ì´({len(features_array)}) < ë£©ë°± ê¸°ê°„({lookback_days})"
                )
                return np.array([])

        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return np.array([])

    def _train_universal_model(self, training_data: Dict) -> bool:
        """
        í†µí•© ëª¨ë¸ í•™ìŠµ (ëª¨ë“  ì¢…ëª© ë°ì´í„°ë¥¼ í•©ì³ì„œ í•™ìŠµ)
        """
        try:
            # ì „ì²´ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            all_X, all_y = [], []
            target_columns = None
            symbol_features = {}  # ì¢…ëª©ë³„ í”¼ì²˜ ì €ì¥

            for symbol, data in training_data.items():
                logger.info(f"ğŸ” {symbol} í†µí•© ëª¨ë¸ìš© ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

                try:
                    features = data["features"]
                    target = data["target"]
                except KeyError as e:
                    logger.error(
                        f"âŒ {symbol}: {e} í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤: {list(data.keys())}"
                    )
                    continue

                # ë©€í‹°íƒ€ê²Ÿì¸ ê²½ìš° ì»¬ëŸ¼ëª… ì €ì¥
                if isinstance(target, pd.DataFrame):
                    if target_columns is None:
                        target_columns = list(target.columns)
                    target = target.values
                elif isinstance(target, pd.Series):
                    target = target.values

                if len(features) < 50 or len(target) < 50:
                    logger.warning(
                        f"{symbol}: í•™ìŠµ ë°ì´í„° ë¶€ì¡± (features: {len(features)}, target: {len(target)})"
                    )
                    continue

                # ì¢…ëª©ë³„ í”¼ì²˜ ì €ì¥
                symbol_features[symbol] = features

                # ì‹œê³„ì—´ ìœˆë„ìš° ë°ì´í„° ìƒì„±
                features_config = self.neural_config.get("features", {})
                lookback = features_config.get("lookback_days", 20)
                X, y = self.prepare_training_data(features, target, lookback)

                if len(X) > 0 and len(y) > 0:
                    all_X.append(X)
                    all_y.append(y)
                else:
                    logger.warning(
                        f"{symbol}: ìœˆë„ìš° ë°ì´í„° ìƒì„± ì‹¤íŒ¨ (X: {len(X)}, y: {len(y)})"
                    )

            if not all_X:
                logger.error("í†µí•© ëª¨ë¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False

            # ì¢…ëª©ë³„ í”¼ì²˜ë¥¼ í†µí•©í•˜ì—¬ ìƒˆë¡œìš´ í”¼ì²˜ ìƒì„±
            logger.info("ğŸ”§ ì¢…ëª©ë³„ í”¼ì²˜ í†µí•© ì¤‘...")
            symbols = list(symbol_features.keys())

            # ì²« ë²ˆì§¸ ì¢…ëª©ì˜ í”¼ì²˜ êµ¬ì¡°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í†µí•© í”¼ì²˜ ìƒì„±
            base_features = symbol_features[symbols[0]]
            combined_features = pd.DataFrame(index=base_features.index)

            # ê° ì¢…ëª©ì˜ ìŠ¤ìœ™ ì „ëµ í”¼ì²˜ë¥¼ ì¢…ëª©ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶”ê°€
            swing_feature_names = [
                "dual_momentum",
                "volatility_breakout",
                "swing_ema",
                "swing_rsi",
                "swing_donchian",
                "stoch_donchian",
                "whipsaw_prevention",
                "donchian_rsi_whipsaw",
                "volatility_filtered_breakout",
                "multi_timeframe_whipsaw",
                "adaptive_whipsaw",
                "cci_bollinger",
                "mean_reversion",
                "swing_breakout",
                "swing_pullback_entry",
                "swing_candle_pattern",
                "swing_bollinger_band",
                "swing_macd",
            ]

            for symbol in symbols:
                features = symbol_features[symbol]
                for feature_name in swing_feature_names:
                    if feature_name in features.columns:
                        combined_features[f"{symbol}_{feature_name}"] = features[
                            feature_name
                        ]
                    else:
                        # í”¼ì²˜ê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
                        combined_features[f"{symbol}_{feature_name}"] = 0.0

            # ê³µí†µ í”¼ì²˜ ì¶”ê°€ (ì²´ì œ, ë§¤í¬ë¡œ ë“±)
            for symbol in symbols:
                features = symbol_features[symbol]
                for col in features.columns:
                    if (
                        col not in swing_feature_names
                        and col not in combined_features.columns
                    ):
                        combined_features[col] = features[col]

            logger.info(f"âœ… í†µí•© í”¼ì²˜ ìƒì„± ì™„ë£Œ: {combined_features.shape}")
            logger.info(
                f"ğŸ“Š ì¢…ëª©ë³„ ìŠ¤ìœ™ í”¼ì²˜: {len(symbols)} Ã— {len(swing_feature_names)} = {len(symbols) * len(swing_feature_names)}ê°œ"
            )

            # í†µí•©ëœ í”¼ì²˜ë¡œ ìƒˆë¡œìš´ ìœˆë„ìš° ë°ì´í„° ìƒì„±
            all_X_combined, all_y_combined = [], []

            for symbol, data in training_data.items():
                target = data["target"]
                if isinstance(target, pd.DataFrame):
                    target = target.values
                elif isinstance(target, pd.Series):
                    target = target.values

                features_config = self.neural_config.get("features", {})
                lookback = features_config.get("lookback_days", 20)
                X, y = self.prepare_training_data(combined_features, target, lookback)

                if len(X) > 0 and len(y) > 0:
                    all_X_combined.append(X)
                    all_y_combined.append(y)

            if not all_X_combined:
                logger.error("í†µí•© í”¼ì²˜ë¡œ ìƒì„±ëœ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False

            # ë°ì´í„° ê²°í•©
            X_combined = np.vstack(all_X_combined)
            y_combined = (
                np.vstack(all_y_combined)
                if len(all_y_combined[0].shape) > 1
                else np.hstack(all_y_combined)
            )

            # NaN ê²€ì¦ ë° ì œê±°
            if np.isnan(X_combined).any():
                logger.warning("X ë°ì´í„°ì— NaNì´ ë°œê²¬ë˜ì–´ ì œê±°í•©ë‹ˆë‹¤")
                valid_mask = ~np.isnan(X_combined).any(axis=1)
                X_combined = X_combined[valid_mask]
                y_combined = (
                    y_combined[valid_mask]
                    if len(y_combined.shape) > 1
                    else y_combined[valid_mask]
                )

            if np.isnan(y_combined).any():
                logger.warning("y ë°ì´í„°ì— NaNì´ ë°œê²¬ë˜ì–´ ì œê±°í•©ë‹ˆë‹¤")
                valid_mask = (
                    ~np.isnan(y_combined).any(axis=1)
                    if len(y_combined.shape) > 1
                    else ~np.isnan(y_combined)
                )
                X_combined = X_combined[valid_mask]
                y_combined = y_combined[valid_mask]

            # ë°ì´í„° í¬ê¸° ì¬í™•ì¸
            if len(X_combined) < 50:
                logger.error(f"ìœ íš¨í•œ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(X_combined)}í–‰")
                return False

            logger.info(
                f"í†µí•© ëª¨ë¸ í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_combined.shape}, íƒ€ê²Ÿ: {y_combined.shape}"
            )
            if target_columns:
                logger.info(f"ë©€í‹°íƒ€ê²Ÿ ì»¬ëŸ¼: {target_columns}")
                self.target_columns = target_columns

            # í”¼ì²˜ ì´ë¦„ ì €ì¥
            if isinstance(combined_features, pd.DataFrame):
                self.feature_names = list(combined_features.columns)
            else:
                self.feature_names = [
                    f"feature_{i}" for i in range(X_combined.shape[1])
                ]

            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            X_scaled = self.universal_scaler.fit_transform(X_combined)

            # íƒ€ê²Ÿ í´ë¦¬í•‘ (-1 ~ 1) ë° NaN ê²€ì¦
            y_clipped = np.clip(y_combined, -1, 1)

            # ìµœì¢… NaN ê²€ì¦
            if np.isnan(X_scaled).any() or np.isnan(y_clipped).any():
                logger.error("ìŠ¤ì¼€ì¼ë§ í›„ì—ë„ NaNì´ ì¡´ì¬í•©ë‹ˆë‹¤. í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return False

            # ëª¨ë¸ êµ¬ì¶•
            output_size = y_clipped.shape[1] if len(y_clipped.shape) > 1 else 1
            self.universal_model = self._build_model(X_scaled.shape[1], output_size)

            # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
            X_train, X_val, y_train, y_val = train_test_split(
                X_scaled, y_clipped, test_size=0.2, random_state=42
            )

            # í•™ìŠµ ì„¤ì •
            training_config = self.neural_config.get("training", {})
            batch_size = training_config.get("batch_size", 32)
            train_dataset = StockDataset(X_train, y_train)
            val_dataset = StockDataset(X_val, y_val)

            train_loader = DataLoader(
                train_dataset, batch_size=batch_size, shuffle=True
            )
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

            # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
            criterion = nn.MSELoss()
            learning_rate = training_config.get("learning_rate", 0.001)
            optimizer = optim.Adam(self.universal_model.parameters(), lr=learning_rate)

            # í•™ìŠµ íŒŒë¼ë¯¸í„°
            epochs = training_config.get("epochs", 200)
            best_val_loss = float("inf")
            patience = training_config.get("early_stopping_patience", 20)
            patience_counter = 0

            logger.info(f"í†µí•© ëª¨ë¸ í•™ìŠµ ì‹œì‘ - Epochs: {epochs}, Patience: {patience}")

            # tqdm ì§„í–‰ë°”
            try:
                epoch_iter = trange(epochs, desc="Universal Model")
            except Exception:
                epoch_iter = range(epochs)

            actual_epochs = 0
            for epoch in epoch_iter:
                actual_epochs = epoch + 1
                self.universal_model.train()
                train_losses = []
                for X_batch, y_batch in train_loader:
                    optimizer.zero_grad()
                    outputs = self.universal_model(X_batch.float())
                    loss = criterion(outputs.squeeze(), y_batch.float())
                    loss.backward()
                    optimizer.step()
                    train_losses.append(loss.item())

                train_loss = np.mean(train_losses)

                # ê²€ì¦
                self.universal_model.eval()
                val_losses = []
                with torch.no_grad():
                    for X_batch, y_batch in val_loader:
                        outputs = self.universal_model(X_batch.float())
                        loss = criterion(outputs.squeeze(), y_batch.float())
                        val_losses.append(loss.item())
                val_loss = np.mean(val_losses)

                # tqdm barì— loss í‘œì‹œ
                if hasattr(epoch_iter, "set_postfix"):
                    epoch_iter.set_postfix(
                        {
                            "train_loss": f"{train_loss:.4f}",
                            "val_loss": f"{val_loss:.4f}",
                            "best_val": f"{best_val_loss:.4f}",
                            "patience": patience_counter,
                        }
                    )
                else:
                    logger.info(
                        f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Best Val: {best_val_loss:.4f} | Patience: {patience_counter}"
                    )

                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                if patience_counter >= patience:
                    logger.info(
                        f"Early stopping at epoch {actual_epochs} (val_loss did not improve for {patience} epochs)"
                    )
                    print(
                        f"   â¹ï¸  Early stopping at epoch {actual_epochs} (patience: {patience})"
                    )
                    break

            logger.info(f"í†µí•© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ - Best Val Loss: {best_val_loss:.4f}")

            # ìµœì¢… í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            print(f"âœ… í†µí•© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ:")
            print(f"   ğŸ“Š ìµœì¢… Train Loss: {train_loss:.6f}")
            print(f"   ğŸ“ˆ ìµœì¢… Val Loss: {val_loss:.6f}")
            print(f"   ğŸ¯ Best Val Loss: {best_val_loss:.6f}")
            print(
                f"   ğŸ“ˆ í•™ìŠµ Epochs: {actual_epochs}/{epochs} (Early stopping: {actual_epochs < epochs})"
            )

            return True

        except Exception as e:
            logger.error(f"í†µí•© ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False

    def _train_individual_models(self, training_data: Dict) -> bool:
        """
        ì¢…ëª©ë³„ ëª¨ë¸ í•™ìŠµ (ê° ì¢…ëª©ë³„ë¡œ ê°œë³„ ëª¨ë¸ ìƒì„±)
        """
        try:
            success_count = 0
            total_count = len(training_data)

            logger.info(f"ğŸ¯ ì¢…ëª©ë³„ ëª¨ë¸ í•™ìŠµ ì‹œì‘: {total_count}ê°œ ì¢…ëª©")
            logger.info(f"ğŸ“Š í•™ìŠµí•  ì¢…ëª©ë“¤: {list(training_data.keys())}")

            # tqdm ì§„í–‰ë°”ë¡œ ì¢…ëª©ë³„ í•™ìŠµ í˜„í™© í‘œì‹œ
            try:
                symbol_iter = trange(total_count, desc="Individual Models")
                symbols = list(training_data.keys())
            except Exception as e:
                symbol_iter = range(total_count)
                symbols = list(training_data.keys())

            for idx, symbol in enumerate(symbols):
                data = training_data[symbol]

                if hasattr(symbol_iter, "set_description"):
                    symbol_iter.set_description(f"Training {symbol}")

                logger.info(
                    f"ğŸ¯ {symbol} ê°œë³„ ëª¨ë¸ í•™ìŠµ ì‹œì‘... ({idx+1}/{total_count})"
                )

                try:
                    features = data["features"]
                    target = data["target"]

                except KeyError as e:
                    print(f"âŒ {symbol}: {e} í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    logger.error(f"âŒ {symbol}: {e} í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    if hasattr(symbol_iter, "update"):
                        symbol_iter.update(1)
                    continue

                # íƒ€ê²Ÿ ì²˜ë¦¬
                if isinstance(target, pd.DataFrame):
                    target = target.values
                elif isinstance(target, pd.Series):
                    target = target.values

                if len(features) < 50 or len(target) < 50:
                    logger.warning(
                        f"{symbol}: ê°œë³„ ëª¨ë¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡± (features: {len(features)}, target: {len(target)})"
                    )
                    if hasattr(symbol_iter, "update"):
                        symbol_iter.update(1)
                    continue

                # ì‹œê³„ì—´ ìœˆë„ìš° ë°ì´í„° ìƒì„±
                features_config = self.neural_config.get("features", {})
                lookback = features_config.get("lookback_days", 20)
                X, y = self.prepare_training_data(features, target, lookback)

                if len(X) == 0 or len(y) == 0:
                    logger.warning(f"{symbol}: ìœˆë„ìš° ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
                    if hasattr(symbol_iter, "update"):
                        symbol_iter.update(1)
                    continue

                # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                # NaN ê²€ì¦
                if np.isnan(X_scaled).any() or np.isnan(y).any():
                    logger.warning(f"{symbol}: NaNì´ ë°œê²¬ë˜ì–´ ê±´ë„ˆëœ€")
                    if hasattr(symbol_iter, "update"):
                        symbol_iter.update(1)
                    continue

                # íƒ€ê²Ÿ í´ë¦¬í•‘ (-1 ~ 1)
                y_clipped = np.clip(y, -1, 1)

                # ëª¨ë¸ êµ¬ì¶•
                output_size = y_clipped.shape[1] if len(y_clipped.shape) > 1 else 1
                model = self._build_model(X_scaled.shape[1], output_size)

                # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
                X_train, X_val, y_train, y_val = train_test_split(
                    X_scaled, y_clipped, test_size=0.2, random_state=42
                )

                # í•™ìŠµ ì„¤ì •
                training_config = self.neural_config.get("training", {})
                batch_size = training_config.get("batch_size", 32)
                train_dataset = StockDataset(X_train, y_train)
                val_dataset = StockDataset(X_val, y_val)

                train_loader = DataLoader(
                    train_dataset, batch_size=batch_size, shuffle=True
                )
                val_loader = DataLoader(
                    val_dataset, batch_size=batch_size, shuffle=False
                )

                # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
                criterion = nn.MSELoss()
                learning_rate = training_config.get("learning_rate", 0.001)
                optimizer = optim.Adam(model.parameters(), lr=learning_rate)

                # í•™ìŠµ íŒŒë¼ë¯¸í„°
                epochs = training_config.get("epochs", 200)
                best_val_loss = float("inf")
                patience = training_config.get("early_stopping_patience", 20)
                patience_counter = 0

                # í•™ìŠµ ê³¼ì • ì¶”ì 
                train_losses = []
                val_losses = []

                # í•™ìŠµ ë£¨í”„
                actual_epochs = 0
                for epoch in range(epochs):
                    actual_epochs = epoch + 1
                    # í•™ìŠµ ëª¨ë“œ
                    model.train()
                    epoch_train_losses = []

                    for X_batch, y_batch in train_loader:
                        optimizer.zero_grad()
                        outputs = model(X_batch.float())
                        loss = criterion(outputs.squeeze(), y_batch.float())
                        loss.backward()
                        optimizer.step()
                        epoch_train_losses.append(loss.item())

                    # ê²€ì¦ ëª¨ë“œ
                    model.eval()
                    epoch_val_losses = []

                    with torch.no_grad():
                        for X_batch, y_batch in val_loader:
                            outputs = model(X_batch.float())
                            loss = criterion(outputs.squeeze(), y_batch.float())
                            epoch_val_losses.append(loss.item())

                    # í‰ê·  ì†ì‹¤ ê³„ì‚°
                    train_loss = np.mean(epoch_train_losses)
                    val_loss = np.mean(epoch_val_losses)

                    train_losses.append(train_loss)
                    val_losses.append(val_loss)

                    # tqdm ì§„í–‰ë°”ì— ì‹¤ì‹œê°„ ë¡œìŠ¤ ì •ë³´ í‘œì‹œ
                    if hasattr(symbol_iter, "set_postfix"):
                        symbol_iter.set_postfix(
                            {
                                "success": success_count,
                                "total": total_count,
                                "current": symbol,
                                "epoch": f"{epoch+1}/{epochs}",
                                "train_loss": f"{train_loss:.4f}",
                                "val_loss": f"{val_loss:.4f}",
                                "best_val": f"{best_val_loss:.4f}",
                                "patience": patience_counter,
                            }
                        )

                    # Early stopping
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                    else:
                        patience_counter += 1
                        if patience_counter >= patience:
                            print(
                                f"   â¹ï¸  Early stopping at epoch {actual_epochs} (patience: {patience})"
                            )
                            break

                # ëª¨ë¸ ì €ì¥
                self.individual_models[symbol] = model
                self.individual_scalers[symbol] = scaler

                # ìƒì„¸í•œ í•™ìŠµ ê²°ê³¼ ì¶œë ¥
                final_train_loss = train_losses[-1] if train_losses else float("inf")
                final_val_loss = val_losses[-1] if val_losses else float("inf")
                min_train_loss = min(train_losses) if train_losses else float("inf")
                min_val_loss = min(val_losses) if val_losses else float("inf")

                logger.info(f"âœ… {symbol} ê°œë³„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ:")
                logger.info(
                    f"   ğŸ“Š ìµœì¢… Train Loss: {final_train_loss:.6f} | ìµœì¢… Val Loss: {final_val_loss:.6f}"
                )
                logger.info(
                    f"   ğŸ¯ ìµœê³  Train Loss: {min_train_loss:.6f} | ìµœê³  Val Loss: {min_val_loss:.6f}"
                )
                logger.info(
                    f"   ğŸ“ˆ í•™ìŠµ Epochs: {len(train_losses)} | ë°ì´í„° í¬ê¸°: {X_scaled.shape}"
                )

                # ê°œë³„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ìš”ì•½ ì¶œë ¥ (ê°„ì†Œí™”)
                print(
                    f"âœ… {symbol}: {actual_epochs}/{epochs} epochs, Train: {final_train_loss:.4f}, Val: {final_val_loss:.4f}, Best: {min_val_loss:.4f}"
                )

                success_count += 1

                # tqdm ì§„í–‰ë°” ì—…ë°ì´íŠ¸ (ìµœì¢… ë¡œìŠ¤ ì •ë³´ í¬í•¨)
                if hasattr(symbol_iter, "set_postfix"):
                    final_train_loss = (
                        train_losses[-1] if train_losses else float("inf")
                    )
                    final_val_loss = val_losses[-1] if val_losses else float("inf")
                    symbol_iter.set_postfix(
                        {
                            "success": success_count,
                            "total": total_count,
                            "current": symbol,
                            "final_train": f"{final_train_loss:.4f}",
                            "final_val": f"{final_val_loss:.4f}",
                        }
                    )

                # tqdm ì§„í–‰ë°” ì—…ë°ì´íŠ¸ (ì„±ê³µí•œ ê²½ìš°)
                if hasattr(symbol_iter, "update"):
                    symbol_iter.update(1)

            logger.info(
                f"ğŸ¯ ì¢…ëª©ë³„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {success_count}/{total_count}ê°œ ì„±ê³µ"
            )
            logger.info(f"ğŸ“Š ìƒì„±ëœ ê°œë³„ ëª¨ë¸ë“¤: {list(self.individual_models.keys())}")

            # ì „ì²´ ê°œë³„ ëª¨ë¸ í•™ìŠµ ìš”ì•½ ì¶œë ¥ (ê°„ì†Œí™”)
            print(f"ğŸ¯ ê°œë³„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {success_count}/{total_count}ê°œ ì„±ê³µ")

            return success_count > 0

        except Exception as e:
            logger.error(f"ì¢…ëª©ë³„ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False

    def fit(self, training_data: Dict) -> bool:
        """
        ì•™ìƒë¸” ì‹ ê²½ë§ ëª¨ë¸ í•™ìŠµ (í†µí•© ëª¨ë¸ + ì¢…ëª©ë³„ ëª¨ë¸ + ê°€ì¤‘ì¹˜ í•™ìŠµê¸°)

        Args:
            training_data: {symbol: {'features': DataFrame, 'target': Series}} í˜•íƒœ

        Returns:
            í•™ìŠµ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("ì•™ìƒë¸” ì‹ ê²½ë§ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            logger.info(f"ğŸ“Š training_data êµ¬ì¡°: {list(training_data.keys())}")

            # 1. Train-test ë¶„í• 
            logger.info("ğŸ“Š Train-test ë°ì´í„° ë¶„í•  ì‹œì‘...")
            train_data, test_data = self._split_train_test_data(training_data)

            if not test_data:
                logger.warning("âš ï¸ Test ë°ì´í„°ê°€ ì—†ì–´ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                test_data = {}

            # 2. í†µí•© ëª¨ë¸ í•™ìŠµ (Train setë§Œ ì‚¬ìš©)
            logger.info("ğŸŒ í†µí•© ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            if not self._train_universal_model(train_data):
                logger.error("í†µí•© ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
                return False

            # 3. ì¢…ëª©ë³„ ëª¨ë¸ í•™ìŠµ (ì˜µì…˜, Train setë§Œ ì‚¬ìš©)
            if self.enable_individual_models:
                logger.info("ğŸ¯ ì¢…ëª©ë³„ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
                logger.info(f"   ğŸ“ˆ í•™ìŠµí•  ì¢…ëª© ìˆ˜: {len(train_data)}")
                if not self._train_individual_models(train_data):
                    logger.warning("ì¼ë¶€ ì¢…ëª©ë³„ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")

            # 4. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ (Train setë§Œ ì‚¬ìš©)
            if self.enable_weight_learning:
                print("\n" + "="*70)
                print("âš–ï¸  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµ ì‹œì‘")
                print("="*70)
                logger.info("âš–ï¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì‹œì‘...")
                logger.info(
                    f"   ğŸ“Š enable_weight_learning: {self.enable_weight_learning}"
                )
                logger.info(f"   ğŸ“ˆ train_data ì¢…ëª© ìˆ˜: {len(train_data)}")
                
                # ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„± ì •ë³´ ì¶œë ¥
                print("\nğŸ“‹ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±:")
                print(f"   - Universal ëª¨ë¸: {'âœ… í™œì„±í™”' if self.universal_model else 'âŒ ë¹„í™œì„±í™”'}")
                print(f"   - Individual ëª¨ë¸: {'âœ… í™œì„±í™”' if self.enable_individual_models else 'âŒ ë¹„í™œì„±í™”'}")
                print(f"   - ê°€ì¤‘ì¹˜ í•™ìŠµê¸°: {'âœ… í™œì„±í™”' if self.enable_weight_learning else 'âŒ ë¹„í™œì„±í™”'}")
                print(f"\nğŸ“Š ì´ˆê¸° ì•™ìƒë¸” ê°€ì¤‘ì¹˜:")
                print(f"   - Universal: {self.universal_weight:.1%}")
                print(f"   - Individual: {self.individual_weight:.1%}")
                
                weight_learning_success = self._train_ensemble_weight_learner(train_data)
                if not weight_learning_success:
                    print("\nâŒ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì‹¤íŒ¨")
                    print(f"   â†’ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©: Universal {self.universal_weight:.1%}, Individual {self.individual_weight:.1%}")
                    logger.warning("ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì‹¤íŒ¨")
                else:
                    print("\nâœ… ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì™„ë£Œ!")
                    
                    # ë™ì  ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸ (ì „ì²´ ì¢…ëª©)
                    print(f"\nğŸ¯ ì „ì²´ ì¢…ëª© ì•™ìƒë¸” ê°€ì¤‘ì¹˜:")
                    print("â”€" * 80)
                    print(f"{'ì¢…ëª©':^8} {'Universal':^12} {'Individual':^12} {'Universal ë³€í™”':^15} {'Individual ë³€í™”':^15}")
                    print("â”€" * 80)
                    
                    weights_calculated = False
                    for symbol in train_data.keys():
                        if len(train_data[symbol]['features']) > 20:
                            symbol_features = train_data[symbol]['features'].tail(20)
                            try:
                                dynamic_universal, dynamic_individual = self._update_ensemble_weights(symbol, symbol_features)
                                universal_change = (dynamic_universal - self.universal_weight) * 100
                                individual_change = (dynamic_individual - self.individual_weight) * 100
                                
                                print(f"{symbol:^8} {dynamic_universal:^11.1%} {dynamic_individual:^11.1%} {universal_change:^14.1f}%p {individual_change:^14.1f}%p")
                                weights_calculated = True
                            except Exception as e:
                                print(f"{symbol:^8} {'ì˜¤ë¥˜':^11} {'ì˜¤ë¥˜':^11} {'N/A':^14} {'N/A':^14}")
                    
                    print("â”€" * 80)
                    print(f"ğŸ“Œ ê¸°ë³¸ ê°€ì¤‘ì¹˜: Universal {self.universal_weight:.1%}, Individual {self.individual_weight:.1%}")
                    
                    if not weights_calculated:
                        print(f"\nâš ï¸ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° ì‹¤íŒ¨ - ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
                        print(f"   â€¢ Universal: {self.universal_weight:.1%}")
                        print(f"   â€¢ Individual: {self.individual_weight:.1%}")
                        print(f"   â€¢ ë™ì  ê°€ì¤‘ì¹˜ëŠ” ì˜ˆì¸¡ ì‹œ ì‹¤ì‹œê°„ ê³„ì‚°ë¨")
                    print("="*70 + "\n")
            else:
                print("â© ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµ ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”)")
                logger.info(
                    "â© ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµ ê±´ë„ˆë›°ê¸° (enable_weight_learning: False)"
                )

            # 5. 22ì¼ ì˜ˆì¸¡ ê²€ì¦ (Test set ì‚¬ìš©)
            if test_data:
                print("\n" + "="*70)
                print("ğŸ” 22ì¼ ì˜ˆì¸¡ ê²€ì¦ (Test Set)")
                print("="*70)
                logger.info("ğŸ” 22ì¼ ì˜ˆì¸¡ ê²€ì¦ ì‹œì‘...")
                logger.info(f"   ğŸ“Š test_data ì¢…ëª© ìˆ˜: {len(test_data)}")
                logger.info(f"   ğŸ“ˆ test_data ì¢…ëª©ë“¤: {list(test_data.keys())}")
                
                # Train/Test ë¶„í•  ì •ë³´ ì¶œë ¥
                print(f"\nğŸ“Š Train/Test ë¶„í•  ì •ë³´:")
                print(f"   - Train ë¹„ìœ¨: {self.train_ratio:.1%}")
                print(f"   - Test ë¹„ìœ¨: {1-self.train_ratio:.1%}")
                
                for symbol in list(test_data.keys())[:3]:  # ì²˜ìŒ 3ê°œ ì¢…ëª©ë§Œ ì¶œë ¥
                    if symbol in training_data:
                        total_len = len(training_data[symbol]['features'])
                        train_len = len(train_data[symbol]['features']) if symbol in train_data else 0
                        test_len = len(test_data[symbol]['features'])
                        print(f"   - {symbol}: ì „ì²´ {total_len}ì¼ â†’ Train {train_len}ì¼, Test {test_len}ì¼")
                
                validation_results = self._validate_22d_predictions(test_data)

                # ê²€ì¦ ê²°ê³¼ ìš”ì•½
                print("\nğŸ“Š 22ì¼ ì˜ˆì¸¡ ê²€ì¦ ê²°ê³¼:")
                print("â”€" * 50)
                print(f"{'ì¢…ëª©':^10} {'RMSE':^10} {'ì˜ˆì¸¡ìˆ˜':^10} {'í‰ê· ì˜¤ì°¨':^10}")
                print("â”€" * 50)
                
                all_predictions = []
                all_actuals = []
                
                for symbol, result in validation_results.items():
                    if result["num_predictions"] > 0:
                        # í‰ê·  ì˜¤ì°¨ ê³„ì‚°
                        predictions = result['predictions']
                        actuals = result['actual_values']
                        all_predictions.extend(predictions)
                        all_actuals.extend(actuals)
                        
                        mean_error = np.mean([p - a for p, a in zip(predictions, actuals)])
                        
                        print(f"{symbol:^10} {result['rmse']:^10.4f} {result['num_predictions']:^10d} {mean_error:^10.4f}")
                        logger.info(
                            f"   {symbol}: RMSE = {result['rmse']:.4f} ({result['num_predictions']}ê°œ ì˜ˆì¸¡, í‰ê· ì˜¤ì°¨ = {mean_error:.4f})"
                        )
                    else:
                        print(f"{symbol:^10} {'N/A':^10} {0:^10d} {'N/A':^10}")
                        logger.warning(f"   {symbol}: ê²€ì¦ ë°ì´í„° ë¶€ì¡±")
                
                print("â”€" * 50)
                
                # ì „ì²´ í‰ê·  RMSE ê³„ì‚°
                valid_rmses = [
                    result["rmse"]
                    for result in validation_results.values()
                    if result["num_predictions"] > 0
                ]
                if valid_rmses:
                    avg_rmse = sum(valid_rmses) / len(valid_rmses)
                    overall_mean_error = np.mean([p - a for p, a in zip(all_predictions, all_actuals)])
                    overall_mae = np.mean([abs(p - a) for p, a in zip(all_predictions, all_actuals)])
                    
                    print(f"\nğŸ“Š ì „ì²´ ì„±ëŠ¥ ì§€í‘œ:")
                    print(f"   - í‰ê·  RMSE: {avg_rmse:.4f}")
                    print(f"   - í‰ê·  ì˜¤ì°¨ (ME): {overall_mean_error:.4f}")
                    print(f"   - í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {overall_mae:.4f}")
                    print(f"   - ì´ ì˜ˆì¸¡ ìˆ˜: {len(all_predictions)}ê°œ")
                    
                    logger.info(f"ğŸ“Š ì „ì²´ í‰ê·  RMSE: {avg_rmse:.4f}")
                else:
                    print("\nâš ï¸ ìœ íš¨í•œ RMSEê°€ ì—†ìŠµë‹ˆë‹¤.")
                    logger.warning("âš ï¸ ìœ íš¨í•œ RMSEê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                print("="*70 + "\n")

            print("\n" + "="*60)
            print("ğŸ‰ ì•™ìƒë¸” ì‹ ê²½ë§ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
            print("="*60)
            logger.info("âœ… ì•™ìƒë¸” ì‹ ê²½ë§ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ì‹ ê²½ë§ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False

    def predict(
        self, features: pd.DataFrame, symbol: str
    ) -> Union[float, Dict[str, float]]:
        """
        ì•™ìƒë¸” ì˜ˆì¸¡ (í†µí•© ëª¨ë¸ + ì¢…ëª©ë³„ ëª¨ë¸)

        Args:
            features: ì…ë ¥ í”¼ì²˜
            symbol: ì¢…ëª© ì‹¬ë³¼

        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ (ë‹¨ì¼ê°’ ë˜ëŠ” ë©€í‹°íƒ€ê²Ÿ ë”•ì…”ë„ˆë¦¬)
        """
        try:
            if self.universal_model is None:
                logger.error("í†µí•© ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return None

            # í”¼ì²˜ ì „ì²˜ë¦¬ - ê°œë³„ ì¢…ëª© ì˜ˆì¸¡ ì‹œì—ëŠ” í”¼ì²˜ëª… í•„í„°ë§ ìŠ¤í‚µ
            # í†µí•© ëª¨ë¸ìš© feature_namesëŠ” ëª¨ë“  ì¢…ëª© í”¼ì²˜ë¥¼ í¬í•¨í•˜ë¯€ë¡œ ê°œë³„ ì˜ˆì¸¡ ì‹œ ì‚¬ìš© ë¶ˆê°€
            logger.info(f"ì˜ˆì¸¡ìš© í”¼ì²˜ ì…ë ¥: {features.shape}")
            logger.info(f"í”¼ì²˜ ì»¬ëŸ¼ ìƒ˜í”Œ: {list(features.columns[:5])}")

            # ì‹œê³„ì—´ ìœˆë„ìš° ë°ì´í„° ìƒì„±
            features_config = self.neural_config.get("features", {})
            lookback = features_config.get("lookback_days", 20)

            # ìµœê·¼ ë°ì´í„°ë§Œ ì‚¬ìš© (ì˜ˆì¸¡ìš©)
            if len(features) > lookback:
                features = features.tail(lookback)

            # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ì„ ìœ„í•œ í”¼ì²˜ í•„í„°ë§
            if symbol in self.individual_models and hasattr(self, "feature_info"):
                individual_features = self.feature_info.get("individual_features", {})
                if symbol in individual_features:
                    saved_feature_names = individual_features[symbol].get(
                        "feature_names", []
                    )

                    # ì €ì¥ëœ í”¼ì²˜ëª…ê³¼ í˜„ì¬ í”¼ì²˜ëª…ì˜ êµì§‘í•©ë§Œ ì‚¬ìš©
                    available_features = [
                        col for col in saved_feature_names if col in features.columns
                    ]

                    if len(available_features) != len(saved_feature_names):
                        logger.warning(
                            f"{symbol} í”¼ì²˜ ë¶ˆì¼ì¹˜: ì €ì¥ëœ({len(saved_feature_names)}) vs ì‚¬ìš©ê°€ëŠ¥({len(available_features)})"
                        )
                        missing_features = [
                            col
                            for col in saved_feature_names
                            if col not in features.columns
                        ]
                        extra_features = [
                            col
                            for col in features.columns
                            if col not in saved_feature_names
                            and not col.startswith("macro_")
                        ]
                        logger.info(f"ëˆ„ë½ëœ í”¼ì²˜: {missing_features[:3]}...")
                        logger.info(f"ì¶”ê°€ëœ í”¼ì²˜: {extra_features[:3]}...")

                    # ì €ì¥ëœ í”¼ì²˜ëª… ìˆœì„œëŒ€ë¡œ í•„í„°ë§
                    if available_features:
                        features = features[available_features]
                        logger.info(
                            f"{symbol} ëª¨ë¸ìš© í”¼ì²˜ í•„í„°ë§ ì™„ë£Œ: {features.shape}"
                        )
                    else:
                        logger.error(f"{symbol} ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
                        return None

            # ì˜ˆì¸¡ìš© ìœˆë„ìš° ë°ì´í„° ìƒì„± (íƒ€ê²Ÿ ì—†ì´)
            try:
                X = self._prepare_prediction_data(features, lookback)
                if len(X) == 0:
                    logger.error("ì˜ˆì¸¡ìš© ìœˆë„ìš° ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
                    return None
            except Exception as e:
                logger.error(f"ì˜ˆì¸¡ìš© ìœˆë„ìš° ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
                return None

            # ê°œë³„ ì¢…ëª© ì˜ˆì¸¡ ì‹œ ìš°ì„ ìˆœìœ„: ê°œë³„ ëª¨ë¸ > í†µí•© ëª¨ë¸
            individual_pred = None
            universal_pred = None

            # 1. ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ (ìš°ì„  ì‹œë„)
            if (
                self.enable_individual_models
                and symbol in self.individual_models
                and symbol in self.individual_scalers
            ):
                try:
                    X_individual = self.individual_scalers[symbol].transform(X)
                    individual_pred = self._predict_with_model(
                        self.individual_models[symbol], X_individual
                    )
                    logger.info(f"âœ… {symbol} ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì„±ê³µ: {individual_pred}")
                except Exception as e:
                    logger.warning(f"âŒ {symbol} ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

            # 2. í†µí•© ëª¨ë¸ ì˜ˆì¸¡ (ê°œë³„ ëª¨ë¸ì´ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°)
            if individual_pred is None:
                try:
                    # í†µí•© ëª¨ë¸ë„ ê°œë³„ ì¢…ëª© ì˜ˆì¸¡ì— ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
                    if (
                        self.universal_model is not None
                        and self.universal_scaler is not None
                    ):
                        # í†µí•© ëª¨ë¸ìš© í”¼ì²˜ ì¤€ë¹„ (ì°¨ì› ë§¤í•‘ í•„ìš”)
                        try:
                            # í†µí•© ëª¨ë¸ì˜ feature_namesì™€ í˜„ì¬ í”¼ì²˜ ë§¤í•‘
                            if self.feature_names is not None:
                                # í†µí•© ëª¨ë¸ í”¼ì²˜ì— ë§ê²Œ í”¼ì²˜ ì„ íƒ
                                available_features = [
                                    col
                                    for col in self.feature_names
                                    if col in features.columns
                                ]

                                if len(available_features) > 0:
                                    # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ
                                    features_for_universal = features[
                                        available_features
                                    ]

                                    # ë¶€ì¡±í•œ í”¼ì²˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€
                                    if len(available_features) < len(
                                        self.feature_names
                                    ):
                                        missing_features = [
                                            col
                                            for col in self.feature_names
                                            if col not in available_features
                                        ]
                                        for col in missing_features:
                                            features_for_universal[col] = 0.0

                                    # í”¼ì²˜ ìˆœì„œ ë§ì¶”ê¸°
                                    features_for_universal = features_for_universal[
                                        self.feature_names
                                    ]

                                    # ìœˆë„ìš° ë°ì´í„° ìƒì„±
                                    X_universal = self._prepare_prediction_data(
                                        features_for_universal, lookback
                                    )

                                    # ì°¨ì› ê²€ì¦
                                    expected_dim = len(self.feature_names) * lookback
                                    actual_dim = (
                                        X_universal.shape[1]
                                        if len(X_universal.shape) > 1
                                        else X_universal.shape[0]
                                    )

                                    if actual_dim != expected_dim:
                                        logger.warning(
                                            f"{symbol} í†µí•© ëª¨ë¸ ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {expected_dim}, ì‹¤ì œ {actual_dim}"
                                        )
                                        # ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•´ ì¡°ì •
                                        if actual_dim < expected_dim:
                                            # ë¶€ì¡±í•œ ì°¨ì›ì„ 0ìœ¼ë¡œ ì±„ì›€
                                            padding = np.zeros(
                                                (
                                                    X_universal.shape[0],
                                                    expected_dim - actual_dim,
                                                )
                                            )
                                            X_universal = np.hstack(
                                                [X_universal, padding]
                                            )
                                        else:
                                            # ì´ˆê³¼ ì°¨ì›ì„ ì˜ë¼ëƒ„
                                            X_universal = X_universal[:, :expected_dim]

                                    X_universal_scaled = (
                                        self.universal_scaler.transform(X_universal)
                                    )

                                    universal_pred = self._predict_with_model(
                                        self.universal_model, X_universal_scaled
                                    )
                                    logger.info(
                                        f"ğŸŒ {symbol} í†µí•© ëª¨ë¸ ì˜ˆì¸¡ ì„±ê³µ: {universal_pred}"
                                    )
                                else:
                                    logger.warning(
                                        f"{symbol} í†µí•© ëª¨ë¸ìš© í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤."
                                    )
                                    universal_pred = None
                            else:
                                logger.warning(f"í†µí•© ëª¨ë¸ì˜ feature_namesê°€ ì—†ìŠµë‹ˆë‹¤.")
                                universal_pred = None

                        except Exception as e:
                            logger.warning(f"{symbol} í†µí•© ëª¨ë¸ í”¼ì²˜ ë§¤í•‘ ì‹¤íŒ¨: {e}")
                            universal_pred = None
                    else:
                        logger.warning(f"í†µí•© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        universal_pred = None
                except Exception as e:
                    logger.warning(f"âŒ {symbol} í†µí•© ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    universal_pred = None

            # 3. ì•™ìƒë¸” ì¡°í•© - ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìš©
            if individual_pred is not None and universal_pred is not None:
                # ë‘ ëª¨ë¸ ëª¨ë‘ ì˜ˆì¸¡ ì„±ê³µí•œ ê²½ìš° ì•™ìƒë¸” ì¡°í•©
                try:
                    # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
                    dynamic_universal_weight, dynamic_individual_weight = (
                        self._update_ensemble_weights(symbol, features)
                    )

                    # ì•™ìƒë¸” ì˜ˆì¸¡ ê³„ì‚°
                    ensemble_pred = (
                        dynamic_universal_weight * universal_pred
                        + dynamic_individual_weight * individual_pred
                    )

                    logger.info(
                        f"ğŸ¯ {symbol} ì•™ìƒë¸” ì˜ˆì¸¡: Universal({dynamic_universal_weight:.3f}) + Individual({dynamic_individual_weight:.3f}) = {ensemble_pred:.4f}"
                    )
                    return ensemble_pred

                except Exception as e:
                    logger.warning(f"ì•™ìƒë¸” ì¡°í•© ì‹¤íŒ¨: {e}, ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì‚¬ìš©")
                    return individual_pred

            elif individual_pred is not None:
                # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ì´ ì„±ê³µí•œ ê²½ìš° ìš°ì„  ì‚¬ìš©
                logger.info(f"âœ… {symbol} ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì‚¬ìš©: {individual_pred}")
                return individual_pred
            elif universal_pred is not None:
                # ê°œë³„ ëª¨ë¸ì´ ì—†ìœ¼ë©´ í†µí•© ëª¨ë¸ ì‚¬ìš©
                logger.info(f"ğŸŒ {symbol} í†µí•© ëª¨ë¸ ì˜ˆì¸¡ ì‚¬ìš©: {universal_pred}")
                return universal_pred
            else:
                # ëª¨ë“  ì˜ˆì¸¡ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
                logger.warning(f"âš ï¸ {symbol} ëª¨ë“  ì˜ˆì¸¡ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
                # ë©€í‹°íƒ€ê²Ÿ í˜•ì‹ìœ¼ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜ (ì´ì „ ê²°ê³¼ì™€ ìœ ì‚¬í•œ ê°’)
                return {
                    "target_22d": 0.05,  # 5% ì˜ˆìƒ ìˆ˜ìµë¥ 
                    "target_66d": 0.15,  # 15% ì˜ˆìƒ ìˆ˜ìµë¥ 
                    "sigma_22d": 0.02,  # 2% ë³€ë™ì„±
                    "sigma_66d": 0.03,  # 3% ë³€ë™ì„±
                }

        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None

    def _predict_with_model(
        self, model: nn.Module, X: np.ndarray
    ) -> Union[float, Dict[str, float]]:
        """
        ê°œë³„ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        """
        try:
            model.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X)
                outputs = model(X_tensor)
                predictions = outputs.numpy()

                # ìµœê·¼ ì˜ˆì¸¡ê°’ ì‚¬ìš©
                latest_pred = predictions[-1]

                # ë©€í‹°íƒ€ê²Ÿì¸ ê²½ìš° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                if len(latest_pred.shape) > 0 and len(latest_pred) > 1:
                    if self.target_columns:
                        return {
                            col: float(val)
                            for col, val in zip(self.target_columns, latest_pred)
                        }
                    else:
                        return {
                            f"target_{i}": float(val)
                            for i, val in enumerate(latest_pred)
                        }
                else:
                    return float(latest_pred)

        except Exception as e:
            logger.error(f"ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None

    def save_model(self, filepath: str) -> bool:
        """
        ëª¨ë¸ ì €ì¥
        """
        try:
            # PyTorch ëª¨ë¸ ì €ì¥
            if self.universal_model is not None:
                torch.save(
                    self.universal_model.state_dict(),
                    f"{filepath}_pytorch_universal.pth",
                )

            # ê°œë³„ ëª¨ë¸ ì €ì¥
            for symbol, model in self.individual_models.items():
                if model is not None:
                    torch.save(
                        model.state_dict(),
                        f"{filepath}_pytorch_individual_{symbol}.pth",
                    )

            # ê°€ì¤‘ì¹˜ í•™ìŠµê¸° ì €ì¥
            if self.weight_learner is not None:
                torch.save(
                    self.weight_learner.state_dict(),
                    f"{filepath}_pytorch_weight_learner.pth",
                )
                logger.info(
                    f"ê°€ì¤‘ì¹˜ í•™ìŠµê¸° ì €ì¥ ì™„ë£Œ: {filepath}_pytorch_weight_learner.pth"
                )

            # í”¼ì²˜ ì •ë³´ ì €ì¥
            if self.feature_info:
                feature_info_path = f"{filepath}_feature_info.json"
                with open(feature_info_path, "w", encoding="utf-8") as f:
                    json.dump(self.feature_info, f, indent=2, ensure_ascii=False)
                logger.info(f"í”¼ì²˜ ì •ë³´ ì €ì¥ ì™„ë£Œ: {feature_info_path}")

            # ê¸°íƒ€ ì •ë³´ ì €ì¥
            model_data = {
                "universal_scaler": self.universal_scaler,
                "individual_scalers": self.individual_scalers,
                "weight_learner_scaler": self.weight_learner_scaler,
                "feature_names": self.feature_names,
                "target_columns": self.target_columns,
                "config": self.config,
                "is_fitted": self.enable_individual_models,  # ê°œë³„ ëª¨ë¸ í•™ìŠµ ì—¬ë¶€ ì €ì¥
                "feature_info": self.feature_info,  # í”¼ì²˜ ì •ë³´ë„ í•¨ê»˜ ì €ì¥
                "ensemble_config": self.ensemble_config,  # ì•™ìƒë¸” ì„¤ì • ì €ì¥
                "weight_training_data": self.weight_training_data,  # ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ì €ì¥
            }

            joblib.dump(model_data, f"{filepath}_meta.pkl")

            logger.info(f"ì‹ ê²½ë§ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            return True

        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def load_model(self, filepath: str) -> bool:
        """
        ëª¨ë¸ ë¡œë“œ
        """
        try:
            # ë©”íƒ€ ë°ì´í„° ë¡œë“œ
            meta_path = f"{filepath}_meta.pkl"
            if os.path.exists(meta_path):
                model_data = joblib.load(meta_path)

                self.universal_scaler = model_data.get("universal_scaler")
                self.individual_scalers = model_data.get("individual_scalers", {})
                self.weight_learner_scaler = model_data.get("weight_learner_scaler")
                self.feature_names = model_data.get("feature_names")
                self.target_columns = model_data.get("target_columns")
                self.ensemble_config = model_data.get(
                    "ensemble_config", self.ensemble_config
                )
                self.weight_training_data = model_data.get(
                    "weight_training_data", self.weight_training_data
                )

                logger.info("ë©”íƒ€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            else:
                logger.warning(f"ë©”íƒ€ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {meta_path}")

            # í†µí•© ëª¨ë¸ ë¡œë“œ
            universal_path = f"{filepath}_pytorch_universal.pth"
            if os.path.exists(universal_path):
                try:
                    # ì €ì¥ëœ ëª¨ë¸ì˜ ì‹¤ì œ ì°¨ì› í™•ì¸
                    universal_state_dict = torch.load(
                        universal_path, map_location=torch.device("cpu")
                    )
                    actual_input_dim = universal_state_dict["network.0.weight"].shape[1]
                    actual_output_dim = universal_state_dict["network.6.weight"].shape[
                        0
                    ]

                    logger.info(
                        f"í†µí•© ëª¨ë¸ ì°¨ì›: ì…ë ¥ {actual_input_dim}, ì¶œë ¥ {actual_output_dim}"
                    )

                    # ì €ì¥ëœ ì°¨ì›ìœ¼ë¡œ ëª¨ë¸ ì¬ìƒì„±
                    self.universal_model = self._build_model(
                        actual_input_dim, actual_output_dim
                    )

                    # ê°€ì¤‘ì¹˜ ë¡œë“œ
                    self.universal_model.load_state_dict(universal_state_dict)
                    self.universal_model.eval()
                    logger.info("í†µí•© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"í†µí•© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

            # ê°œë³„ ëª¨ë¸ë“¤ ë¡œë“œ
            for symbol in self.individual_scalers.keys():
                individual_path = f"{filepath}_pytorch_individual_{symbol}.pth"
                if os.path.exists(individual_path):
                    try:
                        # ì €ì¥ëœ ëª¨ë¸ì˜ ì‹¤ì œ ì°¨ì› í™•ì¸
                        individual_state_dict = torch.load(
                            individual_path, map_location=torch.device("cpu")
                        )
                        individual_input_dim = individual_state_dict[
                            "network.0.weight"
                        ].shape[1]
                        individual_output_dim = individual_state_dict[
                            "network.6.weight"
                        ].shape[0]

                        logger.info(
                            f"{symbol} ê°œë³„ ëª¨ë¸ ì°¨ì›: ì…ë ¥ {individual_input_dim}, ì¶œë ¥ {individual_output_dim}"
                        )

                        # ì €ì¥ëœ ì°¨ì›ìœ¼ë¡œ ëª¨ë¸ ì¬ìƒì„±
                        model = self._build_model(
                            individual_input_dim, individual_output_dim
                        )
                        model.load_state_dict(individual_state_dict)
                        model.eval()
                        self.individual_models[symbol] = model
                        logger.info(f"{symbol} ê°œë³„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"{symbol} ê°œë³„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

            # ê°€ì¤‘ì¹˜ í•™ìŠµê¸° ë¡œë“œ
            weight_learner_path = f"{filepath}_pytorch_weight_learner.pth"
            if os.path.exists(weight_learner_path):
                try:
                    # ê°€ì¤‘ì¹˜ í•™ìŠµê¸° êµ¬ì¡° ì¬ìƒì„± (ì…ë ¥ í¬ê¸°ëŠ” ì•™ìƒë¸” ì…ë ¥ í”¼ì²˜ì— ë”°ë¼ ê²°ì •)
                    input_size = (
                        20  # ê¸°ë³¸ê°’ (ì‹¤ì œë¡œëŠ” ì•™ìƒë¸” ì…ë ¥ í”¼ì²˜ í¬ê¸°ì— ë”°ë¼ ê²°ì •)
                    )
                    self.weight_learner = EnsembleWeightLearner(input_size)

                    # ê°€ì¤‘ì¹˜ ë¡œë“œ
                    self.weight_learner.load_state_dict(torch.load(weight_learner_path))
                    self.weight_learner.eval()
                    logger.info("ê°€ì¤‘ì¹˜ í•™ìŠµê¸° ë¡œë“œ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ê°€ì¤‘ì¹˜ í•™ìŠµê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")

            # í”¼ì²˜ ì •ë³´ ë¡œë“œ
            feature_info_path = f"{filepath}_feature_info.json"
            if os.path.exists(feature_info_path):
                try:
                    with open(feature_info_path, "r", encoding="utf-8") as f:
                        self.feature_info = json.load(f)
                    logger.info("í”¼ì²˜ ì •ë³´ ë¡œë“œ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"í”¼ì²˜ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")

            logger.info(f"ì‹ ê²½ë§ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
            return True

        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _get_default_signal(self, symbol: str) -> Dict:
        """ê¸°ë³¸ ì‹ í˜¸ (ì˜¤ë¥˜ì‹œ)"""
        return {
            "symbol": symbol,
            "timestamp": datetime.now().isoformat(),
            "action": "HOLD",
            "action_strength": 0.5,
            "score": 0.0,
            "confidence": 0.3,
            "position_size": 0.05,
            "execution_priority": 5,
            "timing": {"entry": {"type": "WAIT"}, "exit": {"stop_loss": -0.08}},
            "risk_management": {"risk_level": "MEDIUM"},
            "recommendations": {"primary_recommendation": "ë¶„ì„ ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì‹ í˜¸"},
        }

    def _split_train_test_data(self, training_data: Dict) -> Tuple[Dict, Dict]:
        """
        Train-test ë°ì´í„° ë¶„í• 

        Args:
            training_data: ì „ì²´ í•™ìŠµ ë°ì´í„°

        Returns:
            (train_data, test_data) íŠœí”Œ
        """
        try:
            train_data = {}
            test_data = {}

            for symbol, data in training_data.items():
                features = data["features"]
                target = data["target"]

                # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
                total_days = len(features)
                train_end_idx = int(total_days * self.train_ratio)

                # Train set
                train_features = features.iloc[:train_end_idx]
                train_target = (
                    target.iloc[:train_end_idx]
                    if hasattr(target, "iloc")
                    else target[:train_end_idx]
                )

                # Test set
                test_features = features.iloc[train_end_idx:]
                test_target = (
                    target.iloc[train_end_idx:]
                    if hasattr(target, "iloc")
                    else target[train_end_idx:]
                )

                train_data[symbol] = {
                    "features": train_features,
                    "target": train_target,
                }

                test_data[symbol] = {
                    "features": test_features, 
                    "target": test_target,
                    "full_features": features,  # ì—°ì† ì˜ˆì¸¡ìš© ì „ì²´ í”¼ì²˜
                    "train_end_idx": train_end_idx  # í•™ìŠµ ì¢…ë£Œ ì¸ë±ìŠ¤
                }

                logger.info(
                    f"ğŸ“Š {symbol} ë°ì´í„° ë¶„í• : Train {len(train_features)}ì¼, Test {len(test_features)}ì¼"
                )

            return train_data, test_data

        except Exception as e:
            logger.error(f"Train-test ë¶„í•  ì‹¤íŒ¨: {e}")
            return training_data, {}

    def _validate_22d_predictions(self, test_data: Dict) -> Dict:
        """
        22ì¼ ì˜ˆì¸¡ ê²€ì¦ ìˆ˜í–‰

        Args:
            test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°

        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            logger.info(f"ğŸ” 22ì¼ ì˜ˆì¸¡ ê²€ì¦ ì‹œì‘ - ì´ {len(test_data)}ê°œ ì¢…ëª©")
            validation_results = {}

            for symbol, data in test_data.items():
                features = data["features"]
                target = data["target"]

                logger.info(f"ğŸ” {symbol} 22ì¼ ì˜ˆì¸¡ ê²€ì¦ ì‹œì‘...")
                logger.info(f"   ğŸ“Š features í¬ê¸°: {features.shape}")
                logger.info(
                    f"   ğŸ“ˆ target í¬ê¸°: {target.shape if hasattr(target, 'shape') else len(target)}"
                )

                predictions = []
                actual_values = []
                dates = []

                # 22ì¼ ì „ë¶€í„° ì˜ˆì¸¡ ì‹œì‘ (22ì¼ í›„ ì˜ˆì¸¡ì„ ìœ„í•´)
                for i in range(22, len(features)):
                    # í˜„ì¬ ì‹œì ì˜ í”¼ì²˜ë¡œ 22ì¼ í›„ ì˜ˆì¸¡
                    current_features = features.iloc[: i + 1]  # í˜„ì¬ê¹Œì§€ì˜ ë°ì´í„°

                    try:
                        # 22ì¼ í›„ ì˜ˆì¸¡ ìˆ˜í–‰
                        prediction = self.predict(current_features, symbol)

                        # ì˜ˆì¸¡ê°’ ì¶”ì¶œ (22ì¼ ìˆ˜ìµë¥ )
                        if isinstance(prediction, dict):
                            pred_22d = prediction.get("target_22d", 0.0)
                        elif isinstance(prediction, (int, float)):
                            pred_22d = float(prediction)
                        else:
                            logger.warning(
                                f"âŒ {symbol} {i}ì¼ì°¨ ì˜ˆì¸¡ê°’ íƒ€ì… ì˜¤ë¥˜: {type(prediction)}"
                            )
                            continue

                        # ì‹¤ì œ 22ì¼ í›„ ê°’
                        if i + 22 < len(target):
                            # targetì´ DataFrameì¸ ê²½ìš° target_22d ì»¬ëŸ¼ ì„ íƒ
                            if isinstance(target, pd.DataFrame):
                                if "target_22d" in target.columns:
                                    actual_22d = target["target_22d"].iloc[i + 22]
                                else:
                                    # target_22d ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
                                    actual_22d = target.iloc[i + 22, 0]
                            elif hasattr(target, "iloc"):
                                actual_22d = target.iloc[i + 22]
                            else:
                                actual_22d = target[i + 22]

                            # ì‹¤ì œê°’ì„ ìˆ«ìë¡œ ë³€í™˜
                            if isinstance(actual_22d, (pd.Series, pd.DataFrame)):
                                actual_22d = (
                                    actual_22d.iloc[0] if len(actual_22d) > 0 else 0.0
                                )
                            actual_22d = float(actual_22d)

                            predictions.append(pred_22d)
                            actual_values.append(actual_22d)

                            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬
                            if hasattr(features, "index"):
                                current_date = features.index[i]
                                if hasattr(current_date, "strftime"):
                                    date_str = current_date.strftime("%Y-%m-%d")
                                else:
                                    date_str = str(current_date)
                            else:
                                date_str = f"Day_{i}"

                            dates.append(date_str)

                            # Line-by-line ë¡œê·¸ ì¶œë ¥
                            logger.info(
                                f"ğŸ“… {symbol} {date_str}: ì˜ˆì¸¡ {pred_22d:.4f} vs ì‹¤ì œ {actual_22d:.4f} (ì°¨ì´: {pred_22d - actual_22d:.4f})"
                            )

                    except Exception as e:
                        logger.warning(f"âŒ {symbol} {i}ì¼ì°¨ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                        continue

                if predictions and actual_values:
                    # RMSE ê³„ì‚°
                    rmse = self._calculate_test_rmse(predictions, actual_values)

                    validation_results[symbol] = {
                        "rmse": rmse,
                        "predictions": predictions,
                        "actual_values": actual_values,
                        "dates": dates,
                        "num_predictions": len(predictions),
                    }

                    logger.info(
                        f"âœ… {symbol} ê²€ì¦ ì™„ë£Œ: RMSE = {rmse:.4f} ({len(predictions)}ê°œ ì˜ˆì¸¡)"
                    )
                else:
                    logger.warning(f"âš ï¸ {symbol} ê²€ì¦ ë°ì´í„° ë¶€ì¡±")
                    validation_results[symbol] = {
                        "rmse": float("inf"),
                        "predictions": [],
                        "actual_values": [],
                        "dates": [],
                        "num_predictions": 0,
                    }

            return validation_results

        except Exception as e:
            logger.error(f"22ì¼ ì˜ˆì¸¡ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_test_rmse(
        self, predictions: List[float], actuals: List[float]
    ) -> float:
        """
        RMSE ê³„ì‚°

        Args:
            predictions: ì˜ˆì¸¡ê°’ ë¦¬ìŠ¤íŠ¸
            actuals: ì‹¤ì œê°’ ë¦¬ìŠ¤íŠ¸

        Returns:
            RMSE ê°’
        """
        try:
            if len(predictions) != len(actuals) or len(predictions) == 0:
                return float("inf")

            # RMSE ê³„ì‚°
            squared_errors = [(p - a) ** 2 for p, a in zip(predictions, actuals)]
            mse = sum(squared_errors) / len(squared_errors)
            rmse = mse**0.5

            return rmse

        except Exception as e:
            logger.error(f"RMSE ê³„ì‚° ì‹¤íŒ¨: {e}")
            return float("inf")

    def _train_ensemble_weight_learner(self, training_data: Dict) -> bool:
        """
        ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨

        Args:
            training_data: í›ˆë ¨ ë°ì´í„°

        Returns:
            í•™ìŠµ ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not self.enable_weight_learning:
                logger.info("ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                return True

            logger.info("ğŸ¯ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì‹œì‘...")

            # ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ìƒì„±
            ensemble_inputs, optimal_weights = self._prepare_weight_learning_data(
                training_data
            )

            if len(ensemble_inputs) < self.weight_learning_config.get(
                "min_samples_for_weight_learning", 100
            ):
                logger.warning(
                    f"ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(ensemble_inputs)}ê°œ (ìµœì†Œ {self.weight_learning_config.get('min_samples_for_weight_learning', 100)}ê°œ í•„ìš”)"
                )
                return False

            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            ensemble_inputs_scaled = self.weight_learner_scaler.fit_transform(
                ensemble_inputs
            )

            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            X_train, X_val, y_train, y_val = train_test_split(
                ensemble_inputs_scaled,
                optimal_weights,
                test_size=self.weight_learning_config.get("validation_split", 0.2),
                random_state=42,
            )

            # ë°ì´í„°ì…‹ ìƒì„±
            train_dataset = EnsembleWeightDataset(X_train, y_train)
            val_dataset = EnsembleWeightDataset(X_val, y_val)

            # ë°ì´í„°ë¡œë” ìƒì„±
            batch_size = self.weight_learning_config.get("batch_size", 32)
            train_loader = DataLoader(
                train_dataset, batch_size=batch_size, shuffle=True
            )
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

            # ê°€ì¤‘ì¹˜ í•™ìŠµê¸° ëª¨ë¸ ìƒì„±
            input_size = ensemble_inputs.shape[1]
            self.weight_learner = EnsembleWeightLearner(input_size)

            # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
            criterion = nn.MSELoss()
            learning_rate = self.weight_learning_config.get("learning_rate", 0.001)
            optimizer = optim.Adam(self.weight_learner.parameters(), lr=learning_rate)

            # í•™ìŠµ íŒŒë¼ë¯¸í„°
            epochs = self.weight_learning_config.get("epochs", 100)
            best_val_loss = float("inf")
            patience = 15
            patience_counter = 0

            logger.info(
                f"ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì‹œì‘ - Epochs: {epochs}, Input Size: {input_size}"
            )

            # í›ˆë ¨ ë£¨í”„
            for epoch in range(epochs):
                # í›ˆë ¨
                self.weight_learner.train()
                train_losses = []
                for X_batch, y_batch in train_loader:
                    optimizer.zero_grad()
                    outputs = self.weight_learner(X_batch.float())
                    loss = criterion(outputs, y_batch.float())
                    loss.backward()
                    optimizer.step()
                    train_losses.append(loss.item())

                train_loss = np.mean(train_losses)

                # ê²€ì¦
                self.weight_learner.eval()
                val_losses = []
                with torch.no_grad():
                    for X_batch, y_batch in val_loader:
                        outputs = self.weight_learner(X_batch.float())
                        loss = criterion(outputs, y_batch.float())
                        val_losses.append(loss.item())
                val_loss = np.mean(val_losses)

                # ë¡œê¹…
                if epoch % 10 == 0:
                    logger.info(
                        f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}"
                    )

                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1

                if patience_counter >= patience:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break

            print(f"\nâœ… ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì™„ë£Œ!")
            print(f"   â€¢ ìµœì¢… ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")
            print(f"   â€¢ í›ˆë ¨ ì—í¬í¬: {epochs}íšŒ")
            print(f"   â€¢ í•™ìŠµë¥ : {learning_rate}")
            print(f"   â€¢ ë°°ì¹˜ í¬ê¸°: {batch_size}")
            logger.info(
                f"âœ… ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì™„ë£Œ - Best Val Loss: {best_val_loss:.6f}"
            )
            return True

        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµê¸° í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return False

    def _prepare_weight_learning_data(
        self, training_data: Dict
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ê°€ì¤‘ì¹˜ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„

        Args:
            training_data: í›ˆë ¨ ë°ì´í„°

        Returns:
            (ì•™ìƒë¸” ì…ë ¥ í”¼ì²˜, ìµœì  ê°€ì¤‘ì¹˜) íŠœí”Œ
        """
        try:
            ensemble_inputs = []
            optimal_weights = []

            for symbol, data in training_data.items():
                logger.info(f"ğŸ” {symbol} ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

                try:
                    features = data["features"]
                    target = data["target"]

                    # ì‹¤ì œ ìˆ˜ìµë¥  ê³„ì‚° (íƒ€ê²Ÿì´ ìˆ˜ìµë¥ ì¸ ê²½ìš°)
                    if isinstance(target, pd.DataFrame):
                        actual_returns = target.values
                    elif isinstance(target, pd.Series):
                        actual_returns = target.values
                    else:
                        actual_returns = target

                    # ì‹œê³„ì—´ ë°ì´í„°ì˜ ì—¬ëŸ¬ ì‹œì ì—ì„œ ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ìƒì„±
                    # 22ì¼ í›„ ì˜ˆì¸¡ì„ ìœ„í•´ 22ì¼ ì „ë¶€í„° ì‹œì‘
                    start_idx = 22
                    end_idx = len(features) - 22  # 22ì¼ í›„ ë°ì´í„°ê°€ ìˆëŠ” ì§€ì ê¹Œì§€ë§Œ

                    logger.info(
                        f"ğŸ“Š {symbol} ì‹œê³„ì—´ ë°ì´í„°: {len(features)}ì¼, ê°€ì¤‘ì¹˜ í•™ìŠµ êµ¬ê°„: {start_idx}~{end_idx}"
                    )

                    for i in range(start_idx, end_idx, 5):  # 5ì¼ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                        try:
                            # í˜„ì¬ ì‹œì ê¹Œì§€ì˜ ë°ì´í„°ë¡œ ì˜ˆì¸¡
                            current_features = features.iloc[: i + 1]
                            current_returns = actual_returns[: i + 1]

                            # 22ì¼ í›„ ì‹¤ì œ ìˆ˜ìµë¥  (ëª©í‘œê°’)
                            if i + 22 < len(actual_returns):
                                future_returns = actual_returns[
                                    i : i + 22
                                ]  # 22ì¼ê°„ì˜ ìˆ˜ìµë¥ 
                            else:
                                continue

                            # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì‹œë„
                            individual_pred = None
                            if symbol in self.individual_models:
                                try:
                                    # ê°œë³„ ëª¨ë¸ìš© í”¼ì²˜ ì¤€ë¹„
                                    individual_features = current_features.copy()
                                    if symbol in self.individual_scalers:
                                        X_individual = self.individual_scalers[
                                            symbol
                                        ].transform(
                                            self._prepare_prediction_data(
                                                individual_features, 20
                                            )
                                        )
                                        individual_pred = self._predict_with_model(
                                            self.individual_models[symbol], X_individual
                                        )
                                except Exception as e:
                                    logger.debug(
                                        f"{symbol} {i}ì¼ì°¨ ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}"
                                    )

                            # í†µí•© ëª¨ë¸ ì˜ˆì¸¡ ì‹œë„
                            universal_pred = None
                            if self.universal_model is not None:
                                try:
                                    # í†µí•© ëª¨ë¸ìš© í”¼ì²˜ ì¤€ë¹„
                                    if self.feature_names is not None:
                                        available_features = [
                                            col
                                            for col in self.feature_names
                                            if col in current_features.columns
                                        ]

                                        if len(available_features) > 0:
                                            features_for_universal = current_features[
                                                available_features
                                            ]

                                            # ë¶€ì¡±í•œ í”¼ì²˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€
                                            if len(available_features) < len(
                                                self.feature_names
                                            ):
                                                missing_features = [
                                                    col
                                                    for col in self.feature_names
                                                    if col not in available_features
                                                ]
                                                for col in missing_features:
                                                    features_for_universal[col] = 0.0

                                            # í”¼ì²˜ ìˆœì„œ ë§ì¶”ê¸°
                                            features_for_universal = (
                                                features_for_universal[
                                                    self.feature_names
                                                ]
                                            )

                                            # ìœˆë„ìš° ë°ì´í„° ìƒì„±
                                            X_universal = self._prepare_prediction_data(
                                                features_for_universal, 20
                                            )

                                            # ì°¨ì› ê²€ì¦ ë° ì¡°ì •
                                            expected_dim = len(self.feature_names) * 20
                                            actual_dim = (
                                                X_universal.shape[1]
                                                if len(X_universal.shape) > 1
                                                else X_universal.shape[0]
                                            )

                                            if actual_dim != expected_dim:
                                                if actual_dim < expected_dim:
                                                    padding = np.zeros(
                                                        (
                                                            X_universal.shape[0],
                                                            expected_dim - actual_dim,
                                                        )
                                                    )
                                                    X_universal = np.hstack(
                                                        [X_universal, padding]
                                                    )
                                                else:
                                                    X_universal = X_universal[
                                                        :, :expected_dim
                                                    ]

                                            X_universal_scaled = (
                                                self.universal_scaler.transform(
                                                    X_universal
                                                )
                                            )
                                            universal_pred = self._predict_with_model(
                                                self.universal_model, X_universal_scaled
                                            )
                                except Exception as e:
                                    logger.debug(
                                        f"{symbol} {i}ì¼ì°¨ í†µí•© ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}"
                                    )

                            # ë©”íƒ€ í”¼ì²˜ ìƒì„± (ìˆœí™˜ ì˜ì¡´ì„± ì œê±°)
                            meta_features = self._create_meta_features_for_weight_learning(
                                current_features, symbol
                            )

                            if meta_features is not None and len(meta_features) == 20:
                                # ìµœì  ê°€ì¤‘ì¹˜ ê³„ì‚° (ì˜ˆì¸¡ê°’ ê¸°ë°˜)
                                optimal_weight = self._calculate_optimal_weights(
                                    individual_pred, universal_pred, future_returns
                                )

                                if optimal_weight is not None and len(optimal_weight) == 2:
                                    ensemble_inputs.append(meta_features)
                                    optimal_weights.append(optimal_weight)

                        except Exception as e:
                            logger.debug(
                                f"{symbol} {i}ì¼ì°¨ ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}"
                            )
                            continue

                except Exception as e:
                    logger.error(f"{symbol} ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
                    continue

            logger.info(f"ğŸ“Š ìƒì„±ëœ ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„°: {len(ensemble_inputs)}ê°œ")

            if not ensemble_inputs:
                logger.error("ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return np.array([]), np.array([])

            return np.array(ensemble_inputs), np.array(optimal_weights)

        except Exception as e:
            logger.error(f"ê°€ì¤‘ì¹˜ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return np.array([]), np.array([])

    def _create_ensemble_input_features(
        self,
        features: pd.DataFrame,
        symbol: str,
        individual_pred: Optional[float],
        universal_pred: Optional[float],
        actual_returns: np.ndarray,
    ) -> Optional[np.ndarray]:
        """
        ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµì„ ìœ„í•œ ì…ë ¥ í”¼ì²˜ ìƒì„±

        Args:
            features: í”¼ì²˜ ë°ì´í„°
            symbol: ì¢…ëª© ì‹¬ë³¼
            individual_pred: ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’
            universal_pred: í†µí•© ëª¨ë¸ ì˜ˆì¸¡ê°’
            actual_returns: ì‹¤ì œ ìˆ˜ìµë¥ 

        Returns:
            ì•™ìƒë¸” ì…ë ¥ í”¼ì²˜ (20ì°¨ì›)
        """
        try:
            # ì˜ˆì¸¡ê°’ì„ ìˆ«ìë¡œ ë³€í™˜
            if isinstance(individual_pred, dict):
                individual_pred = individual_pred.get("target_22d", 0.0)
            elif individual_pred is None:
                individual_pred = 0.0

            if isinstance(universal_pred, dict):
                universal_pred = universal_pred.get("target_22d", 0.0)
            elif universal_pred is None:
                universal_pred = 0.0

            # ì˜ˆì¸¡ê°’ ì°¨ì´
            pred_diff = abs(individual_pred - universal_pred)

            # ì˜ˆì¸¡ê°’ í¬ê¸°
            individual_magnitude = abs(individual_pred)
            universal_magnitude = abs(universal_pred)

            # ì˜ˆì¸¡ê°’ ë¹„ìœ¨
            if universal_magnitude > 0:
                pred_ratio = individual_magnitude / universal_magnitude
            else:
                pred_ratio = 1.0

            # ì‹¤ì œ ìˆ˜ìµë¥  í†µê³„
            if len(actual_returns) > 0:
                actual_mean = np.mean(actual_returns)
                actual_std = np.std(actual_returns)
                actual_min = np.min(actual_returns)
                actual_max = np.max(actual_returns)
            else:
                actual_mean = actual_std = actual_min = actual_max = 0.0

            # í”¼ì²˜ í†µê³„ (ìµœê·¼ 20ì¼)
            recent_features = features.tail(20)
            if len(recent_features) > 0:
                feature_mean = recent_features.mean().mean()
                feature_std = recent_features.std().mean()
                feature_min = recent_features.min().min()
                feature_max = recent_features.max().max()
            else:
                feature_mean = feature_std = feature_min = feature_max = 0.0

            # ì‹œì¥ ìƒí™© í”¼ì²˜ (ê¸°ë³¸ê°’)
            market_volatility = 0.02  # ê¸°ë³¸ ë³€ë™ì„±
            market_trend = 0.0  # ê¸°ë³¸ íŠ¸ë Œë“œ
            regime_stability = 0.5  # ê¸°ë³¸ ì•ˆì •ì„±

            # ì¢…ëª© í•´ì‹œ ìƒì„±
            symbol_hash = hash(symbol) % 1000 / 1000.0  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

            # 20ì°¨ì› ì•™ìƒë¸” ì…ë ¥ í”¼ì²˜ ìƒì„±
            ensemble_features = np.array(
                [
                    individual_pred,  # 1. ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’
                    universal_pred,  # 2. í†µí•© ëª¨ë¸ ì˜ˆì¸¡ê°’
                    pred_diff,  # 3. ì˜ˆì¸¡ê°’ ì°¨ì´
                    individual_magnitude,  # 4. ê°œë³„ ì˜ˆì¸¡ í¬ê¸°
                    universal_magnitude,  # 5. í†µí•© ì˜ˆì¸¡ í¬ê¸°
                    pred_ratio,  # 6. ì˜ˆì¸¡ê°’ ë¹„ìœ¨
                    actual_mean,  # 7. ì‹¤ì œ ìˆ˜ìµë¥  í‰ê· 
                    actual_std,  # 8. ì‹¤ì œ ìˆ˜ìµë¥  í‘œì¤€í¸ì°¨
                    actual_min,  # 9. ì‹¤ì œ ìˆ˜ìµë¥  ìµœì†Œê°’
                    actual_max,  # 10. ì‹¤ì œ ìˆ˜ìµë¥  ìµœëŒ€ê°’
                    feature_mean,  # 11. í”¼ì²˜ í‰ê· 
                    feature_std,  # 12. í”¼ì²˜ í‘œì¤€í¸ì°¨
                    feature_min,  # 13. í”¼ì²˜ ìµœì†Œê°’
                    feature_max,  # 14. í”¼ì²˜ ìµœëŒ€ê°’
                    market_volatility,  # 15. ì‹œì¥ ë³€ë™ì„±
                    market_trend,  # 16. ì‹œì¥ íŠ¸ë Œë“œ
                    regime_stability,  # 17. ì²´ì œ ì•ˆì •ì„±
                    len(features),  # 18. ë°ì´í„° ê¸¸ì´
                    symbol_hash,  # 19. ì¢…ëª© í•´ì‹œ (ê³ ì •ê°’)
                    1.0,  # 20. ë°”ì´ì–´ìŠ¤ í•­
                ]
            )

            return ensemble_features

        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ì…ë ¥ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _calculate_optimal_weights(
        self,
        individual_pred: Optional[float],
        universal_pred: Optional[float],
        actual_returns: np.ndarray,
    ) -> Optional[np.ndarray]:
        """
        ì‹¤ì œ ìˆ˜ìµë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°

        Args:
            individual_pred: ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’
            universal_pred: í†µí•© ëª¨ë¸ ì˜ˆì¸¡ê°’
            actual_returns: ì‹¤ì œ ìˆ˜ìµë¥ 

        Returns:
            ìµœì  ê°€ì¤‘ì¹˜ [universal_weight, individual_weight]
        """
        try:
            # ì˜ˆì¸¡ê°’ì„ ìˆ«ìë¡œ ë³€í™˜
            if isinstance(individual_pred, dict):
                individual_pred = individual_pred.get("target_22d", 0.0)
            elif individual_pred is None:
                individual_pred = 0.0

            if isinstance(universal_pred, dict):
                universal_pred = universal_pred.get("target_22d", 0.0)
            elif universal_pred is None:
                universal_pred = 0.0

            # ì‹¤ì œ ìˆ˜ìµë¥ ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë°˜í™˜
            if len(actual_returns) == 0:
                return np.array([0.7, 0.3])  # ê¸°ë³¸ ê°€ì¤‘ì¹˜

            # ì‹¤ì œ ìˆ˜ìµë¥  í‰ê· 
            actual_return = np.mean(actual_returns)

            # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ì˜¤ì°¨ ê³„ì‚°
            individual_error = abs(individual_pred - actual_return)
            universal_error = abs(universal_pred - actual_return)

            # ì˜¤ì°¨ì˜ ì—­ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš© (ì˜¤ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
            total_error = individual_error + universal_error
            if total_error > 0:
                individual_weight = universal_error / total_error
                universal_weight = individual_error / total_error
            else:
                # ì˜¤ì°¨ê°€ 0ì´ë©´ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©
                individual_weight = 0.3
                universal_weight = 0.7

            # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
            total_weight = individual_weight + universal_weight
            individual_weight /= total_weight
            universal_weight /= total_weight

            return np.array([universal_weight, individual_weight])

        except Exception as e:
            logger.error(f"ìµœì  ê°€ì¤‘ì¹˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.array([0.7, 0.3])  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë°˜í™˜

    def _update_ensemble_weights(
        self, symbol: str, features: pd.DataFrame
    ) -> Tuple[float, float]:
        """
        ë™ì ìœ¼ë¡œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ê°œì„ ëœ ë²„ì „)

        Args:
            symbol: ì¢…ëª© ì‹¬ë³¼
            features: ì¢…ëª© í”¼ì²˜

        Returns:
            (universal_weight, individual_weight) íŠœí”Œ
        """
        try:
            if self.weight_learner is None:
                logger.warning(f"{symbol} ê°€ì¤‘ì¹˜ í•™ìŠµê¸°ê°€ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©")
                return self.universal_weight, self.individual_weight

            # ë©”íƒ€ í”¼ì²˜ ìƒì„± (ì˜ˆì¸¡ê°’ ì œì™¸, ì‹œì¥ ìƒí™© ê¸°ë°˜)
            meta_features = self._create_meta_features_for_weight_learning(features, symbol)
            
            if meta_features is not None and len(meta_features) > 0:
                try:
                    # ê°€ì¤‘ì¹˜ í•™ìŠµê¸°ë¡œ ë™ì  ê°€ì¤‘ì¹˜ ì˜ˆì¸¡
                    self.weight_learner.eval()
                    with torch.no_grad():
                        # ì°¨ì› ë§ì¶”ê¸°
                        if len(meta_features.shape) == 1:
                            meta_features = meta_features.reshape(1, -1)
                        
                        # ìŠ¤ì¼€ì¼ë§
                        X_scaled = self.weight_learner_scaler.transform(meta_features)
                        X_tensor = torch.FloatTensor(X_scaled)
                        
                        # ê°€ì¤‘ì¹˜ ì˜ˆì¸¡
                        weights = self.weight_learner(X_tensor)
                        weights_np = weights.numpy()[0]

                        dynamic_universal_weight = float(weights_np[0])
                        dynamic_individual_weight = float(weights_np[1])

                        # ìœ íš¨ì„± ê²€ì¦
                        if dynamic_universal_weight < 0 or dynamic_individual_weight < 0:
                            logger.warning(f"{symbol} ìŒìˆ˜ ê°€ì¤‘ì¹˜ ê°ì§€, ê¸°ë³¸ê°’ ì‚¬ìš©")
                            return self.universal_weight, self.individual_weight

                        # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
                        total_weight = dynamic_universal_weight + dynamic_individual_weight
                        if total_weight > 0:
                            dynamic_universal_weight /= total_weight
                            dynamic_individual_weight /= total_weight
                        else:
                            return self.universal_weight, self.individual_weight

                        logger.info(
                            f"ğŸ¯ {symbol} ë™ì  ê°€ì¤‘ì¹˜: Universal={dynamic_universal_weight:.3f}, Individual={dynamic_individual_weight:.3f}"
                        )

                        return dynamic_universal_weight, dynamic_individual_weight

                except Exception as e:
                    logger.error(f"{symbol} ê°€ì¤‘ì¹˜ í•™ìŠµê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                    return self.universal_weight, self.individual_weight
            
            # ë©”íƒ€ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
            logger.warning(f"{symbol} ë©”íƒ€ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return self.universal_weight, self.individual_weight

        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return self.universal_weight, self.individual_weight

    def _create_meta_features_for_weight_learning(
        self, features: pd.DataFrame, symbol: str
    ) -> Optional[np.ndarray]:
        """
        ê°€ì¤‘ì¹˜ í•™ìŠµê¸°ë¥¼ ìœ„í•œ ë©”íƒ€ í”¼ì²˜ ìƒì„± (ìˆœí™˜ ì˜ì¡´ì„± ì œê±°)
        
        Args:
            features: ì¢…ëª© í”¼ì²˜ ë°ì´í„°
            symbol: ì¢…ëª© ì‹¬ë³¼
            
        Returns:
            ë©”íƒ€ í”¼ì²˜ ë°°ì—´ (ì˜ˆì¸¡ê°’ ì˜ì¡´ì„± ì—†ìŒ)
        """
        try:
            # ìµœê·¼ 20ì¼ ë°ì´í„°ë§Œ ì‚¬ìš©
            recent_features = features.tail(20)
            if len(recent_features) < 10:
                logger.warning(f"{symbol} ë©”íƒ€ í”¼ì²˜ ìƒì„±ìš© ë°ì´í„° ë¶€ì¡±")
                return None
            
            meta_features = []
            
            # 1. ì‹œì¥ ë³€ë™ì„± ì§€í‘œ
            if 'volatility_regime' in recent_features.columns:
                volatility_level = recent_features['volatility_regime'].mean()
                meta_features.append(volatility_level)
            else:
                meta_features.append(0.0)
            
            # 2. ì¶”ì„¸ ê°•ë„
            if 'trend_strength' in recent_features.columns:
                trend_strength = recent_features['trend_strength'].mean()
                meta_features.append(trend_strength)
            else:
                meta_features.append(0.0)
            
            # 3. ëª¨ë©˜í…€ ë‹¤ì´ë²„ì „ìŠ¤
            if 'momentum_divergence' in recent_features.columns:
                divergence_level = recent_features['momentum_divergence'].mean()
                meta_features.append(divergence_level)
            else:
                meta_features.append(0.0)
            
            # 4. ë³€ë™ì„± ìŠ¤í
            if 'volatility_skew' in recent_features.columns:
                skew_level = recent_features['volatility_skew'].mean()
                meta_features.append(skew_level)
            else:
                meta_features.append(0.0)
            
            # 5. ì§€ì§€/ì €í•­ ê°•ë„
            if 'support_resistance' in recent_features.columns:
                support_resistance = recent_features['support_resistance'].mean()
                meta_features.append(support_resistance)
            else:
                meta_features.append(0.0)
            
            # 6. ì‹œì¥ ë¯¸ì‹œêµ¬ì¡°
            if 'market_microstructure' in recent_features.columns:
                microstructure = recent_features['market_microstructure'].mean()
                meta_features.append(microstructure)
            else:
                meta_features.append(0.0)
            
            # 7. ë³¼ë¥¨-ê°€ê²© ì¶”ì„¸
            if 'volume_price_trend' in recent_features.columns:
                vpt = recent_features['volume_price_trend'].mean()
                meta_features.append(vpt)
            else:
                meta_features.append(0.0)
            
            # 8. ê°€ê²©-ë³¼ë¥¨ ì˜¤ì‹¤ë ˆì´í„°
            if 'price_volume_oscillator' in recent_features.columns:
                pvo = recent_features['price_volume_oscillator'].mean()
                meta_features.append(pvo)
            else:
                meta_features.append(0.0)
            
            # 9-12. ì‹œì¥ ì²´ì œ ì›í•« ì¸ì½”ë”©
            regime_features = []
            for regime in ['bullish', 'bearish', 'sideways', 'volatile']:
                regime_col = f'regime_{regime}'
                if regime_col in recent_features.columns:
                    regime_value = recent_features[regime_col].iloc[-1]  # ìµœì‹  ì²´ì œ
                    regime_features.append(regime_value)
                else:
                    regime_features.append(0.0)
            meta_features.extend(regime_features)
            
            # 13-20. ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ í†µê³„
            technical_indicators = [
                'dual_momentum', 'volatility_breakout', 'swing_ema', 'swing_rsi',
                'swing_donchian', 'stoch_donchian', 'whipsaw_prevention', 'swing_macd'
            ]
            
            for indicator in technical_indicators:
                if indicator in recent_features.columns:
                    # ìµœê·¼ ê°’ê³¼ ê³¼ê±° í‰ê· ì˜ ì°¨ì´
                    recent_mean = recent_features[indicator].tail(5).mean()
                    historical_mean = recent_features[indicator].mean()
                    diff = recent_mean - historical_mean
                    meta_features.append(diff)
                else:
                    meta_features.append(0.0)
            
            # ìµœì¢… 20ì°¨ì›ìœ¼ë¡œ ë§ì¶”ê¸°
            if len(meta_features) < 20:
                meta_features.extend([0.0] * (20 - len(meta_features)))
            elif len(meta_features) > 20:
                meta_features = meta_features[:20]
            
            return np.array(meta_features)
            
        except Exception as e:
            logger.error(f"{symbol} ë©”íƒ€ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return None


def main():
    """ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤"""
    import argparse
    import json
    import os
    import sys
    import glob
    from pathlib import Path

    parser = argparse.ArgumentParser(description="ì‹ ê²½ë§ ê°œë³„ ì¢…ëª© ì˜ˆì¸¡ê¸°")
    parser.add_argument("--train", action="store_true", help="ëª¨ë¸ í•™ìŠµ")
    parser.add_argument("--force", action="store_true", help="ê°•ì œ ì¬í•™ìŠµ")
    parser.add_argument(
        "--data-dir", type=str, default="data/trader", help="ì¢…ëª© ë°ì´í„° ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--config", type=str, default="config/config_trader.json", help="ì„¤ì • íŒŒì¼"
    )
    parser.add_argument(
        "--model-dir", type=str, default="models/trader", help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument("--predict", action="store_true", help="ì¢…ëª© ì˜ˆì¸¡")
    parser.add_argument("--symbol", type=str, help="ì˜ˆì¸¡í•  ì¢…ëª© ì½”ë“œ")
    parser.add_argument("--experiment", action="store_true", help="ë‹¤ì–‘í•œ ì‹ ê²½ë§ êµ¬ì¡° ì‹¤í—˜")
    parser.add_argument(
        "--experiment-config", 
        type=str, 
        default="config/neural_experiments.json", 
        help="ì‹¤í—˜ ì„¤ì • íŒŒì¼"
    )

    args = parser.parse_args()

    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    try:
        with open(args.config, "r", encoding="utf-8") as f:
            config = json.load(f)
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # ì‹ ê²½ë§ ëª¨ë¸ ì´ˆê¸°í™”
    neural_predictor = StockPredictionNetwork(config)

    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
    model_dir = Path(args.model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)
    model_path = model_dir / "neural_predictor"
    meta_path = str(model_path) + "_meta.pkl"

    if args.train:
        print("ğŸ§  ì‹ ê²½ë§ ê°œë³„ ì¢…ëª© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘")

        # --force ì˜µì…˜ì´ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì¬í•™ìŠµ
        if args.force:
            print("âš¡ ê°•ì œ ì¬í•™ìŠµ ëª¨ë“œ - ê¸°ì¡´ ëª¨ë¸ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ í•™ìŠµ")
        else:
            # ê¸°ì¡´ ëª¨ë¸ í™•ì¸
            if os.path.exists(meta_path):
                try:
                    if neural_predictor.load_model(str(model_path)):
                        print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                        print(
                            "ğŸ“Š ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš© - ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµ ë° 22ì¼ ê²€ì¦ì€ ê±´ë„ˆëœ€"
                        )
                        return
                except Exception as e:
                    print(f"âš ï¸  ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    print("ğŸ”„ ìƒˆë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤")

        # ì¢…ëª© ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“Š ì¢…ëª© ë°ì´í„° ë¡œë“œ: {args.data_dir}")
        trader_files = glob.glob(f"{args.data_dir}/*.csv")

        if not trader_files:
            print("âŒ ì¢…ëª© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"  í™•ì¸ ê²½ë¡œ: {args.data_dir}")
            sys.exit(1)

        # ëª¨ë“  ì¢…ëª© ë°ì´í„° ë¡œë“œ
        training_data = {}

        # ë§¤í¬ë¡œ ë°ì´í„° ë¡œë“œ
        print("ğŸ“Š ë§¤í¬ë¡œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        macro_data = None
        try:
            # data/macro ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë§¤í¬ë¡œ CSV íŒŒì¼ ë¡œë“œ
            macro_dir = Path("data/macro")
            if macro_dir.exists():
                macro_files = list(macro_dir.glob("*.csv"))
                if macro_files:
                    print(f"ğŸ“ˆ ë§¤í¬ë¡œ íŒŒì¼ {len(macro_files)}ê°œ ë°œê²¬")
                    macro_data_list = []

                    # ì¤‘ë³µ íŒŒì¼ ì²˜ë¦¬: _data.csv íŒŒì¼ ìš°ì„ , ê·¸ ë‹¤ìŒ ì¼ë°˜ .csv íŒŒì¼
                    processed_symbols = set()

                    # 1ë‹¨ê³„: _data.csv íŒŒì¼ë“¤ ë¨¼ì € ì²˜ë¦¬
                    for macro_file in macro_files:
                        if macro_file.name.endswith("_data.csv"):
                            try:
                                df = pd.read_csv(macro_file)
                                if not df.empty and "datetime" in df.columns:
                                    # íŒŒì¼ëª…ì—ì„œ ì‹¬ë³¼ ì¶”ì¶œ
                                    symbol = macro_file.stem.replace(
                                        "_data", ""
                                    ).replace("^", "")

                                    if symbol not in processed_symbols:
                                        df["macro_symbol"] = symbol
                                        df["datetime"] = pd.to_datetime(df["datetime"])
                                        df = df.fillna(method="ffill").fillna(
                                            method="bfill"
                                        )
                                        macro_data_list.append(df)
                                        processed_symbols.add(symbol)
                                        print(
                                            f"   âœ… {symbol}: {len(df)}í–‰ ë¡œë“œ ({macro_file.name})"
                                        )
                            except Exception as e:
                                print(f"   âš ï¸ {macro_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")

                    # 2ë‹¨ê³„: ì¼ë°˜ .csv íŒŒì¼ë“¤ ì²˜ë¦¬ (ì´ë¯¸ ì²˜ë¦¬ëœ ì‹¬ë³¼ ì œì™¸)
                    for macro_file in macro_files:
                        if not macro_file.name.endswith("_data.csv"):
                            try:
                                df = pd.read_csv(macro_file)
                                if not df.empty and "datetime" in df.columns:
                                    # íŒŒì¼ëª…ì—ì„œ ì‹¬ë³¼ ì¶”ì¶œ
                                    symbol = macro_file.stem.replace("^", "")

                                    if symbol not in processed_symbols:
                                        df["macro_symbol"] = symbol
                                        df["datetime"] = pd.to_datetime(df["datetime"])
                                        df = df.fillna(method="ffill").fillna(
                                            method="bfill"
                                        )
                                        macro_data_list.append(df)
                                        processed_symbols.add(symbol)
                                        print(
                                            f"   âœ… {symbol}: {len(df)}í–‰ ë¡œë“œ ({macro_file.name})"
                                        )
                            except Exception as e:
                                print(f"   âš ï¸ {macro_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")

                    if macro_data_list:
                        # ëª¨ë“  ë§¤í¬ë¡œ ë°ì´í„° ë³‘í•©
                        macro_data = pd.concat(macro_data_list, ignore_index=True)
                        print(f"ğŸ“Š ë§¤í¬ë¡œ ë°ì´í„° ë³‘í•© ì™„ë£Œ: {macro_data.shape}")

                        # ê³µí†µ datetime ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                        macro_data = macro_data.sort_values("datetime")
                        print(
                            f"ğŸ“… ë§¤í¬ë¡œ ë°ì´í„° datetime ë²”ìœ„: {macro_data['datetime'].min()} ~ {macro_data['datetime'].max()}"
                        )
                    else:
                        print("âš ï¸ ìœ íš¨í•œ ë§¤í¬ë¡œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                else:
                    print("âš ï¸ ë§¤í¬ë¡œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            else:
                print("âš ï¸ data/macro ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"âŒ ë§¤í¬ë¡œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        for file_path in trader_files:
            symbol = os.path.basename(file_path).split("_")[0]  # íŒŒì¼ëª…ì—ì„œ ì¢…ëª©ëª… ì¶”ì¶œ
            print(f"ğŸ“ˆ {symbol} ë°ì´í„° ë¡œë“œ ì¤‘...")

            try:
                stock_data = pd.read_csv(file_path)
                if len(stock_data) < 50:
                    print(f"âš ï¸  {symbol}: ë°ì´í„° ë¶€ì¡± ({len(stock_data)}í–‰), ê±´ë„ˆëœ€")
                    continue

                # í”¼ì²˜ ìƒì„± (ë§¤í¬ë¡œ ë°ì´í„° í¬í•¨)
                features = neural_predictor.create_features(
                    stock_data,
                    symbol,
                    {"regime": "NEUTRAL", "confidence": 0.5},
                    macro_data,
                )

                # í”¼ì²˜ ì°¨ì› ë¡œê¹… (ê°„ì†Œí™”)
                if features is not None:
                    lookback_days = (
                        config.get("neural_network", {})
                        .get("features", {})
                        .get("lookback_days", 20)
                    )
                    input_dim = len(features.columns) * lookback_days
                    print(
                        f"   ğŸ“Š {symbol}: {len(features.columns)}ì°¨ì› â†’ {input_dim}ì°¨ì› ì…ë ¥ (lookback: {lookback_days}ì¼)"
                    )
                else:
                    print(f"   âŒ {symbol}: í”¼ì²˜ ìƒì„± ì‹¤íŒ¨")

                # íƒ€ê²Ÿ ìƒì„± (22ì¼, 66ì¼ í›„ ìˆ˜ìµë¥  + ê°ê°ì˜ í‘œì¤€í¸ì°¨)
                target_forward_days = config.get("neural_network", {}).get(
                    "target_forward_days", [22, 66]
                )
                if isinstance(target_forward_days, list):
                    targets = {}
                    for days in target_forward_days:
                        # ë¯¸ë˜ ìˆ˜ìµë¥  ê³„ì‚°
                        future_returns = (
                            stock_data["close"].pct_change(days).shift(-days)
                        )
                        # NaN ì²˜ë¦¬: ffillê³¼ bfill ì‚¬ìš©
                        future_returns = future_returns.fillna(method="ffill").fillna(
                            method="bfill"
                        )
                        targets[f"target_{days}d"] = future_returns

                        # í•´ë‹¹ ê¸°ê°„ì˜ ìˆ˜ìµë¥  í‘œì¤€í¸ì°¨ ê³„ì‚°
                        rolling_returns = stock_data["close"].pct_change()
                        rolling_std = rolling_returns.rolling(
                            window=days, min_periods=1
                        ).std()
                        # NaN ì²˜ë¦¬: ffillê³¼ bfill ì‚¬ìš©
                        rolling_std = rolling_std.fillna(method="ffill").fillna(
                            method="bfill"
                        )
                        targets[f"sigma_{days}d"] = rolling_std

                    target = pd.DataFrame(targets)
                else:
                    # ë‹¨ì¼ íƒ€ê²Ÿ (22ì¼ ìˆ˜ìµë¥  + í‘œì¤€í¸ì°¨)
                    future_returns = stock_data["close"].pct_change(22).shift(-22)
                    future_returns = future_returns.fillna(method="ffill").fillna(
                        method="bfill"
                    )

                    rolling_returns = stock_data["close"].pct_change()
                    rolling_std = rolling_returns.rolling(
                        window=22, min_periods=1
                    ).std()
                    rolling_std = rolling_std.fillna(method="ffill").fillna(
                        method="bfill"
                    )

                    target = pd.DataFrame(
                        {"target_22d": future_returns, "sigma_22d": rolling_std}
                    )

                training_data[symbol] = {
                    "features": features,
                    "target": target,
                    "data": stock_data,
                }

                print(f"âœ… {symbol} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(stock_data)}í–‰")

            except Exception as e:
                print(f"âŒ {symbol} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue

        if not training_data:
            print("âŒ í•™ìŠµ ê°€ëŠ¥í•œ ì¢…ëª© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        print(f"ğŸ“Š ì´ {len(training_data)}ê°œ ì¢…ëª© ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"ğŸ“ˆ ì¢…ëª©ë“¤: {list(training_data.keys())}")

        # íƒ€ê²Ÿ ì •ë³´ í•œ ë²ˆë§Œ í‘œì‹œ (ì²« ë²ˆì§¸ ì¢…ëª© ê¸°ì¤€)
        if training_data:
            first_symbol = list(training_data.keys())[0]
            first_target = training_data[first_symbol]["target"]
            print(
                f"ğŸ“Š íƒ€ê²Ÿ êµ¬ì¡°: {list(first_target.columns)} (í¬ê¸°: {first_target.shape})"
            )

            # í”¼ì²˜ ì°¨ì› ìš”ì•½ (ê°„ì†Œí™”)
            lookback_days = (
                config.get("neural_network", {})
                .get("features", {})
                .get("lookback_days", 20)
            )
            feature_summary = []
            for symbol, data in training_data.items():
                features = data["features"]
                if features is not None:
                    feature_summary.append(f"{symbol}:{len(features.columns)}")

            print(
                f"\nğŸ” í”¼ì²˜ ìš”ì•½: {' | '.join(feature_summary)} (lookback: {lookback_days}ì¼)"
            )

        # ì‹ ê²½ë§ í•™ìŠµ
        print("ğŸ§  ì‹ ê²½ë§ í•™ìŠµ ì‹œì‘...")
        print(
            f"ğŸ“Š enable_individual_models: {neural_predictor.enable_individual_models}"
        )
        print(f"ğŸ“ˆ í•™ìŠµí•  ì¢…ëª© ìˆ˜: {len(training_data)}")

        if neural_predictor.fit(training_data):
            print("âœ… ì‹ ê²½ë§ í•™ìŠµ ì„±ê³µ")
            neural_predictor.save_model(str(model_path))
            print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        else:
            print("âŒ ì‹ ê²½ë§ í•™ìŠµ ì‹¤íŒ¨")
            sys.exit(1)

    elif args.predict:
        print("ğŸ”® ì¢…ëª© ì˜ˆì¸¡")

        if not args.symbol:
            print("âŒ --symbol ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)

        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(meta_path):
            print("âŒ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € --train ì˜µì…˜ìœ¼ë¡œ í•™ìŠµí•˜ì„¸ìš”.")
            sys.exit(1)

        if not neural_predictor.load_model(str(model_path)):
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            sys.exit(1)

        # ì¢…ëª© ë°ì´í„° ë¡œë“œ
        symbol_file = f"{args.data_dir}/{args.symbol}_*.csv"
        symbol_files = glob.glob(symbol_file)

        if not symbol_files:
            print(f"âŒ {args.symbol} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        stock_data = pd.read_csv(symbol_files[0])
        print(f"âœ… {args.symbol} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(stock_data)}í–‰")

        # ì˜ˆì¸¡ ì‹¤í–‰ (ê°„ë‹¨í•œ í”¼ì²˜ ìƒì„±)
        features = pd.DataFrame(
            {
                "close": stock_data["close"],
                "volume": stock_data["volume"],
                "high": stock_data["high"],
                "low": stock_data["low"],
            }
        )

        prediction = neural_predictor.predict(features, args.symbol)
        
        # ìµœì¢… ì‹œì  ê¸°ì¤€ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
        current_date = features.index[-1] if hasattr(features, 'index') else "í˜„ì¬"
        print("\n" + "="*60)
        print(f"ğŸ”® {args.symbol} ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ (ê¸°ì¤€ì¼: {current_date})")
        print("="*60)
        
        if isinstance(prediction, dict):
            for key, value in prediction.items():
                if 'target_22d' in key:
                    print(f"ğŸ¯ 22ì¼ í›„ ì˜ˆìƒ ìˆ˜ìµë¥ : {value:.4f} ({value*100:.2f}%)")
                elif 'target_66d' in key:
                    print(f"ğŸ¯ 66ì¼ í›„ ì˜ˆìƒ ìˆ˜ìµë¥ : {value:.4f} ({value*100:.2f}%)")
                else:
                    print(f"   {key}: {value:.4f}")
        else:
            print(f"ğŸ¯ 22ì¼ í›„ ì˜ˆìƒ ìˆ˜ìµë¥ : {prediction:.4f} ({prediction*100:.2f}%)")
        
        print("="*60 + "\n")

    elif args.experiment:
        print("\n" + "="*70)
        print("ğŸ§ª ì‹ ê²½ë§ êµ¬ì¡° ì‹¤í—˜ ëª¨ë“œ")
        print("="*70)
        
        # ì‹¤í—˜ì„ ìœ„í•œ í•¨ìˆ˜ ì„í¬íŠ¸
        from .neural_experiment import run_neural_experiments
        
        # ì‹¤í—˜ ì‹¤í–‰
        experiment_results = run_neural_experiments(
            config_path=args.config,
            experiment_config_path=args.experiment_config,
            data_dir=args.data_dir,
            model_dir=args.model_dir,
            force_retrain=args.force
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ† ì‹¤í—˜ ê²°ê³¼ ìš”ì•½:")
        print("="*70)
        for symbol, results in experiment_results.items():
            print(f"\nğŸ“Š {symbol}:")
            for model_name, performance in results.items():
                print(f"   - {model_name}: RMSE = {performance['rmse']:.4f}")
        
    else:
        print("ì‚¬ìš©ë²•:")
        print("  --train --data-dir data/trader     # ëª¨ë¸ í•™ìŠµ")
        print("  --predict --symbol AAPL            # ì¢…ëª© ì˜ˆì¸¡")
        print("  --experiment                       # ë‹¤ì–‘í•œ ì‹ ê²½ë§ êµ¬ì¡° ì‹¤í—˜")
        print("  --force                            # ê°•ì œ ì¬í•™ìŠµ")


if __name__ == "__main__":
    main()
