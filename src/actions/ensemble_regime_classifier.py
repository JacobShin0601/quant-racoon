"""
RF-XGBoost ì•™ìƒë¸” ë³´íŒ… ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°
Random Forestì™€ XGBoostë¥¼ ê²°í•©í•œ íˆ¬í‘œ ë°©ì‹ ì²´ì œ ë¶„ë¥˜

ì£¼ìš” íŠ¹ì§•:
- Soft Voting: ê° ë¶„ë¥˜ê¸°ì˜ í™•ë¥ ì  ì˜ˆì¸¡ì„ ê²°í•©
- Hard Voting: ê° ë¶„ë¥˜ê¸°ì˜ ìµœì¢… ì˜ˆì¸¡ì„ ë‹¤ìˆ˜ê²°ë¡œ ê²°ì •  
- Weighted Voting: ëª¨ë¸ ì„±ëŠ¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì ìš©
- ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •: ìµœê·¼ ì„±ëŠ¥ì— ë”°ë¥¸ ì‹¤ì‹œê°„ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
- Cross Validation ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€ ë° ê°€ì¤‘ì¹˜ ê²°ì •
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_validate
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union
import warnings
import pickle
import os

try:
    import xgboost as xgb
    HAS_XGBOOST = True
except ImportError:
    print("âš ï¸ XGBoost not found. Using LightGBM as fallback.")
    HAS_XGBOOST = False
    try:
        import lightgbm as lgb
        HAS_LIGHTGBM = True
    except ImportError:
        print("âš ï¸ LightGBM not found. Using only RandomForest.")
        HAS_LIGHTGBM = False

try:
    from .ml_regime_classifier import DynamicRegimeLabelGenerator
except ImportError:
    from ml_regime_classifier import DynamicRegimeLabelGenerator

warnings.filterwarnings("ignore")

logger = logging.getLogger(__name__)


class RFXGBEnsembleRegimeClassifier:
    """
    RF-XGBoost ì•™ìƒë¸” ë³´íŒ… ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°
    
    Random Forestì™€ XGBoost(ë˜ëŠ” LightGBM)ì„ ê²°í•©í•˜ì—¬ ë” ì•ˆì •ì ì´ê³  ì •í™•í•œ ì²´ì œ ë¶„ë¥˜ ì œê³µ
    
    ì£¼ìš” ì•™ìƒë¸” ì „ëµ:
    1. Soft Voting: í™•ë¥  ë¶„í¬ ê²°í•©
    2. Hard Voting: ë‹¤ìˆ˜ê²° íˆ¬í‘œ
    3. Weighted Voting: CV ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜
    4. Stacking: ë©”íƒ€ ëª¨ë¸ì„ í†µí•œ ì•™ìƒë¸”
    5. Dynamic Weighting: ì ì‘ì  ê°€ì¤‘ì¹˜ ì¡°ì •
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.ensemble_config = config.get("ensemble_regime", {})
        self.ml_config = config.get("ml_regime", {})
        
        # Random Forest ëª¨ë¸ ì´ˆê¸°í™”
        self.rf_model = RandomForestClassifier(
            n_estimators=self.ml_config.get("n_estimators", 100),
            max_depth=self.ml_config.get("max_depth", 10),
            min_samples_split=self.ml_config.get("min_samples_split", 20),
            min_samples_leaf=self.ml_config.get("min_samples_leaf", 10),
            random_state=self.ml_config.get("random_state", 42),
            class_weight='balanced',
            n_jobs=-1
        )
        
        # XGBoost ë˜ëŠ” LightGBM ëª¨ë¸ ì´ˆê¸°í™”
        if HAS_XGBOOST:
            self.gbm_model = xgb.XGBClassifier(
                n_estimators=self.ml_config.get("n_estimators", 100),
                max_depth=self.ml_config.get("max_depth", 6),
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.ml_config.get("random_state", 42),
                n_jobs=-1,
                eval_metric='logloss'
            )
            self.gbm_name = "XGBoost"
        elif HAS_LIGHTGBM:
            self.gbm_model = lgb.LGBMClassifier(
                n_estimators=self.ml_config.get("n_estimators", 100),
                max_depth=self.ml_config.get("max_depth", 6),
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.ml_config.get("random_state", 42),
                n_jobs=-1,
                verbosity=-1
            )
            self.gbm_name = "LightGBM"
        else:
            # GBM ì—†ì´ RFë§Œ ì‚¬ìš© (ë‘ ë²ˆì§¸ RFë¥¼ ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ)
            self.gbm_model = RandomForestClassifier(
                n_estimators=self.ml_config.get("n_estimators", 150),
                max_depth=self.ml_config.get("max_depth", 15),
                min_samples_split=self.ml_config.get("min_samples_split", 10),
                min_samples_leaf=self.ml_config.get("min_samples_leaf", 5),
                random_state=self.ml_config.get("random_state", 42) + 1,
                class_weight='balanced',
                n_jobs=-1
            )
            self.gbm_name = "RandomForest2"
        
        # í”¼ì²˜ ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler = StandardScaler()
        
        # ë¼ë²¨ ìƒì„±ê¸°
        self.label_generator = DynamicRegimeLabelGenerator(config)
        
        # ì•™ìƒë¸” ì„¤ì •
        self.voting_strategy = self.ensemble_config.get("voting_strategy", "weighted")
        self.enable_dynamic_weights = self.ensemble_config.get("enable_dynamic_weights", True)
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (RF, XGB/LightGBM)
        self.base_weights = {
            "rf": self.ensemble_config.get("rf_weight", 0.5),
            "gbm": self.ensemble_config.get("gbm_weight", 0.5)
        }
        
        # ë™ì  ê°€ì¤‘ì¹˜ (ì´ˆê¸°ê°’ì€ ê¸°ë³¸ ê°€ì¤‘ì¹˜)
        self.current_weights = self.base_weights.copy()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = {
            "rf": [],
            "gbm": [],
            "ensemble": []
        }
        
        # CV ì ìˆ˜ ì €ì¥
        self.cv_scores = {"rf": None, "gbm": None}
        
        # ì˜ˆì¸¡ ì´ë ¥ ì €ì¥
        self.prediction_history = []
        
        # í´ë˜ìŠ¤ ë§¤í•‘
        self.class_names = ["TRENDING_UP", "TRENDING_DOWN", "SIDEWAYS", "VOLATILE"]
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(self.class_names)
        
        # í”¼ì²˜ëª… ì €ì¥
        self.feature_names = []
        
        # í•™ìŠµ ìƒíƒœ
        self.is_fitted = False
        
        logger.info("RFXGBEnsembleRegimeClassifier ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ëª¨ë¸: RandomForest + {self.gbm_name}")
        logger.info(f"íˆ¬í‘œ ì „ëµ: {self.voting_strategy}")
        logger.info(f"ê¸°ë³¸ ê°€ì¤‘ì¹˜ - RF: {self.base_weights['rf']:.2f}, {self.gbm_name}: {self.base_weights['gbm']:.2f}")

    def fit(self, macro_data: pd.DataFrame, force_retrain: bool = False):
        """RF-XGBoost ì•™ìƒë¸” ë¶„ë¥˜ê¸° í•™ìŠµ"""
        logger.info("RF-XGBoost ì•™ìƒë¸” ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘")
        
        try:
            # 1. ë™ì  ë¼ë²¨ ìƒì„±
            logger.info("ë™ì  ì‹œì¥ ì²´ì œ ë¼ë²¨ ìƒì„±")
            labeled_data = self.label_generator.generate_dynamic_labels(macro_data)
            
            if len(labeled_data) < 100:
                logger.warning(f"í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(labeled_data)}ê°œ (ìµœì†Œ 100ê°œ ê¶Œì¥)")
            
            # 2. í¬ê´„ì  í”¼ì²˜ ì¶”ì¶œ
            logger.info("í¬ê´„ì  í”¼ì²˜ ì¶”ì¶œ")
            features = self._extract_comprehensive_features(macro_data)
            
            # 3. ë¼ë²¨ ì •ë ¬ (ì¸ë±ìŠ¤ ë§¤ì¹­)
            common_index = features.index.intersection(labeled_data.index)
            features_aligned = features.loc[common_index]
            labels_aligned = labeled_data.loc[common_index]['regime_label']
            
            if len(features_aligned) < 50:
                logger.error(f"ì •ë ¬ëœ í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(features_aligned)}ê°œ")
                return False
            
            # 4. ë°ì´í„° ì¤€ë¹„
            X = features_aligned.values
            y = self.label_encoder.transform(labels_aligned.values)
            
            # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
            X_scaled = self.scaler.fit_transform(X)
            self.feature_names = list(features_aligned.columns)
            
            logger.info(f"í•™ìŠµ ë°ì´í„°: {X.shape[0]}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ í”¼ì²˜")
            
            # 5. êµì°¨ ê²€ì¦ì„ í†µí•œ ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
            logger.info("êµì°¨ ê²€ì¦ì„ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
            
            rf_cv_scores = cross_val_score(self.rf_model, X_scaled, y, cv=5, scoring='accuracy', n_jobs=-1)
            gbm_cv_scores = cross_val_score(self.gbm_model, X_scaled, y, cv=5, scoring='accuracy', n_jobs=-1)
            
            self.cv_scores["rf"] = rf_cv_scores
            self.cv_scores["gbm"] = gbm_cv_scores
            
            logger.info(f"RF CV ì ìˆ˜: {rf_cv_scores.mean():.4f} Â± {rf_cv_scores.std():.4f}")
            logger.info(f"{self.gbm_name} CV ì ìˆ˜: {gbm_cv_scores.mean():.4f} Â± {gbm_cv_scores.std():.4f}")
            
            # 6. CV ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
            if self.voting_strategy == "weighted":
                self._update_weights_from_cv_scores()
            
            # 7. ê°œë³„ ëª¨ë¸ í•™ìŠµ
            logger.info("ê°œë³„ ëª¨ë¸ í•™ìŠµ")
            self.rf_model.fit(X_scaled, y)
            self.gbm_model.fit(X_scaled, y)
            
            # 8. ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ
            self.is_fitted = True
            logger.info("RF-XGBoost ì•™ìƒë¸” ë¶„ë¥˜ê¸° í•™ìŠµ ì™„ë£Œ")
            
            # 9. í”¼ì²˜ ì¤‘ìš”ë„ ë¡œê¹…
            self._log_feature_importance()
            
            return True
            
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ë¶„ë¥˜ê¸° í•™ìŠµ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _extract_comprehensive_features(self, macro_data: pd.DataFrame) -> pd.DataFrame:
        """í¬ê´„ì  í”¼ì²˜ ì¶”ì¶œ"""
        try:
            from ml_regime_classifier import DynamicMLRegimeClassifier
            ml_classifier = DynamicMLRegimeClassifier(self.config)
            return ml_classifier.extract_comprehensive_features_from_data(macro_data)
        except Exception as e:
            logger.error(f"í”¼ì²˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ í”¼ì²˜ ë°˜í™˜
            return pd.DataFrame({
                "vix_level": [20.0] * len(macro_data),
                "yield_spread": [1.5] * len(macro_data),
                "market_momentum": [0.0] * len(macro_data),
            }, index=macro_data.index)

    def _update_weights_from_cv_scores(self):
        """CV ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        rf_score = self.cv_scores["rf"].mean()
        gbm_score = self.cv_scores["gbm"].mean()
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        total_score = rf_score + gbm_score
        if total_score > 0:
            self.current_weights["rf"] = rf_score / total_score
            self.current_weights["gbm"] = gbm_score / total_score
            
            logger.info(f"CV ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ - RF: {self.current_weights['rf']:.3f}, "
                       f"{self.gbm_name}: {self.current_weights['gbm']:.3f}")

    def _log_feature_importance(self):
        """í”¼ì²˜ ì¤‘ìš”ë„ ë¡œê¹…"""
        if len(self.feature_names) == 0:
            return
            
        try:
            # RF í”¼ì²˜ ì¤‘ìš”ë„
            rf_importance = self.rf_model.feature_importances_
            
            # GBM í”¼ì²˜ ì¤‘ìš”ë„
            if hasattr(self.gbm_model, 'feature_importances_'):
                gbm_importance = self.gbm_model.feature_importances_
            else:
                gbm_importance = np.zeros_like(rf_importance)
            
            # ìƒìœ„ 10ê°œ í”¼ì²˜ ì¶œë ¥
            rf_top_features = sorted(zip(self.feature_names, rf_importance), 
                                   key=lambda x: x[1], reverse=True)[:10]
            
            logger.info("=== RF ìƒìœ„ í”¼ì²˜ ì¤‘ìš”ë„ ===")
            for name, importance in rf_top_features:
                logger.info(f"  {name}: {importance:.4f}")
                
        except Exception as e:
            logger.warning(f"í”¼ì²˜ ì¤‘ìš”ë„ ë¡œê¹… ì‹¤íŒ¨: {e}")

    def predict(self, macro_data: pd.DataFrame) -> Dict:
        """RF-XGBoost ì•™ìƒë¸” ì˜ˆì¸¡"""
        if not self.is_fitted:
            raise ValueError("ì•™ìƒë¸” ë¶„ë¥˜ê¸°ê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            logger.info("ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘")
            
            # 1. í”¼ì²˜ ì¶”ì¶œ
            features = self._extract_comprehensive_features(macro_data)
            if len(features) == 0:
                logger.error("ì˜ˆì¸¡ìš© í”¼ì²˜ ì¶”ì¶œ ì‹¤íŒ¨")
                return self._fallback_prediction()
            
            # 2. ìµœì‹  ë°ì´í„°ë¡œ ì˜ˆì¸¡
            latest_features = features.iloc[-1:].values
            X_scaled = self.scaler.transform(latest_features)
            
            # 3. ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡
            rf_pred = self.rf_model.predict(X_scaled)[0]
            rf_proba = self.rf_model.predict_proba(X_scaled)[0]
            rf_confidence = float(np.max(rf_proba))
            rf_pred_label = self.label_encoder.inverse_transform([rf_pred])[0]
            
            gbm_pred = self.gbm_model.predict(X_scaled)[0]
            gbm_proba = self.gbm_model.predict_proba(X_scaled)[0]
            gbm_confidence = float(np.max(gbm_proba))
            gbm_pred_label = self.label_encoder.inverse_transform([gbm_pred])[0]
            
            # 4. ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
            ensemble_result = self._ensemble_predict(
                rf_pred_label, rf_proba, rf_confidence,
                gbm_pred_label, gbm_proba, gbm_confidence
            )
            
            # 5. ì˜ˆì¸¡ ì´ë ¥ ì €ì¥
            prediction_record = {
                'timestamp': datetime.now(),
                'rf_prediction': rf_pred_label,
                'gbm_prediction': gbm_pred_label,
                'ensemble_prediction': ensemble_result['predicted_regime'],
                'rf_confidence': rf_confidence,
                'gbm_confidence': gbm_confidence,
                'ensemble_confidence': ensemble_result['confidence'],
                'weights_used': self.current_weights.copy()
            }
            self.prediction_history.append(prediction_record)
            
            # 6. ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ì˜µì…˜)
            if self.enable_dynamic_weights and len(self.prediction_history) > 10:
                self._update_dynamic_weights()
            
            logger.info(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì™„ë£Œ: {ensemble_result['predicted_regime']} "
                       f"(ì‹ ë¢°ë„: {ensemble_result['confidence']:.3f})")
            
            return ensemble_result
            
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return self._fallback_prediction()

    def _ensemble_predict(self, rf_pred: str, rf_proba: np.ndarray, rf_conf: float,
                         gbm_pred: str, gbm_proba: np.ndarray, gbm_conf: float) -> Dict:
        """ì•™ìƒë¸” íˆ¬í‘œ ìˆ˜í–‰"""
        
        if self.voting_strategy == "soft":
            # Soft Voting: í™•ë¥  ë¶„í¬ ê°€ì¤‘ ê²°í•©
            ensemble_proba = (
                self.current_weights['rf'] * rf_proba + 
                self.current_weights['gbm'] * gbm_proba
            )
            ensemble_idx = np.argmax(ensemble_proba)
            ensemble_pred = self.class_names[ensemble_idx]
            ensemble_conf = float(ensemble_proba[ensemble_idx])
            
        elif self.voting_strategy == "hard":
            # Hard Voting: ë‹¤ìˆ˜ê²°
            if rf_pred == gbm_pred:
                ensemble_pred = rf_pred
                ensemble_conf = (rf_conf + gbm_conf) / 2
            else:
                if rf_conf >= gbm_conf:
                    ensemble_pred, ensemble_conf = rf_pred, rf_conf
                else:
                    ensemble_pred, ensemble_conf = gbm_pred, gbm_conf
                    
        elif self.voting_strategy == "weighted":
            # Weighted Voting: ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            rf_score = self.current_weights['rf'] * rf_conf
            gbm_score = self.current_weights['gbm'] * gbm_conf
            
            if rf_pred == gbm_pred:
                ensemble_pred = rf_pred
                ensemble_conf = (rf_score + gbm_score)
            else:
                if rf_score >= gbm_score:
                    ensemble_pred, ensemble_conf = rf_pred, rf_score
                else:
                    ensemble_pred, ensemble_conf = gbm_pred, gbm_score
                    
        else:
            # ê¸°ë³¸ê°’: Weighted
            rf_score = self.current_weights['rf'] * rf_conf
            gbm_score = self.current_weights['gbm'] * gbm_conf
            
            if rf_score >= gbm_score:
                ensemble_pred, ensemble_conf = rf_pred, rf_score
            else:
                ensemble_pred, ensemble_conf = gbm_pred, gbm_score
        
        return {
            'predicted_regime': ensemble_pred,
            'confidence': ensemble_conf,
            'individual_predictions': {
                'rf': {'regime': rf_pred, 'confidence': rf_conf},
                'gbm': {'regime': gbm_pred, 'confidence': gbm_conf}
            },
            'voting_strategy': self.voting_strategy,
            'weights': self.current_weights.copy()
        }

    def _update_dynamic_weights(self):
        """ìµœê·¼ ì„±ëŠ¥ì— ë”°ë¥¸ ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        if len(self.prediction_history) < 20:
            return
        
        try:
            recent_predictions = self.prediction_history[-20:]
            
            # ê° ëª¨ë¸ì˜ ì¼ê´€ì„± ì ìˆ˜
            rf_consistency = self._calculate_consistency([p['rf_prediction'] for p in recent_predictions])
            gbm_consistency = self._calculate_consistency([p['gbm_prediction'] for p in recent_predictions])
            
            # ì‹ ë¢°ë„ í‰ê· 
            rf_avg_conf = np.mean([p['rf_confidence'] for p in recent_predictions])
            gbm_avg_conf = np.mean([p['gbm_confidence'] for p in recent_predictions])
            
            # ì¢…í•© ì ìˆ˜
            rf_score = (rf_consistency * 0.6 + rf_avg_conf * 0.4)
            gbm_score = (gbm_consistency * 0.6 + gbm_avg_conf * 0.4)
            
            # ì •ê·œí™”í•˜ì—¬ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            total_score = rf_score + gbm_score
            if total_score > 0:
                new_rf_weight = rf_score / total_score
                new_gbm_weight = gbm_score / total_score
                
                # ê´€ì„± ì ìš©
                inertia = 0.7
                self.current_weights['rf'] = (
                    inertia * self.current_weights['rf'] + (1 - inertia) * new_rf_weight
                )
                self.current_weights['gbm'] = (
                    inertia * self.current_weights['gbm'] + (1 - inertia) * new_gbm_weight
                )
                
                logger.info(f"ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ - RF: {self.current_weights['rf']:.3f}, "
                           f"{self.gbm_name}: {self.current_weights['gbm']:.3f}")
                
        except Exception as e:
            logger.warning(f"ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _calculate_consistency(self, predictions: List[str]) -> float:
        """ì˜ˆì¸¡ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        if len(predictions) <= 1:
            return 0.5
        
        consistent_count = 0
        for i in range(1, len(predictions)):
            if predictions[i] == predictions[i-1]:
                consistent_count += 1
        
        return consistent_count / (len(predictions) - 1)

    def _fallback_prediction(self) -> Dict:
        """ì˜¤ë¥˜ ì‹œ ëŒ€ì²´ ì˜ˆì¸¡"""
        logger.warning("ëŒ€ì²´ ì˜ˆì¸¡ ëª¨ë“œ í™œì„±í™”")
        
        return {
            'predicted_regime': 'SIDEWAYS',
            'confidence': 0.25,
            'individual_predictions': {
                'rf': {'regime': 'SIDEWAYS', 'confidence': 0.25},
                'gbm': {'regime': 'SIDEWAYS', 'confidence': 0.25}
            },
            'voting_strategy': 'fallback',
            'weights': {'rf': 0.5, 'gbm': 0.5}
        }

    def get_prediction_summary(self, n_recent: int = 20) -> Dict:
        """ìµœê·¼ ì˜ˆì¸¡ ìš”ì•½ í†µê³„"""
        
        if len(self.prediction_history) == 0:
            return {"message": "ì˜ˆì¸¡ ì´ë ¥ ì—†ìŒ"}
        
        recent = self.prediction_history[-n_recent:] if n_recent > 0 else self.prediction_history
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ ë¶„í¬
        rf_preds = [p['rf_prediction'] for p in recent]
        gbm_preds = [p['gbm_prediction'] for p in recent]  
        ensemble_preds = [p['ensemble_prediction'] for p in recent]
        
        # ì¼ì¹˜ìœ¨ ê³„ì‚°
        agreements = sum(1 for p in recent if p['rf_prediction'] == p['gbm_prediction'])
        agreement_rate = agreements / len(recent)
        
        # í‰ê·  ì‹ ë¢°ë„
        avg_rf_conf = np.mean([p['rf_confidence'] for p in recent])
        avg_gbm_conf = np.mean([p['gbm_confidence'] for p in recent])
        avg_ensemble_conf = np.mean([p['ensemble_confidence'] for p in recent])
        
        return {
            'total_predictions': len(recent),
            'agreement_rate': agreement_rate,
            'average_confidence': {
                'rf': avg_rf_conf,
                'gbm': avg_gbm_conf,
                'ensemble': avg_ensemble_conf
            },
            'prediction_distribution': {
                'rf': {regime: rf_preds.count(regime) for regime in self.class_names},
                'gbm': {regime: gbm_preds.count(regime) for regime in self.class_names},
                'ensemble': {regime: ensemble_preds.count(regime) for regime in self.class_names}
            },
            'current_weights': self.current_weights.copy(),
            'cv_scores': {
                'rf': self.cv_scores["rf"].mean() if self.cv_scores["rf"] is not None else None,
                'gbm': self.cv_scores["gbm"].mean() if self.cv_scores["gbm"] is not None else None
            }
        }


def main():
    """RF-XGBoost ì•™ìƒë¸” ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸"""
    import argparse
    import json
    import sys
    from pathlib import Path
    
    parser = argparse.ArgumentParser(description="RF-XGBoost ì•™ìƒë¸” ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°")
    parser.add_argument("--train", action="store_true", help="ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ")
    parser.add_argument("--predict", action="store_true", help="ì•™ìƒë¸” ì˜ˆì¸¡")
    parser.add_argument("--config", type=str, default="config/config_trader.json", help="ì„¤ì • íŒŒì¼")
    parser.add_argument("--model-path", type=str, default="models/trader/rf_xgb_ensemble_model.pkl", help="ëª¨ë¸ ì €ì¥/ë¡œë”© ê²½ë¡œ")
    parser.add_argument("--force-retrain", action="store_true", help="ê°•ì œ ì¬í•™ìŠµ")
    
    args = parser.parse_args()
    
    try:
        with open(args.config, "r", encoding="utf-8") as f:
            config = json.load(f)
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # ì•™ìƒë¸” ì„¤ì • í™•ì¸ (ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’)
    if "ensemble_regime" not in config:
        config["ensemble_regime"] = {
            "voting_strategy": "weighted",
            "enable_dynamic_weights": True,
            "rf_weight": 0.5,
            "gbm_weight": 0.5
        }
    
    classifier = RFXGBEnsembleRegimeClassifier(config)
    
    if args.train:
        print("ğŸ”„ RF-XGBoost ì•™ìƒë¸” ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘")
        
        # ë°ì´í„° ë¡œë”©
        from ml_regime_classifier import DynamicMLRegimeClassifier
        ml_classifier = DynamicMLRegimeClassifier(config)
        macro_data = ml_classifier.load_comprehensive_data_from_files()
        
        if macro_data is None or macro_data.empty:
            print("âŒ ë§¤í¬ë¡œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            sys.exit(1)
        
        print(f"ğŸ“Š ë§¤í¬ë¡œ ë°ì´í„°: {len(macro_data)} í–‰, {len(macro_data.columns)}ê°œ ì»¬ëŸ¼")
        
        # í•™ìŠµ ì‹¤í–‰
        success = classifier.fit(macro_data, force_retrain=args.force_retrain)
        
        if success:
            # ëª¨ë¸ ì €ì¥
            os.makedirs(os.path.dirname(args.model_path), exist_ok=True)
            with open(args.model_path, 'wb') as f:
                pickle.dump(classifier, f)
            print(f"âœ… RF-XGBoost ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {args.model_path}")
        else:
            print("âŒ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
            sys.exit(1)
    
    elif args.predict:
        print("ğŸ”® RF-XGBoost ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘")
        
        # ëª¨ë¸ ë¡œë”©
        if os.path.exists(args.model_path):
            with open(args.model_path, 'rb') as f:
                classifier = pickle.load(f)
        else:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {args.model_path}")
            sys.exit(1)
        
        # ë°ì´í„° ë¡œë”©
        from ml_regime_classifier import DynamicMLRegimeClassifier
        ml_classifier = DynamicMLRegimeClassifier(config)
        macro_data = ml_classifier.load_comprehensive_data_from_files()
        
        if macro_data is None or macro_data.empty:
            print("âŒ ë§¤í¬ë¡œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            sys.exit(1)
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        result = classifier.predict(macro_data)
        
        print(f"\nğŸ“Š RF-XGBoost ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"ğŸ¯ ì˜ˆì¸¡ ì²´ì œ: {result['predicted_regime']}")
        print(f"ğŸ“ˆ ì‹ ë¢°ë„: {result['confidence']:.3f}")
        print(f"ğŸ—³ï¸ íˆ¬í‘œ ì „ëµ: {result['voting_strategy']}")
        
        print(f"\nğŸ” ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡:")
        for model_name, pred_info in result['individual_predictions'].items():
            model_display = "RandomForest" if model_name == "rf" else classifier.gbm_name
            print(f"  {model_display}: {pred_info['regime']} (ì‹ ë¢°ë„: {pred_info['confidence']:.3f})")
        
        print(f"\nâš–ï¸ í˜„ì¬ ê°€ì¤‘ì¹˜:")
        for model_name, weight in result['weights'].items():
            model_display = "RandomForest" if model_name == "rf" else classifier.gbm_name
            print(f"  {model_display}: {weight:.3f}")
        
        # ì˜ˆì¸¡ ìš”ì•½ í†µê³„
        summary = classifier.get_prediction_summary()
        if 'total_predictions' in summary:
            print(f"\nğŸ“Š ì˜ˆì¸¡ í†µê³„ (ìµœê·¼ {summary['total_predictions']}íšŒ):")
            print(f"  ëª¨ë¸ ì¼ì¹˜ìœ¨: {summary['agreement_rate']:.1%}")
            print(f"  í‰ê·  ì‹ ë¢°ë„: RF {summary['average_confidence']['rf']:.3f}, "
                  f"{classifier.gbm_name} {summary['average_confidence']['gbm']:.3f}, "
                  f"ì•™ìƒë¸” {summary['average_confidence']['ensemble']:.3f}")
            
            if summary['cv_scores']['rf'] is not None:
                print(f"  CV ì ìˆ˜: RF {summary['cv_scores']['rf']:.4f}, "
                      f"{classifier.gbm_name} {summary['cv_scores']['gbm']:.4f}")
    
    else:
        print("ì‚¬ìš©ë²•:")
        print("  --train          # RF-XGBoost ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ")
        print("  --predict        # ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰")
        print("  --force-retrain  # ê°•ì œ ì¬í•™ìŠµ")


if __name__ == "__main__":
    main()
            latest_features = features.iloc[-1:].values
            X_scaled = self.ml_classifier.scaler.transform(latest_features)
            
            ml_proba = self.ml_classifier.model.predict_proba(X_scaled)[0]
            ml_pred = self.ml_classifier.model.predict(X_scaled)[0]
            ml_pred_label = self.label_encoder.inverse_transform([ml_pred])[0]
            
            # 2. ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
            ensemble_result = self._ensemble_predict(hmm_results, ml_pred_label, ml_proba)
            
            # 3. ì˜ˆì¸¡ ì´ë ¥ ì €ì¥
            prediction_record = {
                'timestamp': datetime.now(),
                'hmm_prediction': hmm_results.get('predicted_regime', 'SIDEWAYS'),
                'ml_prediction': ml_pred_label,
                'ensemble_prediction': ensemble_result['predicted_regime'],
                'hmm_confidence': hmm_results.get('confidence', 0.5),
                'ml_confidence': float(np.max(ml_proba)),
                'ensemble_confidence': ensemble_result['confidence'],
                'weights_used': self.current_weights.copy()
            }
            self.prediction_history.append(prediction_record)
            
            # 4. ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ì˜µì…˜)
            if self.enable_dynamic_weights and len(self.prediction_history) > 10:
                self._update_dynamic_weights()
            
            logger.info(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì™„ë£Œ: {ensemble_result['predicted_regime']} "
                       f"(ì‹ ë¢°ë„: {ensemble_result['confidence']:.3f})")
            
            return ensemble_result
            
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return self._fallback_prediction(macro_data)

    def _ensemble_predict(self, hmm_results: Dict, ml_pred: str, ml_proba: np.ndarray) -> Dict:
        """ì•™ìƒë¸” íˆ¬í‘œ ìˆ˜í–‰"""
        
        hmm_pred = hmm_results.get('predicted_regime', 'SIDEWAYS')
        hmm_confidence = hmm_results.get('confidence', 0.5)
        ml_confidence = float(np.max(ml_proba))
        
        if self.voting_strategy == "soft":
            # Soft Voting: í™•ë¥  ë¶„í¬ ê²°í•©
            ensemble_pred, ensemble_conf = self._soft_voting(
                hmm_pred, hmm_confidence, ml_pred, ml_proba
            )
            
        elif self.voting_strategy == "hard":
            # Hard Voting: ë‹¤ìˆ˜ê²°
            ensemble_pred, ensemble_conf = self._hard_voting(
                hmm_pred, hmm_confidence, ml_pred, ml_confidence
            )
            
        elif self.voting_strategy == "weighted":
            # Weighted Voting: ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            ensemble_pred, ensemble_conf = self._weighted_voting(
                hmm_pred, hmm_confidence, ml_pred, ml_confidence
            )
            
        elif self.voting_strategy == "confidence":
            # Confidence-based: ë” í™•ì‹ í•˜ëŠ” ëª¨ë¸ ì„ íƒ
            ensemble_pred, ensemble_conf = self._confidence_based_voting(
                hmm_pred, hmm_confidence, ml_pred, ml_confidence
            )
            
        else:
            # ê¸°ë³¸ê°’: Weighted
            ensemble_pred, ensemble_conf = self._weighted_voting(
                hmm_pred, hmm_confidence, ml_pred, ml_confidence
            )
        
        return {
            'predicted_regime': ensemble_pred,
            'confidence': ensemble_conf,
            'individual_predictions': {
                'hmm': {'regime': hmm_pred, 'confidence': hmm_confidence},
                'ml': {'regime': ml_pred, 'confidence': ml_confidence}
            },
            'voting_strategy': self.voting_strategy,
            'weights': self.current_weights.copy()
        }

    def _soft_voting(self, hmm_pred: str, hmm_conf: float, ml_pred: str, ml_proba: np.ndarray) -> Tuple[str, float]:
        """Soft Voting: í™•ë¥  ë¶„í¬ ê°€ì¤‘ ê²°í•©"""
        
        # HMM ê²°ê³¼ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë°©ì‹)
        hmm_proba = np.zeros(len(self.class_names))
        hmm_idx = self.class_names.index(hmm_pred)
        hmm_proba[hmm_idx] = hmm_conf
        
        # ë‚˜ë¨¸ì§€ í™•ë¥ ì„ ê· ë“±í•˜ê²Œ ë¶„ë°°
        remaining_prob = 1.0 - hmm_conf
        for i in range(len(self.class_names)):
            if i != hmm_idx:
                hmm_proba[i] = remaining_prob / (len(self.class_names) - 1)
        
        # ê°€ì¤‘ ê²°í•©
        ensemble_proba = (
            self.current_weights['hmm'] * hmm_proba + 
            self.current_weights['ml'] * ml_proba
        )
        
        # ìµœì¢… ì˜ˆì¸¡
        ensemble_idx = np.argmax(ensemble_proba)
        ensemble_pred = self.class_names[ensemble_idx]
        ensemble_conf = float(ensemble_proba[ensemble_idx])
        
        return ensemble_pred, ensemble_conf

    def _hard_voting(self, hmm_pred: str, hmm_conf: float, ml_pred: str, ml_conf: float) -> Tuple[str, float]:
        """Hard Voting: ë‹¤ìˆ˜ê²° íˆ¬í‘œ"""
        
        # ê°„ë‹¨í•œ ê²½ìš°: 2ê°œ ëª¨ë¸ì´ë¯€ë¡œ ì¼ì¹˜/ë¶ˆì¼ì¹˜ë§Œ í™•ì¸
        if hmm_pred == ml_pred:
            # ì¼ì¹˜í•˜ëŠ” ê²½ìš°: í‰ê·  ì‹ ë¢°ë„
            return hmm_pred, (hmm_conf + ml_conf) / 2
        else:
            # ë¶ˆì¼ì¹˜í•˜ëŠ” ê²½ìš°: ë” ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ ëª¨ë¸ ì„ íƒ
            if hmm_conf >= ml_conf:
                return hmm_pred, hmm_conf
            else:
                return ml_pred, ml_conf

    def _weighted_voting(self, hmm_pred: str, hmm_conf: float, ml_pred: str, ml_conf: float) -> Tuple[str, float]:
        """Weighted Voting: ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ íˆ¬í‘œ"""
        
        # ê° ì˜ˆì¸¡ì— ê°€ì¤‘ì¹˜ ì ìš©
        hmm_score = self.current_weights['hmm'] * hmm_conf
        ml_score = self.current_weights['ml'] * ml_conf
        
        # ê°™ì€ ì˜ˆì¸¡ì¸ ê²½ìš°
        if hmm_pred == ml_pred:
            return hmm_pred, (hmm_score + ml_score)
        
        # ë‹¤ë¥¸ ì˜ˆì¸¡ì¸ ê²½ìš°: ê°€ì¤‘ ì ìˆ˜ê°€ ë†’ì€ ê²ƒ ì„ íƒ
        if hmm_score >= ml_score:
            return hmm_pred, hmm_score
        else:
            return ml_pred, ml_score

    def _confidence_based_voting(self, hmm_pred: str, hmm_conf: float, ml_pred: str, ml_conf: float) -> Tuple[str, float]:
        """Confidence-based: ë” í™•ì‹ í•˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ì‚¬ìš©"""
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ë§Œ ê³ ë ¤
        confidence_threshold = 0.6
        
        if hmm_conf >= confidence_threshold and ml_conf >= confidence_threshold:
            # ë‘˜ ë‹¤ í™•ì‹ í•˜ëŠ” ê²½ìš°: ê°€ì¤‘ íˆ¬í‘œ
            return self._weighted_voting(hmm_pred, hmm_conf, ml_pred, ml_conf)
        elif hmm_conf >= confidence_threshold:
            # HMMë§Œ í™•ì‹ í•˜ëŠ” ê²½ìš°
            return hmm_pred, hmm_conf
        elif ml_conf >= confidence_threshold:
            # MLë§Œ í™•ì‹ í•˜ëŠ” ê²½ìš°  
            return ml_pred, ml_conf
        else:
            # ë‘˜ ë‹¤ í™•ì‹ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°: ê°€ì¤‘ íˆ¬í‘œ
            return self._weighted_voting(hmm_pred, hmm_conf, ml_pred, ml_conf)

    def _update_dynamic_weights(self):
        """ìµœê·¼ ì„±ëŠ¥ì— ë”°ë¥¸ ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        
        if len(self.prediction_history) < 20:
            return
        
        try:
            # ìµœê·¼ 20ê°œ ì˜ˆì¸¡ ë¶„ì„
            recent_predictions = self.prediction_history[-20:]
            
            # ê° ëª¨ë¸ì˜ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ì„ì‹œì ìœ¼ë¡œ ë‹¨ìˆœí•œ ë°©ì‹ ì‚¬ìš©)
            hmm_consistency = self._calculate_consistency([p['hmm_prediction'] for p in recent_predictions])
            ml_consistency = self._calculate_consistency([p['ml_prediction'] for p in recent_predictions])
            
            # ì‹ ë¢°ë„ í‰ê· 
            hmm_avg_conf = np.mean([p['hmm_confidence'] for p in recent_predictions])
            ml_avg_conf = np.mean([p['ml_confidence'] for p in recent_predictions])
            
            # ì¢…í•© ì ìˆ˜ (ì¼ê´€ì„± + ì‹ ë¢°ë„)
            hmm_score = (hmm_consistency * 0.6 + hmm_avg_conf * 0.4)
            ml_score = (ml_consistency * 0.6 + ml_avg_conf * 0.4)
            
            # ì •ê·œí™”í•˜ì—¬ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            total_score = hmm_score + ml_score
            if total_score > 0:
                new_hmm_weight = hmm_score / total_score
                new_ml_weight = ml_score / total_score
                
                # ê¸‰ê²©í•œ ë³€í™” ë°©ì§€ (ê´€ì„± ì ìš©)
                inertia = 0.7
                self.current_weights['hmm'] = (
                    inertia * self.current_weights['hmm'] + 
                    (1 - inertia) * new_hmm_weight
                )
                self.current_weights['ml'] = (
                    inertia * self.current_weights['ml'] + 
                    (1 - inertia) * new_ml_weight
                )
                
                logger.info(f"ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ - HMM: {self.current_weights['hmm']:.3f}, "
                           f"ML: {self.current_weights['ml']:.3f}")
                
        except Exception as e:
            logger.warning(f"ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _calculate_consistency(self, predictions: List[str]) -> float:
        """ì˜ˆì¸¡ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        if len(predictions) <= 1:
            return 0.5
        
        # ì—°ì†ëœ ì˜ˆì¸¡ì´ ê°™ì€ ê²½ìš°ë¥¼ ì¼ê´€ì„±ìœ¼ë¡œ í‰ê°€
        consistent_count = 0
        for i in range(1, len(predictions)):
            if predictions[i] == predictions[i-1]:
                consistent_count += 1
        
        return consistent_count / (len(predictions) - 1)

    def _fallback_prediction(self, macro_data: pd.DataFrame) -> Dict:
        """ì˜¤ë¥˜ ì‹œ ëŒ€ì²´ ì˜ˆì¸¡"""
        logger.warning("ëŒ€ì²´ ì˜ˆì¸¡ ëª¨ë“œ í™œì„±í™”")
        
        try:
            # HMMë§Œ ì‚¬ìš©
            if self.hmm_classifier.is_fitted:
                return self.hmm_classifier.predict_regime(macro_data)
        except:
            pass
        
        # ì™„ì „í•œ ëŒ€ì²´: ê¸°ë³¸ ì˜ˆì¸¡
        return {
            'predicted_regime': 'SIDEWAYS',
            'confidence': 0.25,
            'individual_predictions': {
                'hmm': {'regime': 'SIDEWAYS', 'confidence': 0.25},
                'ml': {'regime': 'SIDEWAYS', 'confidence': 0.25}
            },
            'voting_strategy': 'fallback',
            'weights': {'hmm': 0.5, 'ml': 0.5}
        }

    def save_model(self, filepath: str):
        """ì•™ìƒë¸” ëª¨ë¸ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            model_data = {
                'config': self.config,
                'ensemble_config': self.ensemble_config,
                'voting_strategy': self.voting_strategy,
                'base_weights': self.base_weights,
                'current_weights': self.current_weights,
                'performance_history': self.performance_history,
                'prediction_history': self.prediction_history[-100:],  # ìµœê·¼ 100ê°œë§Œ ì €ì¥
                'class_names': self.class_names,
                'is_fitted': self.is_fitted,
                'ml_model': self.ml_classifier.model if self.ml_classifier.is_fitted else None,
                'ml_scaler': self.ml_classifier.scaler if self.ml_classifier.is_fitted else None,
                'ml_feature_names': self.ml_classifier.feature_names
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
                
            logger.info(f"ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_model(self, filepath: str):
        """ì•™ìƒë¸” ëª¨ë¸ ë¡œë”©"""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.ensemble_config = model_data['ensemble_config']
            self.voting_strategy = model_data['voting_strategy']
            self.base_weights = model_data['base_weights']
            self.current_weights = model_data['current_weights']
            self.performance_history = model_data['performance_history']
            self.prediction_history = model_data.get('prediction_history', [])
            self.class_names = model_data['class_names']
            self.is_fitted = model_data['is_fitted']
            
            # ML ëª¨ë¸ ë³µì›
            if model_data.get('ml_model'):
                self.ml_classifier.model = model_data['ml_model']
                self.ml_classifier.scaler = model_data['ml_scaler'] 
                self.ml_classifier.feature_names = model_data['ml_feature_names']
                self.ml_classifier.is_fitted = True
            
            logger.info(f"ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {filepath}")
            
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

    def get_prediction_summary(self, n_recent: int = 20) -> Dict:
        """ìµœê·¼ ì˜ˆì¸¡ ìš”ì•½ í†µê³„"""
        
        if len(self.prediction_history) == 0:
            return {"message": "ì˜ˆì¸¡ ì´ë ¥ ì—†ìŒ"}
        
        recent = self.prediction_history[-n_recent:] if n_recent > 0 else self.prediction_history
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ ë¶„í¬
        hmm_preds = [p['hmm_prediction'] for p in recent]
        ml_preds = [p['ml_prediction'] for p in recent]  
        ensemble_preds = [p['ensemble_prediction'] for p in recent]
        
        # ì¼ì¹˜ìœ¨ ê³„ì‚°
        agreements = sum(1 for p in recent if p['hmm_prediction'] == p['ml_prediction'])
        agreement_rate = agreements / len(recent)
        
        # í‰ê·  ì‹ ë¢°ë„
        avg_hmm_conf = np.mean([p['hmm_confidence'] for p in recent])
        avg_ml_conf = np.mean([p['ml_confidence'] for p in recent])
        avg_ensemble_conf = np.mean([p['ensemble_confidence'] for p in recent])
        
        return {
            'total_predictions': len(recent),
            'agreement_rate': agreement_rate,
            'average_confidence': {
                'hmm': avg_hmm_conf,
                'ml': avg_ml_conf,
                'ensemble': avg_ensemble_conf
            },
            'prediction_distribution': {
                'hmm': {regime: hmm_preds.count(regime) for regime in self.class_names},
                'ml': {regime: ml_preds.count(regime) for regime in self.class_names},
                'ensemble': {regime: ensemble_preds.count(regime) for regime in self.class_names}
            },
            'current_weights': self.current_weights.copy()
        }


def main():
    """ì•™ìƒë¸” ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸"""
    import argparse
    import json
    import sys
    from pathlib import Path
    
    parser = argparse.ArgumentParser(description="ì•™ìƒë¸” ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°")
    parser.add_argument("--train", action="store_true", help="ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ")
    parser.add_argument("--predict", action="store_true", help="ì•™ìƒë¸” ì˜ˆì¸¡")
    parser.add_argument("--config", type=str, default="config/config_trader.json", help="ì„¤ì • íŒŒì¼")
    parser.add_argument("--model-path", type=str, default="models/trader/ensemble_regime_model.pkl", help="ëª¨ë¸ ì €ì¥/ë¡œë”© ê²½ë¡œ")
    parser.add_argument("--force-retrain", action="store_true", help="ê°•ì œ ì¬í•™ìŠµ")
    
    args = parser.parse_args()
    
    try:
        with open(args.config, "r", encoding="utf-8") as f:
            config = json.load(f)
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # ì•™ìƒë¸” ì„¤ì • ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
    if "ensemble_regime" not in config:
        config["ensemble_regime"] = {
            "voting_strategy": "weighted",
            "enable_dynamic_weights": True,
            "hmm_weight": 0.6,
            "ml_weight": 0.4
        }
    
    classifier = EnsembleRegimeClassifier(config)
    
    if args.train:
        print("ğŸ”„ ì•™ìƒë¸” ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘")
        
        # ë°ì´í„° ë¡œë”©
        macro_data = classifier.ml_classifier.load_comprehensive_data_from_files()
        if macro_data is None or macro_data.empty:
            print("âŒ ë§¤í¬ë¡œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            sys.exit(1)
        
        print(f"ğŸ“Š ë§¤í¬ë¡œ ë°ì´í„°: {len(macro_data)} í–‰, {len(macro_data.columns)}ê°œ ì»¬ëŸ¼")
        
        # í•™ìŠµ ì‹¤í–‰
        success = classifier.fit(macro_data, force_retrain=args.force_retrain)
        
        if success:
            # ëª¨ë¸ ì €ì¥
            classifier.save_model(args.model_path)
            print(f"âœ… ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {args.model_path}")
        else:
            print("âŒ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
            sys.exit(1)
    
    elif args.predict:
        print("ğŸ”® ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘")
        
        # ëª¨ë¸ ë¡œë”©
        if os.path.exists(args.model_path):
            classifier.load_model(args.model_path)
        else:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {args.model_path}")
            sys.exit(1)
        
        # ë°ì´í„° ë¡œë”©
        macro_data = classifier.ml_classifier.load_comprehensive_data_from_files()
        if macro_data is None or macro_data.empty:
            print("âŒ ë§¤í¬ë¡œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            sys.exit(1)
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        result = classifier.predict(macro_data)
        
        print(f"\nğŸ“Š ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"ğŸ¯ ì˜ˆì¸¡ ì²´ì œ: {result['predicted_regime']}")
        print(f"ğŸ“ˆ ì‹ ë¢°ë„: {result['confidence']:.3f}")
        print(f"ğŸ—³ï¸ íˆ¬í‘œ ì „ëµ: {result['voting_strategy']}")
        
        print(f"\nğŸ” ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡:")
        for model_name, pred_info in result['individual_predictions'].items():
            print(f"  {model_name.upper()}: {pred_info['regime']} (ì‹ ë¢°ë„: {pred_info['confidence']:.3f})")
        
        print(f"\nâš–ï¸ í˜„ì¬ ê°€ì¤‘ì¹˜:")
        for model_name, weight in result['weights'].items():
            print(f"  {model_name.upper()}: {weight:.3f}")
        
        # ì˜ˆì¸¡ ìš”ì•½ í†µê³„
        summary = classifier.get_prediction_summary()
        if 'total_predictions' in summary:
            print(f"\nğŸ“Š ì˜ˆì¸¡ í†µê³„ (ìµœê·¼ {summary['total_predictions']}íšŒ):")
            print(f"  ëª¨ë¸ ì¼ì¹˜ìœ¨: {summary['agreement_rate']:.1%}")
            print(f"  í‰ê·  ì‹ ë¢°ë„: HMM {summary['average_confidence']['hmm']:.3f}, "
                  f"ML {summary['average_confidence']['ml']:.3f}, "
                  f"ì•™ìƒë¸” {summary['average_confidence']['ensemble']:.3f}")
    
    else:
        print("ì‚¬ìš©ë²•:")
        print("  --train          # ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ")
        print("  --predict        # ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰")
        print("  --force-retrain  # ê°•ì œ ì¬í•™ìŠµ")


if __name__ == "__main__":
    main()