#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ê°•í™”í•™ìŠµ í€€íŠ¸ë§¤ë§¤ ì‹œìŠ¤í…œ

ì „í†µ í€€íŠ¸ ì „ëµ + ê°•í™”í•™ìŠµ ì¡°í•©:
1. ì „í†µ ì „ëµ ì‹ í˜¸ë¥¼ ìƒíƒœì— í¬í•¨
2. ê°•í™”í•™ìŠµì´ ì „í†µ ì‹ í˜¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµ
3. ìµœì¢… í–‰ë™ = ì „í†µ ì‹ í˜¸ + RL ë³´ì •
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pandas as pd
from collections import deque, namedtuple
import random
from typing import Dict, List, Tuple, Optional, Any
import logging
from datetime import datetime
import os
import gym
from gym import spaces

# ê¸°ì¡´ í”„ë ˆì„ì›Œí¬ import
from .strategies import BaseStrategy
from .calculate_index import StrategyParams, TechnicalIndicators
from .backtest_strategies import BacktestEngine

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger = logging.getLogger(__name__)

# Experience êµ¬ì¡°ì²´
Experience = namedtuple(
    "Experience", ["state", "action", "reward", "next_state", "done", "log_prob"]
)


class HybridTradingEnvironment(gym.Env):
    """í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ë ˆì´ë”© í™˜ê²½ (ì „í†µ ì‹ í˜¸ + RL)"""

    def __init__(
        self,
        data: pd.DataFrame,
        traditional_signals: pd.DataFrame,
        window_size: int = 20,
        initial_balance: float = 100000.0,
        transaction_cost: float = 0.001,
        min_holding_days: int = 1,
        max_holding_days: int = 21,
        action_type: str = "continuous",
    ):
        """
        Args:
            data: ê°€ê²© ë°ì´í„°
            traditional_signals: ì „í†µ ì „ëµ ì‹ í˜¸ (swing ì „ëµ ê²°ê³¼)
            action_type: "discrete" or "continuous"
        """
        super().__init__()

        self.data = data.copy()
        self.traditional_signals = traditional_signals.copy()
        self.window_size = window_size
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        self.min_holding_days = min_holding_days
        self.max_holding_days = max_holding_days
        self.action_type = action_type

        # ìƒíƒœ ë³€ìˆ˜ë“¤
        self.current_step = 0
        self.balance = initial_balance
        self.position = 0  # -1: ìˆ, 0: í˜„ê¸ˆ, 1: ë¡±
        self.position_size = 0
        self.entry_price = 0
        self.entry_step = 0
        self.holding_days = 0

        # ì„±ê³¼ ì¶”ì 
        self.portfolio_value_history = []
        self.trade_history = []
        self.max_portfolio_value = initial_balance

        # ìƒíƒœ íŠ¹ì„± ì •ì˜
        self.feature_columns = self._define_features()
        self.n_features = len(self.feature_columns)

        # ì „í†µ ì‹ í˜¸ ì»¬ëŸ¼
        self.signal_columns = self._define_signal_columns()

        # Gym í™˜ê²½ ì„¤ì •
        if action_type == "discrete":
            # ì´ì‚° í–‰ë™: [ë§¤ë„, ë³´ìœ , ë§¤ìˆ˜]
            self.action_space = spaces.Discrete(3)
        else:
            # ì—°ì† í–‰ë™: [ì „í†µ ì‹ í˜¸ ê°€ì¤‘ì¹˜ (-1 ~ 1), RL ë³´ì • (-0.5 ~ 0.5)]
            self.action_space = spaces.Box(
                low=np.array([-1.0, -0.5]), high=np.array([1.0, 0.5]), dtype=np.float32
            )

        # ìƒíƒœ ê³µê°„: [window_size * (n_features + signal_features + position_info)]
        state_size = self.window_size * (self.n_features + len(self.signal_columns) + 4)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(state_size,), dtype=np.float32
        )

        # ë°ì´í„° ì •ê·œí™”
        self._normalize_data()

    def _define_features(self) -> List[str]:
        """ìƒíƒœë¡œ ì‚¬ìš©í•  íŠ¹ì„±ë“¤ ì •ì˜"""
        base_features = ["open", "high", "low", "close", "volume"]

        technical_features = [
            "rsi",
            "macd",
            "macd_signal",
            "macd_histogram",
            "bb_upper",
            "bb_middle",
            "bb_lower",
            "ema_short",
            "ema_long",
            "atr",
            "stoch_k",
            "stoch_d",
            "williams_r",
            "cci",
        ]

        available_features = []
        for feature in base_features + technical_features:
            if feature in self.data.columns:
                available_features.append(feature)

        logger.info(
            f"ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±: {len(available_features)}ê°œ - {available_features}"
        )
        return available_features

    def _define_signal_columns(self) -> List[str]:
        """ì „í†µ ì‹ í˜¸ ì»¬ëŸ¼ ì •ì˜"""
        signal_columns = []

        # ì „í†µ ì „ëµ ì‹ í˜¸ ì»¬ëŸ¼ë“¤
        potential_signals = ["signal", "position", "weight", "confidence", "score"]

        for col in potential_signals:
            if col in self.traditional_signals.columns:
                signal_columns.append(col)

        logger.info(f"ì „í†µ ì‹ í˜¸ ì»¬ëŸ¼: {len(signal_columns)}ê°œ - {signal_columns}")
        return signal_columns

    def _normalize_data(self):
        """ë°ì´í„° ì •ê·œí™” (Z-score)"""
        self.data_normalized = self.data.copy()

        for feature in self.feature_columns:
            if feature in self.data.columns:
                mean = self.data[feature].mean()
                std = self.data[feature].std()
                if std > 0:
                    self.data_normalized[feature] = (self.data[feature] - mean) / std
                else:
                    self.data_normalized[feature] = 0

        # ì „í†µ ì‹ í˜¸ ì •ê·œí™”
        self.signals_normalized = self.traditional_signals.copy()
        for signal_col in self.signal_columns:
            if signal_col in self.traditional_signals.columns:
                mean = self.traditional_signals[signal_col].mean()
                std = self.traditional_signals[signal_col].std()
                if std > 0:
                    self.signals_normalized[signal_col] = (
                        self.traditional_signals[signal_col] - mean
                    ) / std
                else:
                    self.signals_normalized[signal_col] = 0

    def reset(self) -> np.ndarray:
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.current_step = self.window_size
        self.balance = self.initial_balance
        self.position = 0
        self.position_size = 0
        self.entry_price = 0
        self.entry_step = 0
        self.holding_days = 0

        self.portfolio_value_history = []
        self.trade_history = []
        self.max_portfolio_value = self.initial_balance

        return self._get_state()

    def _get_state(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜ (ê°€ê²© ë°ì´í„° + ì „í†µ ì‹ í˜¸ + í¬ì§€ì…˜ ì •ë³´)"""
        if self.current_step < self.window_size:
            # ì´ˆê¸° ë°ì´í„° ë¶€ì¡± ì‹œ
            state = np.zeros((self.window_size, self.n_features))
            signals_state = np.zeros((self.window_size, len(self.signal_columns)))

            available_data = self.data_normalized.iloc[: self.current_step][
                self.feature_columns
            ].values
            available_signals = self.signals_normalized.iloc[: self.current_step][
                self.signal_columns
            ].values

            state[-len(available_data) :] = available_data
            signals_state[-len(available_signals) :] = available_signals
        else:
            start_idx = self.current_step - self.window_size
            end_idx = self.current_step

            state = self.data_normalized.iloc[start_idx:end_idx][
                self.feature_columns
            ].values
            signals_state = self.signals_normalized.iloc[start_idx:end_idx][
                self.signal_columns
            ].values

        # í¬ì§€ì…˜ ì •ë³´ ì¶”ê°€
        position_info = np.array(
            [
                self.position,
                self.position_size / self.initial_balance,
                self.holding_days / self.max_holding_days,
                (
                    (self.current_step - self.entry_step) / len(self.data)
                    if self.position != 0
                    else 0
                ),
            ]
        )
        position_expanded = np.tile(position_info, (self.window_size, 1))

        # ëª¨ë“  ìƒíƒœ ì •ë³´ ê²°í•©
        state_with_signals = np.concatenate(
            [state, signals_state, position_expanded], axis=1
        )
        return state_with_signals.flatten()

    def _get_traditional_signal(self) -> float:
        """í˜„ì¬ ì‹œì ì˜ ì „í†µ ì‹ í˜¸ ê°€ì ¸ì˜¤ê¸°"""
        if self.current_step >= len(self.traditional_signals):
            return 0.0

        # ì „í†µ ì‹ í˜¸ ê²°í•© (ê°€ì¤‘ í‰ê· )
        current_signals = self.traditional_signals.iloc[self.current_step]

        signal_value = 0.0
        total_weight = 0.0

        if "signal" in current_signals:
            signal_value += current_signals["signal"] * 0.4
            total_weight += 0.4

        if "position" in current_signals:
            signal_value += current_signals["position"] * 0.3
            total_weight += 0.3

        if "weight" in current_signals:
            signal_value += current_signals["weight"] * 0.2
            total_weight += 0.2

        if "confidence" in current_signals:
            signal_value += current_signals["confidence"] * 0.1
            total_weight += 0.1

        if total_weight > 0:
            return signal_value / total_weight
        else:
            return 0.0

    def step(self, action) -> Tuple[np.ndarray, float, bool, Dict]:
        """í•œ ìŠ¤í… ì‹¤í–‰"""
        if self.current_step >= len(self.data) - 1:
            return self._get_state(), 0, True, {}

        current_price = self.data.iloc[self.current_step]["close"]
        next_price = self.data.iloc[self.current_step + 1]["close"]

        # ì „í†µ ì‹ í˜¸ ê°€ì ¸ì˜¤ê¸°
        traditional_signal = self._get_traditional_signal()

        # í–‰ë™ ì‹¤í–‰
        if self.action_type == "discrete":
            reward = self._execute_discrete_action(
                action, traditional_signal, current_price, next_price
            )
        else:
            reward = self._execute_continuous_action(
                action, traditional_signal, current_price, next_price
            )

        # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
        self.current_step += 1

        # ë³´ìœ  ê¸°ê°„ ì—…ë°ì´íŠ¸
        if self.position != 0:
            self.holding_days += 1

        # í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ì—…ë°ì´íŠ¸
        portfolio_value = self._calculate_portfolio_value()
        self.portfolio_value_history.append(portfolio_value)

        # ìµœëŒ€ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ì—…ë°ì´íŠ¸
        if portfolio_value > self.max_portfolio_value:
            self.max_portfolio_value = portfolio_value

        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        done = (self.current_step >= len(self.data) - 1) or (
            portfolio_value <= self.initial_balance * 0.5
        )

        info = {
            "portfolio_value": portfolio_value,
            "position": self.position,
            "balance": self.balance,
            "holding_days": self.holding_days,
            "traditional_signal": traditional_signal,
            "drawdown": (self.max_portfolio_value - portfolio_value)
            / self.max_portfolio_value,
        }

        return self._get_state(), reward, done, info

    def _execute_discrete_action(
        self,
        action: int,
        traditional_signal: float,
        current_price: float,
        next_price: float,
    ) -> float:
        """ì´ì‚° í–‰ë™ ì‹¤í–‰ (ì „í†µ ì‹ í˜¸ì™€ ì¡°í•©)"""
        reward = 0

        def calculate_transaction_cost(amount):
            return amount * self.transaction_cost

        # ì „í†µ ì‹ í˜¸ë¥¼ ê³ ë ¤í•œ í–‰ë™ ê²°ì •
        # action: 0(ë§¤ë„), 1(ë³´ìœ ), 2(ë§¤ìˆ˜)
        # traditional_signal: -1 ~ 1 (ë§¤ë„ ~ ë§¤ìˆ˜)

        # ì „í†µ ì‹ í˜¸ì™€ RL í–‰ë™ì˜ ì¼ì¹˜ë„ ê³„ì‚°
        signal_alignment = 0.0
        if action == 2 and traditional_signal > 0:  # RL ë§¤ìˆ˜ + ì „í†µ ë§¤ìˆ˜
            signal_alignment = traditional_signal
        elif action == 0 and traditional_signal < 0:  # RL ë§¤ë„ + ì „í†µ ë§¤ë„
            signal_alignment = -traditional_signal
        elif action == 1 and abs(traditional_signal) < 0.2:  # RL ë³´ìœ  + ì „í†µ ì¤‘ë¦½
            signal_alignment = 1.0 - abs(traditional_signal)
        else:  # ì‹ í˜¸ ë¶ˆì¼ì¹˜
            signal_alignment = -abs(traditional_signal) * 0.5

        # ê¸°ì¡´ DQN ë¡œì§
        if action == 2 and self.position <= 0:  # ë§¤ìˆ˜
            if self.position < 0:  # ìˆ í¬ì§€ì…˜ ì²­ì‚°
                pnl = (self.entry_price - current_price) * abs(self.position_size)
                transaction_cost = calculate_transaction_cost(
                    abs(self.position_size) * current_price
                )
                self.balance += pnl - transaction_cost

                self.trade_history.append(
                    {
                        "type": "short_close",
                        "price": current_price,
                        "size": self.position_size,
                        "pnl": pnl - transaction_cost,
                        "holding_days": self.holding_days,
                        "traditional_signal": traditional_signal,
                    }
                )

            # ìƒˆë¡œìš´ ë¡± í¬ì§€ì…˜
            available_amount = self.balance * 0.95
            self.position_size = available_amount / current_price
            transaction_cost = calculate_transaction_cost(available_amount)
            self.balance -= available_amount + transaction_cost
            self.position = 1
            self.entry_price = current_price
            self.entry_step = self.current_step
            self.holding_days = 0

            price_change = (next_price - current_price) / current_price
            reward = price_change - self.transaction_cost + signal_alignment * 0.1

        elif action == 0 and self.position >= 0:  # ë§¤ë„
            if self.position > 0:
                if self.holding_days < self.min_holding_days:
                    reward = -0.02  # ì¡°ê¸° ë§¤ë„ í˜ë„í‹°
                else:
                    sell_amount = self.position_size * current_price
                    pnl = (current_price - self.entry_price) * self.position_size
                    transaction_cost = calculate_transaction_cost(sell_amount)
                    self.balance += sell_amount - transaction_cost

                    self.trade_history.append(
                        {
                            "type": "long_close",
                            "price": current_price,
                            "size": self.position_size,
                            "pnl": pnl - transaction_cost,
                            "holding_days": self.holding_days,
                            "traditional_signal": traditional_signal,
                        }
                    )

                    price_change = (current_price - self.entry_price) / self.entry_price
                    reward = (
                        price_change - self.transaction_cost + signal_alignment * 0.1
                    )

                self.position = 0
                self.position_size = 0
                self.entry_price = 0
                self.entry_step = 0
                self.holding_days = 0

        elif action == 1:  # ë³´ìœ 
            if self.position != 0:
                if self.holding_days >= self.max_holding_days:
                    # ê°•ì œ ì²­ì‚°
                    if self.position > 0:
                        sell_amount = self.position_size * current_price
                        pnl = (current_price - self.entry_price) * self.position_size
                        transaction_cost = calculate_transaction_cost(sell_amount)
                        self.balance += sell_amount - transaction_cost

                        self.trade_history.append(
                            {
                                "type": "long_close_forced",
                                "price": current_price,
                                "size": self.position_size,
                                "pnl": pnl - transaction_cost,
                                "holding_days": self.holding_days,
                                "traditional_signal": traditional_signal,
                            }
                        )

                        price_change = (
                            current_price - self.entry_price
                        ) / self.entry_price
                        reward = price_change - self.transaction_cost - 0.01

                        self.position = 0
                        self.position_size = 0
                        self.entry_price = 0
                        self.entry_step = 0
                        self.holding_days = 0
                else:
                    # ì •ìƒ ë³´ìœ 
                    if self.position > 0:
                        price_change = (next_price - current_price) / current_price
                        holding_bonus = (
                            min(self.holding_days / self.max_holding_days, 1.0) * 0.05
                        )
                        reward = (
                            price_change * (0.1 + holding_bonus)
                            + signal_alignment * 0.05
                        )
                    elif self.position < 0:
                        price_change = (current_price - next_price) / current_price
                        holding_bonus = (
                            min(self.holding_days / self.max_holding_days, 1.0) * 0.05
                        )
                        reward = (
                            price_change * (0.1 + holding_bonus)
                            + signal_alignment * 0.05
                        )
            else:
                market_return = (next_price - current_price) / current_price
                reward = -abs(market_return) * 0.05 + signal_alignment * 0.05

        return reward

    def _execute_continuous_action(
        self,
        action: np.ndarray,
        traditional_signal: float,
        current_price: float,
        next_price: float,
    ) -> float:
        """ì—°ì† í–‰ë™ ì‹¤í–‰ (ì „í†µ ì‹ í˜¸ì™€ ì¡°í•©)"""
        # action[0]: ì „í†µ ì‹ í˜¸ ê°€ì¤‘ì¹˜ (-1 ~ 1)
        # action[1]: RL ë³´ì • (-0.5 ~ 0.5)
        traditional_weight = action[0]
        rl_correction = action[1]

        # ìµœì¢… ì‹ í˜¸ = ì „í†µ ì‹ í˜¸ * ê°€ì¤‘ì¹˜ + RL ë³´ì •
        final_signal = traditional_signal * traditional_weight + rl_correction
        final_signal = np.clip(final_signal, -1.0, 1.0)

        reward = 0

        def calculate_transaction_cost(amount):
            return amount * self.transaction_cost

        # í˜„ì¬ í¬ì§€ì…˜ê³¼ ëª©í‘œ í¬ì§€ì…˜ì˜ ì°¨ì´ ê³„ì‚°
        current_position_ratio = (
            self.position_size * current_price / self.initial_balance
        )
        position_change = final_signal - current_position_ratio

        # í¬ì§€ì…˜ ì¡°ì •
        if abs(position_change) > 0.01:  # 1% ì´ìƒ ì°¨ì´ë‚  ë•Œë§Œ ê±°ë˜
            if position_change > 0:  # ë¡± í¬ì§€ì…˜ ì¦ê°€
                if self.position <= 0:  # ê¸°ì¡´ í¬ì§€ì…˜ ì²­ì‚°
                    if self.position < 0:
                        pnl = (self.entry_price - current_price) * abs(
                            self.position_size
                        )
                        transaction_cost = calculate_transaction_cost(
                            abs(self.position_size) * current_price
                        )
                        self.balance += pnl - transaction_cost

                        self.trade_history.append(
                            {
                                "type": "short_close",
                                "price": current_price,
                                "size": self.position_size,
                                "pnl": pnl - transaction_cost,
                                "holding_days": self.holding_days,
                                "traditional_signal": traditional_signal,
                                "final_signal": final_signal,
                            }
                        )

                # ìƒˆë¡œìš´ ë¡± í¬ì§€ì…˜
                target_amount = final_signal * self.initial_balance
                available_amount = min(target_amount, self.balance * 0.95)

                if available_amount > 0:
                    self.position_size = available_amount / current_price
                    transaction_cost = calculate_transaction_cost(available_amount)
                    self.balance -= available_amount + transaction_cost
                    self.position = 1
                    self.entry_price = current_price
                    self.entry_step = self.current_step
                    self.holding_days = 0

                    reward -= self.transaction_cost

            elif position_change < 0:  # í¬ì§€ì…˜ ê°ì†Œ
                if self.position > 0:
                    # ë¶€ë¶„ ì²­ì‚°
                    sell_ratio = min(abs(position_change), 1.0)
                    sell_amount = self.position_size * current_price * sell_ratio
                    pnl = (
                        (current_price - self.entry_price)
                        * self.position_size
                        * sell_ratio
                    )
                    transaction_cost = calculate_transaction_cost(sell_amount)
                    self.balance += sell_amount - transaction_cost

                    self.position_size *= 1 - sell_ratio
                    if self.position_size < 0.01:  # ê±°ì˜ ì—†ìœ¼ë©´ ì™„ì „ ì²­ì‚°
                        self.position = 0
                        self.position_size = 0
                        self.entry_price = 0
                        self.entry_step = 0
                        self.holding_days = 0

                    reward += pnl - transaction_cost

        # ë³´ìœ  ê¸°ê°„ ê°€ì¤‘ì¹˜ë¥¼ ê³ ë ¤í•œ ë³´ìƒ
        if self.position != 0:
            price_change = (
                (next_price - current_price) / current_price
                if self.position > 0
                else (current_price - next_price) / current_price
            )

            # ì „í†µ ì‹ í˜¸ì™€ì˜ ì¼ì¹˜ë„ ë³´ë„ˆìŠ¤
            signal_alignment = 1.0 - abs(final_signal - traditional_signal)
            holding_bonus = min(self.holding_days / self.max_holding_days, 1.0) * 0.05
            reward += price_change * (0.1 + holding_bonus) + signal_alignment * 0.05
        else:
            # í˜„ê¸ˆ ë³´ìœ  ì‹œ ê¸°íšŒë¹„ìš©
            market_return = (next_price - current_price) / current_price
            reward -= abs(market_return) * 0.05

        return reward

    def _calculate_portfolio_value(self) -> float:
        """í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°"""
        current_price = self.data.iloc[self.current_step]["close"]

        if self.position > 0:  # ë¡± í¬ì§€ì…˜
            position_value = self.position_size * current_price
            return self.balance + position_value
        elif self.position < 0:  # ìˆ í¬ì§€ì…˜
            position_value = (self.entry_price - current_price) * abs(
                self.position_size
            )
            return self.balance + position_value
        else:  # í˜„ê¸ˆ
            return self.balance


class HybridActorCriticNetwork(nn.Module):
    """í•˜ì´ë¸Œë¦¬ë“œ Actor-Critic ë„¤íŠ¸ì›Œí¬"""

    def __init__(
        self,
        state_size: int,
        action_size: int,
        hidden_size: int = 128,
        action_type: str = "continuous",
    ):
        super(HybridActorCriticNetwork, self).__init__()

        self.action_type = action_type

        # ê³µí†µ ë ˆì´ì–´
        self.fc1 = nn.Linear(state_size, hidden_size * 2)
        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)

        # Actor (ì •ì±…) ë„¤íŠ¸ì›Œí¬
        if action_type == "discrete":
            self.actor = nn.Linear(hidden_size // 2, action_size)
        else:
            self.actor = nn.Linear(hidden_size // 2, action_size * 2)  # mean, log_std

        # Critic (ê°€ì¹˜) ë„¤íŠ¸ì›Œí¬
        self.critic = nn.Linear(hidden_size // 2, 1)

        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))

        actor_output = self.actor(x)
        critic_output = self.critic(x)

        return actor_output, critic_output

    def get_action(self, state: torch.Tensor, training: bool = True):
        """í–‰ë™ ì„ íƒ"""
        actor_output, critic_value = self.forward(state)

        if self.action_type == "discrete":
            if training:
                action_probs = F.softmax(actor_output, dim=-1)
                dist = torch.distributions.Categorical(action_probs)
                action = dist.sample()
                log_prob = dist.log_prob(action)
                return action, log_prob, critic_value
            else:
                action = torch.argmax(actor_output, dim=-1)
                return action, None, critic_value
        else:
            mean, log_std = actor_output.chunk(2, dim=-1)
            log_std = torch.clamp(log_std, -20, 2)

            if training:
                std = log_std.exp()
                dist = torch.distributions.Normal(mean, std)
                action = dist.sample()
                log_prob = dist.log_prob(action).sum(dim=-1)
                return action, log_prob, critic_value
            else:
                return mean, None, critic_value


class HybridPPOTrainer:
    """í•˜ì´ë¸Œë¦¬ë“œ PPO í›ˆë ¨ê¸°"""

    def __init__(
        self,
        state_size: int,
        action_size: int,
        lr: float = 0.0003,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_ratio: float = 0.2,
        action_type: str = "continuous",
    ):

        self.state_size = state_size
        self.action_size = action_size
        self.lr = lr
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.action_type = action_type

        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.policy = HybridActorCriticNetwork(
            state_size, action_size, action_type=action_type
        ).to(device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        # í›ˆë ¨ í†µê³„
        self.loss_history = []

    def compute_gae(
        self, rewards: List[float], values: List[float], dones: List[bool]
    ) -> List[float]:
        """Generalized Advantage Estimation ê³„ì‚°"""
        advantages = []
        gae = 0

        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1]

            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
            advantages.insert(0, gae)

        return advantages

    def update_policy(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        old_log_probs: torch.Tensor,
        advantages: torch.Tensor,
        returns: torch.Tensor,
        num_epochs: int = 10,
    ):
        """ì •ì±… ì—…ë°ì´íŠ¸"""
        for _ in range(num_epochs):
            new_actions, new_log_probs, values = self.policy.get_action(
                states, training=True
            )

            # PPO í´ë¦¬í•‘
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = (
                torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)
                * advantages
            )

            # Actor ì†ì‹¤
            actor_loss = -torch.min(surr1, surr2).mean()

            # Critic ì†ì‹¤
            critic_loss = F.mse_loss(values.squeeze(), returns)

            # ì „ì²´ ì†ì‹¤
            loss = actor_loss + 0.5 * critic_loss

            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
            self.optimizer.step()

            self.loss_history.append(loss.item())

    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save(
            {
                "policy_state_dict": self.policy.state_dict(),
                "optimizer_state_dict": self.optimizer.state_dict(),
                "loss_history": self.loss_history,
            },
            filepath,
        )

    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=device)
        self.policy.load_state_dict(checkpoint["policy_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.loss_history = checkpoint["loss_history"]


def generate_traditional_signals(data: pd.DataFrame) -> pd.DataFrame:
    """ì „í†µ í€€íŠ¸ ì „ëµ ì‹ í˜¸ ìƒì„± (ìŠ¤ìœ™ ì „ëµ ê¸°ë°˜)"""
    logger.info("ğŸ”„ ì „í†µ í€€íŠ¸ ì „ëµ ì‹ í˜¸ ìƒì„± ì¤‘...")

    signals = data.copy()

    # RSI ê¸°ë°˜ ì‹ í˜¸
    signals["rsi_signal"] = 0
    signals.loc[signals["rsi"] < 30, "rsi_signal"] = 1  # ê³¼ë§¤ë„
    signals.loc[signals["rsi"] > 70, "rsi_signal"] = -1  # ê³¼ë§¤ìˆ˜

    # MACD ê¸°ë°˜ ì‹ í˜¸
    signals["macd_signal"] = 0
    signals.loc[signals["macd"] > signals["macd_signal"], "macd_signal"] = 1
    signals.loc[signals["macd"] < signals["macd_signal"], "macd_signal"] = -1

    # ë³¼ë¦°ì € ë°´ë“œ ê¸°ë°˜ ì‹ í˜¸
    signals["bb_signal"] = 0
    signals.loc[signals["close"] < signals["bb_lower"], "bb_signal"] = 1  # í•˜ë‹¨ ëŒíŒŒ
    signals.loc[signals["close"] > signals["bb_upper"], "bb_signal"] = -1  # ìƒë‹¨ ëŒíŒŒ

    # ì´ë™í‰ê·  ê¸°ë°˜ ì‹ í˜¸
    signals["ma_signal"] = 0
    signals.loc[signals["ema_short"] > signals["ema_long"], "ma_signal"] = 1
    signals.loc[signals["ema_short"] < signals["ema_long"], "ma_signal"] = -1

    # ì¢…í•© ì‹ í˜¸ ìƒì„±
    signals["signal"] = (
        signals["rsi_signal"] * 0.2
        + signals["macd_signal"] * 0.3
        + signals["bb_signal"] * 0.2
        + signals["ma_signal"] * 0.3
    )

    # ì‹ í˜¸ ìŠ¤ë¬´ë”© (ì´ë™í‰ê· )
    signals["signal"] = signals["signal"].rolling(window=5, min_periods=1).mean()

    # í¬ì§€ì…˜ ì‹ í˜¸ (-1: ìˆ, 0: ì¤‘ë¦½, 1: ë¡±)
    signals["position"] = np.where(
        signals["signal"] > 0.1, 1, np.where(signals["signal"] < -0.1, -1, 0)
    )

    # ì‹ ë¢°ë„ ì ìˆ˜ (0~1)
    signals["confidence"] = abs(signals["signal"])

    # ê°€ì¤‘ì¹˜ (í¬ì§€ì…˜ í¬ê¸°)
    signals["weight"] = signals["signal"] * signals["confidence"]

    # ì¢…í•© ì ìˆ˜
    signals["score"] = signals["signal"] * signals["confidence"]

    logger.info("âœ… ì „í†µ í€€íŠ¸ ì „ëµ ì‹ í˜¸ ìƒì„± ì™„ë£Œ")
    return signals


def train_hybrid_rl_agent(
    data: pd.DataFrame,
    traditional_signals: pd.DataFrame,
    episodes: int = 1000,
    model_save_path: str = "models/hybrid_rl_model.pth",
    action_type: str = "continuous",
) -> Tuple[HybridPPOTrainer, List[float]]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í›ˆë ¨"""

    # í™˜ê²½ ì´ˆê¸°í™”
    env = HybridTradingEnvironment(data, traditional_signals, action_type=action_type)
    state = env.reset()
    state_size = len(state)

    # PPO í›ˆë ¨ê¸° ì´ˆê¸°í™”
    trainer = HybridPPOTrainer(
        state_size,
        env.action_space.shape[0] if action_type == "continuous" else 3,
        action_type=action_type,
    )

    episode_rewards = []

    logger.info(f"ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ PPO í›ˆë ¨ ì‹œì‘: {episodes}ê°œ ì—í”¼ì†Œë“œ")
    logger.info(f"ğŸ“Š ìƒíƒœ í¬ê¸°: {state_size}, í–‰ë™ íƒ€ì…: {action_type}")

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        # ì—í”¼ì†Œë“œ ë°ì´í„° ìˆ˜ì§‘
        states, actions, log_probs, rewards, values, dones = [], [], [], [], [], []

        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            action, log_prob, value = trainer.policy.get_action(
                state_tensor, training=True
            )

            # í–‰ë™ ì‹¤í–‰
            if action_type == "discrete":
                action_np = action.item()
            else:
                action_np = action.squeeze().detach().cpu().numpy()

            next_state, reward, done, info = env.step(action_np)

            # ë°ì´í„° ì €ì¥
            states.append(state)
            actions.append(action_np)
            log_probs.append(log_prob.item())
            rewards.append(reward)
            values.append(value.item())
            dones.append(done)

            state = next_state
            total_reward += reward

        # GAE ê³„ì‚°
        advantages = trainer.compute_gae(rewards, values, dones)
        returns = [r + g for r, g in zip(rewards, advantages)]

        # ì •ì±… ì—…ë°ì´íŠ¸
        if len(states) > 0:
            states_tensor = torch.FloatTensor(states).to(device)
            actions_tensor = torch.FloatTensor(actions).to(device)
            log_probs_tensor = torch.FloatTensor(log_probs).to(device)
            advantages_tensor = torch.FloatTensor(advantages).to(device)
            returns_tensor = torch.FloatTensor(returns).to(device)

            trainer.update_policy(
                states_tensor,
                actions_tensor,
                log_probs_tensor,
                advantages_tensor,
                returns_tensor,
            )

        episode_rewards.append(total_reward)

        if episode % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            logger.info(f"ì—í”¼ì†Œë“œ {episode}: í‰ê·  ë³´ìƒ {avg_reward:.4f}")

    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    trainer.save_model(model_save_path)
    logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ PPO ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")

    return trainer, episode_rewards


class HybridRLTradingStrategy(BaseStrategy):
    """í•˜ì´ë¸Œë¦¬ë“œ ê°•í™”í•™ìŠµ ê¸°ë°˜ íŠ¸ë ˆì´ë”© ì „ëµ"""

    def __init__(
        self,
        params: StrategyParams,
        model_path: Optional[str] = None,
        action_type: str = "continuous",
    ):
        super().__init__(params)
        self.strategy_type = "hybrid_single_asset"
        self.model_path = model_path
        self.action_type = action_type
        self.trainer = None
        self.env = None

    def generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:
        """í•˜ì´ë¸Œë¦¬ë“œ ê°•í™”í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë§¤ë§¤ ì‹ í˜¸ ìƒì„±"""
        if self.trainer is None or self.model_path is None:
            logger.warning(
                "í•˜ì´ë¸Œë¦¬ë“œ ê°•í™”í•™ìŠµ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „í†µ ì‹ í˜¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            traditional_signals = generate_traditional_signals(df)
            return traditional_signals

        # ì „í†µ ì‹ í˜¸ ìƒì„±
        traditional_signals = generate_traditional_signals(df)

        # í™˜ê²½ ì´ˆê¸°í™”
        self.env = HybridTradingEnvironment(
            df, traditional_signals, action_type=self.action_type
        )
        state = self.env.reset()

        signals = []
        done = False

        while not done:
            # í–‰ë™ ì„ íƒ (inference ëª¨ë“œ)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            action, _, _ = self.trainer.policy.get_action(state_tensor, training=False)

            # ì‹ í˜¸ ë³€í™˜
            if self.action_type == "discrete":
                signal_mapping = {0: -1, 1: 0, 2: 1}
                signal = signal_mapping[action.item()]
            else:
                signal = (
                    action.squeeze().cpu().numpy()[0]
                )  # ì²« ë²ˆì§¸ ì°¨ì› (ì „í†µ ì‹ í˜¸ ê°€ì¤‘ì¹˜)

            signals.append(signal)

            # ë‹¤ìŒ ìŠ¤í…
            state, reward, done, info = self.env.step(
                action.item()
                if self.action_type == "discrete"
                else action.squeeze().cpu().numpy()
            )

        # ì‹ í˜¸ DataFrame ìƒì„±
        result_df = df.copy()
        if len(signals) < len(df):
            signals.extend([0] * (len(df) - len(signals)))
        elif len(signals) > len(df):
            signals = signals[: len(df)]

        result_df["hybrid_signal"] = signals
        result_df["traditional_signal"] = traditional_signals["signal"]

        return result_df

    def load_trained_model(self, model_path: str):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

        # ë”ë¯¸ í™˜ê²½ìœ¼ë¡œ state_size ê³„ì‚°
        dummy_data = pd.DataFrame(
            {
                "open": [100],
                "high": [102],
                "low": [98],
                "close": [101],
                "volume": [1000000],
            }
        )
        dummy_signals = pd.DataFrame(
            {
                "signal": [0],
                "position": [0],
                "weight": [0],
                "confidence": [0],
                "score": [0],
            }
        )
        dummy_env = HybridTradingEnvironment(
            dummy_data, dummy_signals, action_type=self.action_type
        )
        state_size = len(dummy_env._get_state())

        # í›ˆë ¨ê¸° ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë“œ
        action_size = 3 if self.action_type == "discrete" else 2
        self.trainer = HybridPPOTrainer(
            state_size, action_size, action_type=self.action_type
        )
        self.trainer.load_model(model_path)

        self.model_path = model_path
        logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê°•í™”í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
