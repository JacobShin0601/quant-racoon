#!/usr/bin/env python3
"""
Random Forest ê¸°ë°˜ ì‹œì¥ ìƒíƒœ ë¶„ë¥˜ê¸°
ì‹¤ì œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹œì¥ ìƒíƒœ í™•ë¥ ì„ ì˜ˆì¸¡
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import logging
import json
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))

from actions.y_finance import YahooFinanceDataCollector
from actions.calculate_index import TechnicalIndicators


class MarketRegimeRF:
    """Random Forest ê¸°ë°˜ ì‹œì¥ ìƒíƒœ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, verbose: bool = True, config_path: str = "config/config_macro.json"):
        """
        MarketRegimeRF ì´ˆê¸°í™”
        
        Args:
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.verbose = verbose
        self.logger = logging.getLogger(__name__)
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        self.config = self._load_config(config_path)
        
        # ëª¨ë¸ ê´€ë ¨ ë³€ìˆ˜ë“¤
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.is_trained = False
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸°
        self.data_collector = YahooFinanceDataCollector()
        self.tech_indicators = TechnicalIndicators()
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        self.model_dir = Path("models/market_regime")
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹œì¥ ìƒíƒœ ë§¤í•‘
        self.regime_mapping = {
            0: 'TRENDING_UP',
            1: 'TRENDING_DOWN', 
            2: 'VOLATILE',
            3: 'SIDEWAYS'
        }
        
        self.regime_mapping_reverse = {v: k for k, v in self.regime_mapping.items()}
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            self._print(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
            return config
        except Exception as e:
            self._print(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}", level="error")
            return {}
    
    def _get_days_back(self, collection_type: str = "default") -> int:
        """ì„¤ì •ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ê°€ì ¸ì˜¤ê¸°"""
        try:
            data_collection = self.config.get('data_collection', {})
            days_back = data_collection.get(f'{collection_type}_days_back', 
                                          data_collection.get('default_days_back', 730))
            return days_back
        except Exception as e:
            self._print(f"ì„¤ì •ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ë¡œë“œ ì‹¤íŒ¨: {e}", level="error")
            return 730  # ê¸°ë³¸ê°’ 2ë…„
    
    def _print(self, *args, level="info", **kwargs):
        """ë¡œê·¸ ì¶œë ¥"""
        if self.verbose:
            if level == "info":
                self.logger.info(*args, **kwargs)
            elif level == "warning":
                self.logger.warning(*args, **kwargs)
            elif level == "error":
                self.logger.error(*args, **kwargs)
            else:
                print(*args, **kwargs)
    
    def collect_training_data(self, start_date: str = None, end_date: str = None, data_dir: str = "data/macro") -> pd.DataFrame:
        """
        í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ (ì €ì¥ëœ ë°ì´í„° ì‚¬ìš©)
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ (Noneì´ë©´ ì„¤ì •ì—ì„œ ê³„ì‚°)
            end_date: ì¢…ë£Œ ë‚ ì§œ (Noneì´ë©´ í˜„ì¬)
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            í•™ìŠµìš© ë°ì´í„°í”„ë ˆì„
        """
        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')
        
        if start_date is None:
            # ì„¤ì •ì—ì„œ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ê°€ì ¸ì˜¤ê¸°
            days_back = self._get_days_back("model_training")
            start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
            self._print(f"ì„¤ì • ê¸°ë°˜ ì‹œì‘ ë‚ ì§œ ì„¤ì •: {start_date} ({days_back}ì¼)")
        
        self._print(f"ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘: {start_date} ~ {end_date}")
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not os.path.isabs(data_dir):
            # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ í•´ê²°
            current_dir = os.getcwd()
            data_dir = os.path.join(current_dir, data_dir)
        
        self._print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
        
        # SPY ë°ì´í„° ë¡œë“œ
        spy_path = os.path.join(data_dir, "spy_data.csv")
        if not os.path.exists(spy_path):
            raise ValueError(f"SPY ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {spy_path}")
        
        spy_data = pd.read_csv(spy_path, index_col=0, parse_dates=False)
        
        self._print(f"ì›ë³¸ SPY ë°ì´í„° í¬ê¸°: {len(spy_data)}ê°œ")
        self._print(f"SPY ë°ì´í„° ì»¬ëŸ¼: {list(spy_data.columns)}")
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™”
        spy_data.columns = spy_data.columns.str.lower()
        
        # ë‚ ì§œ í•„í„°ë§
        if 'datetime' in spy_data.columns:
            self._print("datetime ì»¬ëŸ¼ ë°œê²¬, ë‚ ì§œ í•„í„°ë§ ì‹œì‘")
            try:
                # ì•ˆì „í•œ datetime ë³€í™˜ (íƒ€ì„ì¡´ ì²˜ë¦¬ í¬í•¨)
                spy_data['datetime'] = pd.to_datetime(spy_data['datetime'], utc=True, errors='coerce')
                # UTCì—ì„œ ë¡œì»¬ë¡œ ë³€í™˜í•˜ê³  íƒ€ì„ì¡´ ì •ë³´ ì œê±°
                spy_data['datetime'] = spy_data['datetime'].dt.tz_convert('UTC').dt.tz_localize(None)
                # NaN ê°’ ì œê±°
                spy_data = spy_data.dropna(subset=['datetime'])
                
                # datetime íƒ€ì… í™•ì¸
                if pd.api.types.is_datetime64_any_dtype(spy_data['datetime']):
                    
                    start_dt = pd.to_datetime(start_date)
                    end_dt = pd.to_datetime(end_date)
                    
                    self._print(f"í•„í„°ë§ ì „ ë°ì´í„° í¬ê¸°: {len(spy_data)}ê°œ")
                    self._print(f"ì‹œì‘ ë‚ ì§œ: {start_dt}, ì¢…ë£Œ ë‚ ì§œ: {end_dt}")
                    self._print(f"ë°ì´í„° ë‚ ì§œ ë²”ìœ„: {spy_data['datetime'].min()} ~ {spy_data['datetime'].max()}")
                    
                    spy_data = spy_data[(spy_data['datetime'] >= start_dt) & (spy_data['datetime'] <= end_dt)]
                    self._print(f"í•„í„°ë§ í›„ ë°ì´í„° í¬ê¸°: {len(spy_data)}ê°œ")
                    
                    spy_data.set_index('datetime', inplace=True)
                    self._print(f"ë‚ ì§œ í•„í„°ë§ ì™„ë£Œ: {start_date} ~ {end_date}")
                else:
                    self._print("datetime ì»¬ëŸ¼ì´ datetime íƒ€ì…ì´ ì•„ë‹˜, ì „ì²´ ë°ì´í„° ì‚¬ìš©")
            except Exception as e:
                self._print(f"ë‚ ì§œ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}", level="error")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ë°ì´í„° ì‚¬ìš©
                self._print("ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨ë¡œ ì „ì²´ ë°ì´í„° ì‚¬ìš©")
        else:
            # ì¸ë±ìŠ¤ê°€ ìˆ«ìì¸ ê²½ìš°, ì „ì²´ ë°ì´í„° ì‚¬ìš©
            self._print("ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë°ì´í„° ì‚¬ìš©")
        
        if spy_data.empty:
            raise ValueError("í•„í„°ë§ëœ SPY ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ í™•ì¸
        if len(spy_data) < 50:
            raise ValueError(f"ë°ì´í„° í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(spy_data)}ê°œ (ìµœì†Œ 50ê°œ í•„ìš”)")
        
        self._print(f"SPY ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy_data)}ê°œ")
        
        # ë§¤í¬ë¡œ ë°ì´í„° ë¡œë“œ
        macro_symbols = ['^VIX', '^TNX', '^TYX', '^DXY', 'GC=F', '^TLT', '^TIP']
        macro_data = {}
        
        for symbol in macro_symbols:
            # íŒŒì¼ëª… ë³€í™˜ (íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
            filename = symbol.lower().replace('^', '').replace('=', '') + '_data.csv'
            macro_path = os.path.join(data_dir, filename)
            if os.path.exists(macro_path):
                try:
                    data = pd.read_csv(macro_path, index_col=0, parse_dates=False)
                    if 'datetime' in data.columns:
                        data['datetime'] = pd.to_datetime(data['datetime'], utc=True)
                        data.set_index('datetime', inplace=True)
                    
                    # ë‚ ì§œ í•„í„°ë§
                    try:
                        if 'datetime' in data.columns:
                            # ì•ˆì „í•œ datetime ë³€í™˜
                            data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')
                            data = data.dropna(subset=['datetime'])
                            
                            # íƒ€ì„ì¡´ ì •ë³´ê°€ ìˆìœ¼ë©´ ì œê±°
                            if data['datetime'].dt.tz is not None:
                                data['datetime'] = data['datetime'].dt.tz_localize(None)
                            
                            start_dt = pd.to_datetime(start_date)
                            end_dt = pd.to_datetime(end_date)
                            data = data[(data['datetime'] >= start_dt) & (data['datetime'] <= end_dt)]
                            data.set_index('datetime', inplace=True)
                        else:
                            # ì¸ë±ìŠ¤ê°€ ë‚ ì§œì¸ ê²½ìš°
                            data.index = pd.to_datetime(data.index, errors='coerce')
                            data = data.dropna()
                            
                            if data.index.tz is not None:
                                data.index = data.index.tz_localize(None)
                            
                            start_dt = pd.to_datetime(start_date)
                            end_dt = pd.to_datetime(end_date)
                            data = data[(data.index >= start_dt) & (data.index <= end_dt)]
                    except Exception as e:
                        self._print(f"{symbol} ë‚ ì§œ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}", level="warning")
                        continue
                    
                    if not data.empty:
                        macro_data[symbol] = data
                        self._print(f"  {symbol} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ")
                except Exception as e:
                    self._print(f"  {symbol} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}", level="warning")
        
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        from actions.calculate_index import StrategyParams
        
        default_params = StrategyParams(
            atr_period=14,
            ema_short=20,
            ema_long=50,
            rsi_period=14,
            macd_fast=12,
            macd_slow=26,
            macd_signal=9,
            bb_period=20,
            bb_std=2.0,
            stoch_k_period=14,
            stoch_d_period=3,
            williams_r_period=14,
            cci_period=20,
            adx_period=14,
            obv_smooth_period=20,
            donchian_period=20,
            keltner_period=20,
            keltner_multiplier=2.0,
            volatility_period=20
        )
        self._print(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œì‘ (ë°ì´í„° í¬ê¸°: {len(spy_data)}ê°œ)")
        tech_data = self.tech_indicators.calculate_all_indicators(spy_data, default_params)
        self._print(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ (ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ)")
        
        # ë§¤í¬ë¡œ ë°ì´í„° ë³‘í•©
        self._print(f"ë§¤í¬ë¡œ ë°ì´í„° ë³‘í•© ì‹œì‘ (ë§¤í¬ë¡œ ë°ì´í„° ê°œìˆ˜: {len(macro_data)}ê°œ)")
        for symbol, data in macro_data.items():
            if 'close' in data.columns:
                tech_data[f'{symbol}_close'] = data['close']
            elif 'Close' in data.columns:
                tech_data[f'{symbol}_close'] = data['Close']
        
        self._print(f"ë§¤í¬ë¡œ ë°ì´í„° ë³‘í•© ì™„ë£Œ (ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ)")
        
        # ì¶”ê°€ íŠ¹ì„± ìƒì„±
        self._print("ê³ ê¸‰ íŠ¹ì„± ìƒì„± ì‹œì‘")
        tech_data = self._create_advanced_features(tech_data)
        self._print(f"ê³ ê¸‰ íŠ¹ì„± ìƒì„± ì™„ë£Œ (ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ)")
        
        # NaN ê°’ ì²˜ë¦¬ (ëª¨ë“  ì»¬ëŸ¼ì´ NaNì¸ í–‰ë§Œ ì œê±°)
        self._print(f"NaN ê°’ ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ")
        # NaN ë¹„ìœ¨ í™•ì¸
        nan_ratio = tech_data.isnull().sum() / len(tech_data)
        self._print(f"NaN ë¹„ìœ¨ì´ ë†’ì€ ì»¬ëŸ¼ë“¤: {nan_ratio[nan_ratio > 0.5].index.tolist()}")
        
        # ëª¨ë“  ì»¬ëŸ¼ì´ NaNì¸ í–‰ë§Œ ì œê±°
        tech_data = tech_data.dropna(how='all')
        self._print(f"ëª¨ë“  ì»¬ëŸ¼ì´ NaNì¸ í–‰ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ")
        
        # ë‚˜ë¨¸ì§€ NaN ê°’ì€ 0ìœ¼ë¡œ ì±„ì›€
        tech_data = tech_data.fillna(0)
        self._print(f"NaN ê°’ì„ 0ìœ¼ë¡œ ì±„ìš´ í›„ ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ")
        
        # ë¼ë²¨ ìƒì„±
        self._print("ë¼ë²¨ ìƒì„± ì‹œì‘")
        tech_data = self._create_labels(tech_data)
        
        self._print(f"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(tech_data)}ê°œ ìƒ˜í”Œ")
        return tech_data
    
    def _create_advanced_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ íŠ¹ì„± ìƒì„±"""
        # ì»¬ëŸ¼ëª… ë§¤í•‘
        close_col = 'close' if 'close' in data.columns else 'Close'
        
        # ê¸°ë³¸ ê°€ê²© íŠ¹ì„±ë“¤
        data['returns_1d'] = data[close_col].pct_change()
        data['returns_5d'] = data[close_col].pct_change(5)
        data['returns_20d'] = data[close_col].pct_change(20)
        data['volatility_20d'] = data['returns_1d'].rolling(20).std()
        # ë°ì´í„° í¬ê¸°ì— ë§ê²Œ rolling window ì¡°ì •
        max_window = min(60, len(data) // 2)
        data['volatility_60d'] = data['returns_1d'].rolling(max_window).std()
        
        # ì´ë™í‰ê·  ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'sma_20' in data.columns and 'sma_50' in data.columns:
            data['sma_ratio'] = data['sma_20'] / data['sma_50']
            data['price_sma20_ratio'] = data[close_col] / data['sma_20']
            data['price_sma50_ratio'] = data[close_col] / data['sma_50']
        
        # RSI ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'rsi' in data.columns:
            data['rsi_ma'] = data['rsi'].rolling(14).mean()
            data['rsi_std'] = data['rsi'].rolling(14).std()
            data['rsi_zscore'] = (data['rsi'] - data['rsi_ma']) / data['rsi_std']
        
        # MACD ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'macd' in data.columns and 'macd_signal' in data.columns:
            data['macd_histogram'] = data['macd'] - data['macd_signal']
            data['macd_ratio'] = data['macd'] / (data['macd_signal'] + 1e-8)
        
        # ë³¼ë¦°ì € ë°´ë“œ ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'bb_upper' in data.columns and 'bb_lower' in data.columns:
            data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data[close_col]
            data['bb_position'] = (data[close_col] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'])
        
        # ATR ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'atr' in data.columns:
            data['atr_ratio'] = data['atr'] / data[close_col]
            data['atr_ma'] = data['atr'].rolling(14).mean()
            data['atr_ratio_ma'] = data['atr_ratio'].rolling(14).mean()
        
        # ê±°ë˜ëŸ‰ ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'volume' in data.columns:
            data['volume_ma'] = data['volume'].rolling(20).mean()
            data['volume_ratio'] = data['volume'] / data['volume_ma']
            data['volume_price_trend'] = (data['volume'] * data['returns_1d']).rolling(20).sum()
        
        # ë§¤í¬ë¡œ íŠ¹ì„±ë“¤
        macro_features = ['^VIX_close', '^TNX_close', '^TYX_close', '^DXY_close', 'GC=F_close', '^TLT_close', '^TIP_close']
        
        for feature in macro_features:
            if feature in data.columns:
                # ë§¤í¬ë¡œ ì§€í‘œì˜ ë³€í™”ìœ¨
                data[f'{feature}_change'] = data[feature].pct_change()
                data[f'{feature}_ma'] = data[feature].rolling(20).mean()
                data[f'{feature}_ratio'] = data[feature] / data[f'{feature}_ma']
                
                # VIX íŠ¹ë³„ ì²˜ë¦¬
                if feature == '^VIX_close':
                    data['vix_volatility'] = data[feature].rolling(20).std()
                    data['vix_percentile'] = data[feature].rolling(252).rank(pct=True)
                
                # ê¸ˆë¦¬ ê´€ë ¨ íŠ¹ì„±ë“¤
                if feature in ['^TNX_close', '^TYX_close']:
                    data[f'{feature}_spread'] = data[feature] - data['^TNX_close'] if feature == '^TYX_close' else 0
        
        # ë³µí•© íŠ¹ì„±ë“¤
        if 'rsi' in data.columns and 'volatility_20d' in data.columns:
            data['rsi_volatility'] = data['rsi'] * data['volatility_20d']
        
        if 'macd' in data.columns and 'volume_ratio' in data.columns:
            data['macd_volume'] = data['macd'] * data['volume_ratio']
        
        if '^VIX_close' in data.columns and 'returns_1d' in data.columns:
            data['vix_return_correlation'] = data['^VIX_close'].rolling(20).corr(data['returns_1d'])
        
        return data
    
    def _create_labels(self, data: pd.DataFrame, params: Dict[str, Any] = None) -> pd.DataFrame:
        """ì‹œì¥ ìƒíƒœ ë¼ë²¨ ìƒì„± - ì™„ì „íˆ ìƒˆë¡œìš´ Quant ê¸°ë°˜ ë¡œì§"""
        self._print("ìƒˆë¡œìš´ Quant ê¸°ë°˜ ë¼ë²¨ ìƒì„± ì‹œì‘")
        
        # ì»¬ëŸ¼ëª… ë§¤í•‘
        close_col = 'close' if 'close' in data.columns else 'Close'
        
        # 1. í•µì‹¬ ì§€í‘œ ê³„ì‚°
        # ìˆ˜ìµë¥  ê³„ì‚° (ë‹¤ì–‘í•œ ê¸°ê°„)
        returns_1d = data[close_col].pct_change()
        returns_5d = data[close_col].pct_change(5)
        returns_20d = data[close_col].pct_change(20)
        
        # ë³€ë™ì„± ê³„ì‚°
        volatility_20d = returns_1d.rolling(20).std()
        
        # 2. íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚° (ADX ê¸°ë°˜)
        trend_strength = np.zeros(len(data))
        if 'adx' in data.columns:
            adx = data['adx']
            trend_strength = np.where(~adx.isna(), adx / 100.0, 0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        
        # 3. ì´ë™í‰ê·  ê¸°ë°˜ íŠ¸ë Œë“œ ë°©í–¥
        trend_direction = np.zeros(len(data))
        if 'sma_20' in data.columns and 'sma_50' in data.columns:
            sma_20 = data['sma_20']
            sma_50 = data['sma_50']
            price = data[close_col]
            valid_mask = ~(sma_20.isna() | sma_50.isna())
            
            # íŠ¸ë Œë“œ ë°©í–¥ (-1: í•˜ë½, 0: ì¤‘ë¦½, 1: ìƒìŠ¹)
            trend_direction = np.where(valid_mask,
                np.where((price > sma_20) & (sma_20 > sma_50), 1,  # ê°•í•œ ìƒìŠ¹
                np.where((price < sma_20) & (sma_20 < sma_50), -1,  # ê°•í•œ í•˜ë½
                0)),  # ì¤‘ë¦½
                0
            )
        
        # 4. RSI ê¸°ë°˜ ëª¨ë©˜í…€
        momentum = np.zeros(len(data))
        if 'rsi' in data.columns:
            rsi = data['rsi']
            momentum = np.where(~rsi.isna(),
                np.where(rsi > 70, -0.5,  # ê³¼ë§¤ìˆ˜
                np.where(rsi < 30, 0.5,   # ê³¼ë§¤ë„
                np.where(rsi > 60, -0.2,  # ì•½í•œ ê³¼ë§¤ìˆ˜
                np.where(rsi < 40, 0.2,   # ì•½í•œ ê³¼ë§¤ë„
                0)))),  # ì¤‘ë¦½
                0
            )
        
        # 5. ë³€ë™ì„± ë ˆë²¨
        volatility_level = np.zeros(len(data))
        if 'atr_ratio' in data.columns:
            atr_ratio = data['atr_ratio']
            volatility_level = np.where(~atr_ratio.isna(),
                np.where(atr_ratio > 0.03, 1.0,    # ë†’ì€ ë³€ë™ì„±
                np.where(atr_ratio > 0.02, 0.5,    # ì¤‘ê°„ ë³€ë™ì„±
                0)),  # ë‚®ì€ ë³€ë™ì„±
                0
            )
        
        # 6. ê±°ë˜ëŸ‰ ì‹ í˜¸
        volume_signal = np.zeros(len(data))
        if 'volume_ratio' in data.columns:
            volume_ratio = data['volume_ratio']
            volume_signal = np.where(~volume_ratio.isna(),
                np.where(volume_ratio > 1.5, 0.3,   # ë†’ì€ ê±°ë˜ëŸ‰
                np.where(volume_ratio < 0.5, -0.3,  # ë‚®ì€ ê±°ë˜ëŸ‰
                0)),  # ì¤‘ê°„ ê±°ë˜ëŸ‰
                0
            )
        
        # 7. ìƒˆë¡œìš´ ë¼ë²¨ë§ ë¡œì§ (í†µê³„ì  ë¶„ìœ„ìˆ˜ ê¸°ë°˜)
        labels = np.zeros(len(data), dtype=int)
        
        for i in range(len(data)):
            # ê° ì§€í‘œë³„ ì ìˆ˜ ê³„ì‚°
            trend_score = trend_direction[i] * trend_strength[i]
            momentum_score = momentum[i]
            volatility_score = volatility_level[i]
            volume_score = volume_signal[i]
            
            # ìµœê·¼ ìˆ˜ìµë¥  ê¸°ë°˜ ì ìˆ˜
            recent_return_score = 0
            if i >= 20:
                recent_return = returns_20d.iloc[i]
                if not pd.isna(recent_return):
                    if recent_return > 0.05:  # 5% ì´ìƒ ìƒìŠ¹
                        recent_return_score = 0.5
                    elif recent_return < -0.05:  # 5% ì´ìƒ í•˜ë½
                        recent_return_score = -0.5
                    elif recent_return > 0.02:  # 2% ì´ìƒ ìƒìŠ¹
                        recent_return_score = 0.2
                    elif recent_return < -0.02:  # 2% ì´ìƒ í•˜ë½
                        recent_return_score = -0.2
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            total_score = (trend_score * 0.3 + 
                          momentum_score * 0.2 + 
                          recent_return_score * 0.3 + 
                          volume_score * 0.1 + 
                          volatility_score * 0.1)
            
            # ë¼ë²¨ ë¶„ë¥˜ (í†µê³„ì  ë¶„ìœ„ìˆ˜ ê¸°ë°˜)
            if i >= 50:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                # ì „ì²´ ì ìˆ˜ ë¶„í¬ì˜ ë¶„ìœ„ìˆ˜ ê³„ì‚°
                all_scores = []
                for j in range(max(0, i-50), i+1):
                    if j < len(data):
                        trend_s = trend_direction[j] * trend_strength[j]
                        momentum_s = momentum[j]
                        vol_s = volume_signal[j]
                        vol_level_s = volatility_level[j]
                        
                        recent_ret_s = 0
                        if j >= 20:
                            recent_ret = returns_20d.iloc[j]
                            if not pd.isna(recent_ret):
                                if recent_ret > 0.05:
                                    recent_ret_s = 0.5
                                elif recent_ret < -0.05:
                                    recent_ret_s = -0.5
                                elif recent_ret > 0.02:
                                    recent_ret_s = 0.2
                                elif recent_ret < -0.02:
                                    recent_ret_s = -0.2
                        
                        score = (trend_s * 0.3 + momentum_s * 0.2 + recent_ret_s * 0.3 + 
                                vol_s * 0.1 + vol_level_s * 0.1)
                        all_scores.append(score)
                
                if len(all_scores) > 0:
                    all_scores = np.array(all_scores)
                    q25 = np.percentile(all_scores, 25)
                    q75 = np.percentile(all_scores, 75)
                    
                    # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë¼ë²¨ë§ (VOLATILE ìš°ì„ ìˆœìœ„ ë†’ì„)
                    if volatility_score > 0.3:  # ë³€ë™ì„± ì„ê³„ê°’ ë‚®ì¶¤
                        labels[i] = 2  # VOLATILE (ë†’ì€ ë³€ë™ì„± ìš°ì„ )
                    elif total_score > q75:
                        labels[i] = 0  # TRENDING_UP (ìƒìœ„ 25%)
                    elif total_score < q25:
                        labels[i] = 1  # TRENDING_DOWN (í•˜ìœ„ 25%)
                    else:
                        labels[i] = 3  # SIDEWAYS (ì¤‘ê°„ 50%)
                else:
                    labels[i] = 3  # ê¸°ë³¸ê°’
            else:
                # ì´ˆê¸° ë°ì´í„°ëŠ” ê¸°ë³¸ ë¼ë²¨ë§ (VOLATILE ìš°ì„ ìˆœìœ„ ë†’ì„)
                if volatility_score > 0.3:  # ë³€ë™ì„± ì„ê³„ê°’ ë‚®ì¶¤
                    labels[i] = 2  # VOLATILE (ë†’ì€ ë³€ë™ì„± ìš°ì„ )
                elif total_score > 0.1:
                    labels[i] = 0  # TRENDING_UP
                elif total_score < -0.1:
                    labels[i] = 1  # TRENDING_DOWN
                else:
                    labels[i] = 3  # SIDEWAYS
        
        data['regime_label'] = labels
        
        # ì ìˆ˜ ì €ì¥ (ë””ë²„ê¹…ìš©)
        total_scores = []
        for i in range(len(data)):
            trend_score = trend_direction[i] * trend_strength[i]
            momentum_score = momentum[i]
            volatility_score = volatility_level[i]
            volume_score = volume_signal[i]
            
            recent_return_score = 0
            if i >= 20:
                recent_return = returns_20d.iloc[i]
                if not pd.isna(recent_return):
                    if recent_return > 0.05:
                        recent_return_score = 0.5
                    elif recent_return < -0.05:
                        recent_return_score = -0.5
                    elif recent_return > 0.02:
                        recent_return_score = 0.2
                    elif recent_return < -0.02:
                        recent_return_score = -0.2
            
            total_score = (trend_score * 0.3 + 
                          momentum_score * 0.2 + 
                          recent_return_score * 0.3 + 
                          volume_score * 0.1 + 
                          volatility_score * 0.1)
            total_scores.append(total_score)
        
        data['regime_score'] = total_scores
        
        self._print(f"ìƒˆë¡œìš´ Quant ê¸°ë°˜ ë¼ë²¨ ìƒì„± ì™„ë£Œ: {len(labels)}ê°œ")
        
        # ë¼ë²¨ ë¶„í¬ í™•ì¸
        label_counts = pd.Series(labels).value_counts().sort_index()
        self._print(f"ë¼ë²¨ ë¶„í¬: {dict(label_counts)}")
        
        # ë¼ë²¨ ë¶ˆê· í˜• í™•ì¸
        total_samples = len(labels)
        for label, count in label_counts.items():
            percentage = count / total_samples * 100
            self._print(f"  ë¼ë²¨ {label} ({self.regime_mapping[label]}): {count}ê°œ ({percentage:.1f}%)")
        
        return data
    
    def prepare_features(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """íŠ¹ì„± ì¤€ë¹„ - ê°œì„ ëœ ë²„ì „"""
        self._print("íŠ¹ì„± ì¤€ë¹„ ì‹œì‘")
        self._print(f"ë°ì´í„° ì»¬ëŸ¼ ìˆ˜: {len(data.columns)}")
        
        # ê¸°ë³¸ íŠ¹ì„±ë“¤ (í•­ìƒ ì¡´ì¬í•´ì•¼ í•˜ëŠ” ê²ƒë“¤)
        base_features = [
            'returns_1d', 'returns_5d', 'returns_20d', 'volatility_20d'
        ]
        
        # ì„ íƒì  íŠ¹ì„±ë“¤ (ì¡´ì¬í•˜ë©´ ì‚¬ìš©)
        optional_features = [
            # ì´ë™í‰ê·  ê´€ë ¨
            'sma_20', 'sma_50', 'sma_ratio', 'price_sma20_ratio', 'price_sma50_ratio',
            
            # ê¸°ìˆ ì  ì§€í‘œë“¤
            'rsi', 'macd', 'macd_signal', 'bb_upper', 'bb_lower', 'bb_width', 'bb_position',
            'atr', 'stoch_k', 'stoch_d', 'williams_r', 'cci', 'adx', 'obv',
            
            # ê±°ë˜ëŸ‰ ê´€ë ¨
            'volume', 'volume_ratio', 'volume_price_trend',
            
            # ë§¤í¬ë¡œ ì§€í‘œë“¤ (ë‹¤ì–‘í•œ ëª…ëª… ê·œì¹™ ì§€ì›)
            'vix_close', '^vix_close', 'vix_close_change', 'vix_close_ratio', 'vix_volatility',
            'tnx_close', '^tnx_close', 'tnx_close_change', 'tnx_close_ratio',
            'tyx_close', '^tyx_close', 'tyx_close_change', 'tyx_close_ratio',
            'dxy_close', '^dxy_close', 'dxy_close_change', 'dxy_close_ratio',
            'gc=f_close', 'gc_close', 'gc_close_change', 'gc_close_ratio',
            'tlt_close', '^tlt_close', 'tlt_close_change', 'tlt_close_ratio',
            'tip_close', '^tip_close', 'tip_close_change', 'tip_close_ratio',
            
            # ë³µí•© íŠ¹ì„±ë“¤
            'macd_volume', 'vix_return_correlation', 'rsi_volatility'
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŠ¹ì„±ë“¤ ì°¾ê¸°
        available_base = [col for col in base_features if col in data.columns]
        available_optional = [col for col in optional_features if col in data.columns]
        
        # íŠ¹ì„± ìš°ì„ ìˆœìœ„ ì„¤ì • (ì¤‘ìš”í•œ íŠ¹ì„±ë“¤ì„ ë¨¼ì € í¬í•¨)
        priority_features = [
            'returns_1d', 'returns_5d', 'returns_20d', 'volatility_20d',
            'rsi', 'macd', 'bb_width', 'volume_ratio'
        ]
        
        # ìš°ì„ ìˆœìœ„ íŠ¹ì„±ë“¤ì„ ë¨¼ì € í¬í•¨
        selected_features = [col for col in priority_features if col in data.columns]
        
        # ë‚˜ë¨¸ì§€ íŠ¹ì„±ë“¤ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        remaining_features = [col for col in available_base + available_optional 
                            if col not in selected_features]
        selected_features.extend(remaining_features)
        
        self._print(f"ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(selected_features)}")
        self._print(f"ì„ íƒëœ íŠ¹ì„±ë“¤: {selected_features[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
        
        # íŠ¹ì„± ë°ì´í„° ì¤€ë¹„
        X = data[selected_features].copy()
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        self._print(f"íŠ¹ì„± ë°ì´í„° í¬ê¸°: {X.shape}")
        self._print(f"NaN ê°’ ë¹„ìœ¨: {X.isnull().sum().sum() / (X.shape[0] * X.shape[1]):.2%}")
        
        # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # NaN ê°’ ì²˜ë¦¬ ê°œì„ 
        # ê° íŠ¹ì„±ë³„ë¡œ ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬
        for col in X.columns:
            if X[col].isnull().sum() > 0:
                # ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ forward fill ì‚¬ìš©
                X[col] = X[col].fillna(method='ffill')
                # ë‚¨ì€ NaN ê°’ì€ 0ìœ¼ë¡œ ì±„ì›€
                X[col] = X[col].fillna(0)
        
        # ìµœì¢… ê²€ì¦
        if X.isnull().sum().sum() > 0:
            self._print("ê²½ê³ : ì—¬ì „íˆ NaN ê°’ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.", level="warning")
            X = X.fillna(0)
        
        self._print(f"ìµœì¢… íŠ¹ì„± ë°ì´í„° í¬ê¸°: {X.shape}")
        self._print(f"ìµœì¢… NaN ê°’ ìˆ˜: {X.isnull().sum().sum()}")
        
        return X, selected_features
    
    def train_model(self, data: pd.DataFrame = None, params: Dict[str, Any] = None, save_model: bool = True) -> Dict[str, Any]:
        """
        Random Forest ëª¨ë¸ í•™ìŠµ
        
        Args:
            data: í•™ìŠµ ë°ì´í„° (Noneì´ë©´ ìë™ ìˆ˜ì§‘)
            params: ì‹œì¥ ìƒíƒœ ë¶„ë¥˜ì— ì‚¬ìš©í•  íŒŒë¼ë¯¸í„° (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            save_model: ëª¨ë¸ ì €ì¥ ì—¬ë¶€
            
        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        if data is None:
            data = self.collect_training_data()
        
        # ë¼ë²¨ ìƒì„± ì‹œ íŒŒë¼ë¯¸í„° ì „ë‹¬
        data = self._create_labels(data, params)
        
        # íŠ¹ì„±ê³¼ ë¼ë²¨ ì¤€ë¹„
        X, feature_names = self.prepare_features(data)
        y = data['regime_label'].dropna()
        
        # ì¸ë±ìŠ¤ ë§ì¶”ê¸°
        common_index = X.index.intersection(y.index)
        X = X.loc[common_index]
        y = y.loc[common_index]
        
        if len(X) < 100:
            raise ValueError(f"í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(X)}ê°œ")
        
        self._print(f"ëª¨ë¸ í•™ìŠµ ì‹œì‘: {len(X)}ê°œ ìƒ˜í”Œ, {len(feature_names)}ê°œ íŠ¹ì„±")
        
        # ì‹œê°„ì  ë¶„í•  (ê³¼ê±° 70%ë¡œ í•™ìŠµ, ìµœê·¼ 30%ë¡œ í…ŒìŠ¤íŠ¸)
        split_idx = int(len(X) * 0.7)
        X_train = X.iloc[:split_idx]
        X_test = X.iloc[split_idx:]
        y_train = y.iloc[:split_idx]
        y_test = y.iloc[split_idx:]
        
        self._print(f"ì‹œê°„ì  ë¶„í• : í•™ìŠµ {len(X_train)}ê°œ, í…ŒìŠ¤íŠ¸ {len(X_test)}ê°œ")
        self._print(f"í•™ìŠµ ê¸°ê°„: {X_train.index[0]} ~ {X_train.index[-1]}")
        self._print(f"í…ŒìŠ¤íŠ¸ ê¸°ê°„: {X_test.index[0]} ~ {X_test.index[-1]}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¼ë²¨ ë¶„í¬ í™•ì¸
        test_label_counts = y_test.value_counts().sort_index()
        self._print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬: {dict(test_label_counts)}")
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Random Forest ëª¨ë¸ ìƒì„± ë° í•™ìŠµ (ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
        self.model = RandomForestClassifier(
            n_estimators=200,  # íŠ¸ë¦¬ ìˆ˜ ì¦ê°€
            max_depth=15,      # ê¹Šì´ ì¦ê°€
            min_samples_split=10,
            min_samples_leaf=5,
            max_features='sqrt',  # íŠ¹ì„± ìˆ˜ ì œí•œ
            bootstrap=True,
            oob_score=True,    # Out-of-bag ì ìˆ˜ í™œì„±í™”
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
        )
        
        self._print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        self.model.fit(X_train_scaled, y_train)
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        train_score = self.model.score(X_train_scaled, y_train)
        test_score = self.model.score(X_test_scaled, y_test)
        oob_score = self.model.oob_score_ if hasattr(self.model, 'oob_score_') else None
        
        # êµì°¨ ê²€ì¦
        cv_scores = cross_val_score(self.model, X_train_scaled, y_train, cv=5, scoring='accuracy')
        
        # ì˜ˆì¸¡ ë° ìƒì„¸ í‰ê°€
        y_train_pred = self.model.predict(X_train_scaled)
        y_test_pred = self.model.predict(X_test_scaled)
        
        # ë¶„ë¥˜ ë³´ê³ ì„œ
        from sklearn.metrics import classification_report, confusion_matrix
        train_report = classification_report(y_train, y_train_pred, output_dict=True)
        test_report = classification_report(y_test, y_test_pred, output_dict=True)
        
        # í˜¼ë™ í–‰ë ¬
        train_cm = confusion_matrix(y_train, y_train_pred)
        test_cm = confusion_matrix(y_test, y_test_pred)
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # ê²°ê³¼ ì €ì¥
        self.feature_names = feature_names
        self.is_trained = True
        
        results = {
            'train_score': train_score,
            'test_score': test_score,
            'oob_score': oob_score,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'feature_importance': feature_importance,
            'train_report': train_report,
            'test_report': test_report,
            'train_confusion_matrix': train_cm.tolist(),
            'test_confusion_matrix': test_cm.tolist(),
            'n_samples': len(X),
            'n_features': len(feature_names),
            'class_distribution': y.value_counts().to_dict()
        }
        
        self._print(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ:")
        self._print(f"  í›ˆë ¨ ì •í™•ë„: {train_score:.4f}")
        self._print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.4f}")
        if oob_score:
            self._print(f"  OOB ì •í™•ë„: {oob_score:.4f}")
        self._print(f"  êµì°¨ ê²€ì¦: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì¶œë ¥
        self._print("í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (í…ŒìŠ¤íŠ¸):")
        for class_label in sorted(y_test.unique()):
            class_name = self.regime_mapping[class_label]
            precision = test_report[str(class_label)]['precision']
            recall = test_report[str(class_label)]['recall']
            f1 = test_report[str(class_label)]['f1-score']
            self._print(f"  {class_name}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}")
        
        # ìƒìœ„ íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
        self._print("ìƒìœ„ 10ê°œ íŠ¹ì„± ì¤‘ìš”ë„:")
        for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
            self._print(f"  {i+1}. {row['feature']}: {row['importance']:.4f}")
        
        # ëª¨ë¸ ì €ì¥ (ìë™ ì €ì¥)
        if save_model:
            self.save_model()
        
        return results
    
    def predict_probabilities(self, data: pd.DataFrame) -> Dict[str, float]:
        """
        í˜„ì¬ ì‹œì¥ ìƒíƒœ í™•ë¥  ì˜ˆì¸¡ - ê°œì„ ëœ ë²„ì „
        
        Args:
            data: ì˜ˆì¸¡í•  ë°ì´í„°
            
        Returns:
            ì‹œì¥ ìƒíƒœë³„ í™•ë¥ 
        """
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # íŠ¹ì„± ì¤€ë¹„
            X, feature_names = self.prepare_features(data)
            
            if X.empty:
                raise ValueError("ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
            latest_data = X.iloc[-1:].copy()
            
            # íŠ¹ì„± ìˆ˜ ê²€ì¦
            if len(latest_data.columns) != len(self.feature_names):
                self._print(f"ê²½ê³ : íŠ¹ì„± ìˆ˜ ë¶ˆì¼ì¹˜ (ì˜ˆì¸¡: {len(latest_data.columns)}, ëª¨ë¸: {len(self.feature_names)})", level="warning")
                
                # ëˆ„ë½ëœ íŠ¹ì„±ë“¤ì„ 0ìœ¼ë¡œ ì±„ì›€
                missing_features = set(self.feature_names) - set(latest_data.columns)
                for feature in missing_features:
                    latest_data[feature] = 0
                
                # ëª¨ë¸ì˜ íŠ¹ì„± ìˆœì„œì— ë§ì¶° ì¬ì •ë ¬
                latest_data = latest_data[self.feature_names]
            
            # ìŠ¤ì¼€ì¼ë§
            latest_scaled = self.scaler.transform(latest_data)
            
            # í™•ë¥  ì˜ˆì¸¡
            probabilities = self.model.predict_proba(latest_scaled)[0]
            
            # ê²°ê³¼ ë§¤í•‘
            result = {}
            for i, prob in enumerate(probabilities):
                if i in self.regime_mapping:
                    regime_name = self.regime_mapping[i].lower()
                    result[regime_name] = float(prob)
                else:
                    self._print(f"ê²½ê³ : ì•Œ ìˆ˜ ì—†ëŠ” ë¼ë²¨ {i}", level="warning")
            
            # í™•ë¥  í•©ê³„ ê²€ì¦
            total_prob = sum(result.values())
            if abs(total_prob - 1.0) > 0.01:
                self._print(f"ê²½ê³ : í™•ë¥  í•©ê³„ê°€ 1ì´ ì•„ë‹™ë‹ˆë‹¤: {total_prob:.4f}", level="warning")
                # ì •ê·œí™”
                for key in result:
                    result[key] /= total_prob
            
            self._print(f"ì˜ˆì¸¡ ì™„ë£Œ: {result}")
            return result
            
        except Exception as e:
            self._print(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", level="error")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            default_result = {regime.lower(): 0.25 for regime in self.regime_mapping.values()}
            return default_result
    
    def save_model(self, filepath: str = None):
        """ëª¨ë¸ ì €ì¥ - ê°œì„ ëœ ë²„ì „"""
        if filepath is None:
            filepath = self.model_dir / "market_regime_rf_model.pkl"
        
        # ëª¨ë¸ ì €ì¥ ì „ ê²€ì¦
        if not self.is_trained or self.model is None:
            raise ValueError("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'regime_mapping': self.regime_mapping,
            'trained_at': datetime.now().isoformat(),
            'model_version': '1.1',  # ë²„ì „ ì •ë³´ ì¶”ê°€
            'n_features': len(self.feature_names) if self.feature_names else 0,
            'model_type': 'RandomForestClassifier'
        }
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # ì•ˆì „í•œ ì €ì¥ (ì„ì‹œ íŒŒì¼ ì‚¬ìš©)
        import tempfile
        import shutil
        
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl')
        temp_path = temp_file.name
        temp_file.close()
        
        try:
            joblib.dump(model_data, temp_path)
            shutil.move(temp_path, filepath)
            self._print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            self._print(f"  ëª¨ë¸ ë²„ì „: {model_data['model_version']}")
            self._print(f"  íŠ¹ì„± ìˆ˜: {model_data['n_features']}")
            self._print(f"  í•™ìŠµ ì‹œê°„: {model_data['trained_at']}")
            
        except Exception as e:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            raise e
    
    def load_model(self, filepath: str = None):
        """ëª¨ë¸ ë¡œë“œ - ê°œì„ ëœ ë²„ì „"""
        if filepath is None:
            filepath = self.model_dir / "market_regime_rf_model.pkl"
        
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        
        try:
            model_data = joblib.load(filepath)
            
            # í•„ìˆ˜ í‚¤ í™•ì¸
            required_keys = ['model', 'scaler', 'feature_names', 'regime_mapping']
            for key in required_keys:
                if key not in model_data:
                    raise ValueError(f"ëª¨ë¸ íŒŒì¼ì— í•„ìˆ˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {key}")
            
            self.model = model_data['model']
            self.scaler = model_data['scaler']
            self.feature_names = model_data['feature_names']
            self.regime_mapping = model_data['regime_mapping']
            self.is_trained = True
            
            self._print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
            self._print(f"  ëª¨ë¸ ë²„ì „: {model_data.get('model_version', 'unknown')}")
            self._print(f"  íŠ¹ì„± ìˆ˜: {len(self.feature_names)}")
            self._print(f"  í•™ìŠµ ì‹œê°„: {model_data.get('trained_at', 'unknown')}")
            
            # ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
            if not hasattr(self.model, 'predict_proba'):
                raise ValueError("ë¡œë“œëœ ëª¨ë¸ì´ ì˜ˆì¸¡ ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self._print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", level="error")
            self.is_trained = False
            raise e
    
    def get_current_market_probabilities(self, data_dir: str = "data/macro") -> Dict[str, float]:
        """
        í˜„ì¬ ì‹œì¥ ìƒíƒœ í™•ë¥  ê³„ì‚° (ì €ì¥ëœ ëª¨ë¸ ìš°ì„  ì‚¬ìš©)
        
        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            í˜„ì¬ ì‹œì¥ ìƒíƒœë³„ í™•ë¥ 
        """
        # ì €ì¥ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¡œë“œ ì‹œë„
        if not self.is_trained:
            try:
                self.load_model()
                self._print("ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except FileNotFoundError:
                self._print("ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                # ëª¨ë¸ í•™ìŠµ
                self.train_model(save_model=True)
        
        # ìµœê·¼ ë°ì´í„° ë¡œë“œ
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=100)).strftime('%Y-%m-%d')
        
        data = self.collect_training_data(start_date, end_date, data_dir)
        
        # í™•ë¥  ì˜ˆì¸¡
        probabilities = self.predict_probabilities(data)
        
        return probabilities


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ê°œì„ ëœ ë²„ì „"""
    print("ğŸš€ Random Forest ì‹œì¥ ìƒíƒœ ë¶„ë¥˜ê¸° ì‹œì‘")
    
    # Random Forest ëª¨ë¸ ì´ˆê¸°í™”
    rf_model = MarketRegimeRF(verbose=True)
    
    try:
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        print("ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„ ì¤‘...")
        rf_model.load_model()
        print("âœ… ê¸°ì¡´ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        print(f"  - íŠ¹ì„± ìˆ˜: {len(rf_model.feature_names)}")
        print(f"  - ì‹œì¥ ìƒíƒœ ìˆ˜: {len(rf_model.regime_mapping)}")
        
    except (FileNotFoundError, ValueError, Exception) as e:
        print(f"âŒ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        try:
            # ëª¨ë¸ í•™ìŠµ
            print("ë°ì´í„° ìˆ˜ì§‘ ë° ëª¨ë¸ í•™ìŠµ ì¤‘...")
            results = rf_model.train_model()
            
            print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
            print(f"  - í›ˆë ¨ ì •í™•ë„: {results['train_score']:.4f}")
            print(f"  - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {results['test_score']:.4f}")
            if results.get('oob_score'):
                print(f"  - OOB ì •í™•ë„: {results['oob_score']:.4f}")
            print(f"  - êµì°¨ ê²€ì¦: {results['cv_mean']:.4f} Â± {results['cv_std']:.4f}")
            
            # í´ë˜ìŠ¤ ë¶„í¬ ì¶œë ¥
            if 'class_distribution' in results:
                print("  - í´ë˜ìŠ¤ ë¶„í¬:")
                for label, count in results['class_distribution'].items():
                    regime_name = rf_model.regime_mapping[label]
                    print(f"    {regime_name}: {count}ê°œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return
    
    # í˜„ì¬ ì‹œì¥ ìƒíƒœ í™•ë¥  ì˜ˆì¸¡
    try:
        print("\nğŸ”® í˜„ì¬ ì‹œì¥ ìƒíƒœ ì˜ˆì¸¡ ì¤‘...")
        probabilities = rf_model.get_current_market_probabilities()
        
        print("\nğŸ“Š í˜„ì¬ ì‹œì¥ ìƒíƒœ í™•ë¥  (ML ê¸°ë°˜):")
        # í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)
        
        for regime, prob in sorted_probs:
            percentage = prob * 100
            if percentage > 50:
                print(f"  ğŸ¯ {regime.upper()}: {percentage:.1f}% (ì£¼ìš” ìƒíƒœ)")
            elif percentage > 25:
                print(f"  ğŸ“ˆ {regime.upper()}: {percentage:.1f}% (ë³´ì¡° ìƒíƒœ)")
            else:
                print(f"  ğŸ“Š {regime.upper()}: {percentage:.1f}%")
        
        # ìµœê³  í™•ë¥  ìƒíƒœ ì¶œë ¥
        max_regime, max_prob = max(probabilities.items(), key=lambda x: x[1])
        print(f"\nğŸ† í˜„ì¬ ì£¼ìš” ì‹œì¥ ìƒíƒœ: {max_regime.upper()} ({max_prob:.1%})")
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        print("ê¸°ë³¸ í™•ë¥  ë°˜í™˜:")
        for regime in ['trending_up', 'trending_down', 'volatile', 'sideways']:
            print(f"  {regime.upper()}: 25.0%")
    
    print("\nâœ… Random Forest ì‹œì¥ ìƒíƒœ ë¶„ë¥˜ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
