#!/usr/bin/env python3
"""
Random Forest ê¸°ë°˜ ì‹œì¥ ìƒíƒœ ë¶„ë¥˜ê¸°
ì‹¤ì œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹œì¥ ìƒíƒœ í™•ë¥ ì„ ì˜ˆì¸¡
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import logging
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))

from actions.y_finance import YahooFinanceDataCollector
from actions.calculate_index import TechnicalIndicators


class MarketRegimeRF:
    """Random Forest ê¸°ë°˜ ì‹œì¥ ìƒíƒœ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, verbose: bool = True):
        """
        MarketRegimeRF ì´ˆê¸°í™”
        
        Args:
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.verbose = verbose
        self.logger = logging.getLogger(__name__)
        
        # ëª¨ë¸ ê´€ë ¨ ë³€ìˆ˜ë“¤
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.is_trained = False
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸°
        self.data_collector = YahooFinanceDataCollector()
        self.tech_indicators = TechnicalIndicators()
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        self.model_dir = Path("models/market_regime")
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹œì¥ ìƒíƒœ ë§¤í•‘
        self.regime_mapping = {
            0: 'TRENDING_UP',
            1: 'TRENDING_DOWN', 
            2: 'VOLATILE',
            3: 'SIDEWAYS'
        }
        
        self.regime_mapping_reverse = {v: k for k, v in self.regime_mapping.items()}
    
    def _print(self, *args, level="info", **kwargs):
        """ë¡œê·¸ ì¶œë ¥"""
        if self.verbose:
            if level == "info":
                self.logger.info(*args, **kwargs)
            elif level == "warning":
                self.logger.warning(*args, **kwargs)
            elif level == "error":
                self.logger.error(*args, **kwargs)
            else:
                print(*args, **kwargs)
    
    def collect_training_data(self, start_date: str = "2020-01-01", end_date: str = None, data_dir: str = "data/macro") -> pd.DataFrame:
        """
        í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ (ì €ì¥ëœ ë°ì´í„° ì‚¬ìš©)
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ (Noneì´ë©´ í˜„ì¬)
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            í•™ìŠµìš© ë°ì´í„°í”„ë ˆì„
        """
        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')
        
        self._print(f"ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘: {start_date} ~ {end_date}")
        
        # SPY ë°ì´í„° ë¡œë“œ
        spy_path = f"{data_dir}/spy_data.csv"
        if not os.path.exists(spy_path):
            raise ValueError(f"SPY ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {spy_path}")
        
        spy_data = pd.read_csv(spy_path, index_col=0, parse_dates=False)
        
        self._print(f"ì›ë³¸ SPY ë°ì´í„° í¬ê¸°: {len(spy_data)}ê°œ")
        self._print(f"SPY ë°ì´í„° ì»¬ëŸ¼: {list(spy_data.columns)}")
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™”
        spy_data.columns = spy_data.columns.str.lower()
        
        # ë‚ ì§œ í•„í„°ë§
        if 'datetime' in spy_data.columns:
            self._print("datetime ì»¬ëŸ¼ ë°œê²¬, ë‚ ì§œ í•„í„°ë§ ì‹œì‘")
            try:
                # ì•ˆì „í•œ datetime ë³€í™˜
                spy_data['datetime'] = pd.to_datetime(spy_data['datetime'], errors='coerce')
                # NaN ê°’ ì œê±°
                spy_data = spy_data.dropna(subset=['datetime'])
                
                # íƒ€ì„ì¡´ ì •ë³´ê°€ ìˆìœ¼ë©´ ì œê±°
                if spy_data['datetime'].dt.tz is not None:
                    spy_data['datetime'] = spy_data['datetime'].dt.tz_localize(None)
                
                start_dt = pd.to_datetime(start_date)
                end_dt = pd.to_datetime(end_date)
                
                self._print(f"í•„í„°ë§ ì „ ë°ì´í„° í¬ê¸°: {len(spy_data)}ê°œ")
                self._print(f"ì‹œì‘ ë‚ ì§œ: {start_dt}, ì¢…ë£Œ ë‚ ì§œ: {end_dt}")
                self._print(f"ë°ì´í„° ë‚ ì§œ ë²”ìœ„: {spy_data['datetime'].min()} ~ {spy_data['datetime'].max()}")
                
                spy_data = spy_data[(spy_data['datetime'] >= start_dt) & (spy_data['datetime'] <= end_dt)]
                self._print(f"í•„í„°ë§ í›„ ë°ì´í„° í¬ê¸°: {len(spy_data)}ê°œ")
                
                spy_data.set_index('datetime', inplace=True)
                self._print(f"ë‚ ì§œ í•„í„°ë§ ì™„ë£Œ: {start_date} ~ {end_date}")
            except Exception as e:
                self._print(f"ë‚ ì§œ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}", level="error")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ë°ì´í„° ì‚¬ìš©
                self._print("ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨ë¡œ ì „ì²´ ë°ì´í„° ì‚¬ìš©")
        else:
            # ì¸ë±ìŠ¤ê°€ ìˆ«ìì¸ ê²½ìš°, ì „ì²´ ë°ì´í„° ì‚¬ìš©
            self._print("ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë°ì´í„° ì‚¬ìš©")
        
        if spy_data.empty:
            raise ValueError("í•„í„°ë§ëœ SPY ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ í™•ì¸
        if len(spy_data) < 50:
            raise ValueError(f"ë°ì´í„° í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(spy_data)}ê°œ (ìµœì†Œ 50ê°œ í•„ìš”)")
        
        self._print(f"SPY ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy_data)}ê°œ")
        
        # ë§¤í¬ë¡œ ë°ì´í„° ë¡œë“œ
        macro_symbols = ['^VIX', '^TNX', '^TYX', '^DXY', 'GC=F', '^TLT', '^TIP']
        macro_data = {}
        
        for symbol in macro_symbols:
            macro_path = f"{data_dir}/{symbol.lower()}_data.csv"
            if os.path.exists(macro_path):
                try:
                    data = pd.read_csv(macro_path, index_col=0, parse_dates=False)
                    if 'datetime' in data.columns:
                        data['datetime'] = pd.to_datetime(data['datetime'], utc=True)
                        data.set_index('datetime', inplace=True)
                    
                    # ë‚ ì§œ í•„í„°ë§
                    try:
                        if 'datetime' in data.columns:
                            # ì•ˆì „í•œ datetime ë³€í™˜
                            data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')
                            data = data.dropna(subset=['datetime'])
                            
                            # íƒ€ì„ì¡´ ì •ë³´ê°€ ìˆìœ¼ë©´ ì œê±°
                            if data['datetime'].dt.tz is not None:
                                data['datetime'] = data['datetime'].dt.tz_localize(None)
                            
                            start_dt = pd.to_datetime(start_date)
                            end_dt = pd.to_datetime(end_date)
                            data = data[(data['datetime'] >= start_dt) & (data['datetime'] <= end_dt)]
                            data.set_index('datetime', inplace=True)
                        else:
                            # ì¸ë±ìŠ¤ê°€ ë‚ ì§œì¸ ê²½ìš°
                            data.index = pd.to_datetime(data.index, errors='coerce')
                            data = data.dropna()
                            
                            if data.index.tz is not None:
                                data.index = data.index.tz_localize(None)
                            
                            start_dt = pd.to_datetime(start_date)
                            end_dt = pd.to_datetime(end_date)
                            data = data[(data.index >= start_dt) & (data.index <= end_dt)]
                    except Exception as e:
                        self._print(f"{symbol} ë‚ ì§œ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}", level="warning")
                        continue
                    
                    if not data.empty:
                        macro_data[symbol] = data
                        self._print(f"  {symbol} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ")
                except Exception as e:
                    self._print(f"  {symbol} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}", level="warning")
        
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        from ..actions.calculate_index import StrategyParams
        
        default_params = StrategyParams(
            atr_period=14,
            ema_short=20,
            ema_long=50,
            rsi_period=14,
            macd_fast=12,
            macd_slow=26,
            macd_signal=9,
            bb_period=20,
            bb_std=2.0,
            stoch_k_period=14,
            stoch_d_period=3,
            williams_r_period=14,
            cci_period=20,
            adx_period=14,
            obv_smooth_period=20,
            donchian_period=20,
            keltner_period=20,
            keltner_multiplier=2.0,
            volatility_period=20
        )
        self._print(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œì‘ (ë°ì´í„° í¬ê¸°: {len(spy_data)}ê°œ)")
        tech_data = self.tech_indicators.calculate_all_indicators(spy_data, default_params)
        self._print(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ (ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ)")
        
        # ë§¤í¬ë¡œ ë°ì´í„° ë³‘í•©
        self._print(f"ë§¤í¬ë¡œ ë°ì´í„° ë³‘í•© ì‹œì‘ (ë§¤í¬ë¡œ ë°ì´í„° ê°œìˆ˜: {len(macro_data)}ê°œ)")
        for symbol, data in macro_data.items():
            if 'close' in data.columns:
                tech_data[f'{symbol}_close'] = data['close']
            elif 'Close' in data.columns:
                tech_data[f'{symbol}_close'] = data['Close']
        
        self._print(f"ë§¤í¬ë¡œ ë°ì´í„° ë³‘í•© ì™„ë£Œ (ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ)")
        
        # ì¶”ê°€ íŠ¹ì„± ìƒì„±
        self._print("ê³ ê¸‰ íŠ¹ì„± ìƒì„± ì‹œì‘")
        tech_data = self._create_advanced_features(tech_data)
        self._print(f"ê³ ê¸‰ íŠ¹ì„± ìƒì„± ì™„ë£Œ (ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ)")
        
        # NaN ê°’ ì²˜ë¦¬ (ëª¨ë“  ì»¬ëŸ¼ì´ NaNì¸ í–‰ë§Œ ì œê±°)
        self._print(f"NaN ê°’ ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ")
        # NaN ë¹„ìœ¨ í™•ì¸
        nan_ratio = tech_data.isnull().sum() / len(tech_data)
        self._print(f"NaN ë¹„ìœ¨ì´ ë†’ì€ ì»¬ëŸ¼ë“¤: {nan_ratio[nan_ratio > 0.5].index.tolist()}")
        
        # ëª¨ë“  ì»¬ëŸ¼ì´ NaNì¸ í–‰ë§Œ ì œê±°
        tech_data = tech_data.dropna(how='all')
        self._print(f"ëª¨ë“  ì»¬ëŸ¼ì´ NaNì¸ í–‰ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ")
        
        # ë‚˜ë¨¸ì§€ NaN ê°’ì€ 0ìœ¼ë¡œ ì±„ì›€
        tech_data = tech_data.fillna(0)
        self._print(f"NaN ê°’ì„ 0ìœ¼ë¡œ ì±„ìš´ í›„ ë°ì´í„° í¬ê¸°: {len(tech_data)}ê°œ")
        
        self._print(f"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(tech_data)}ê°œ ìƒ˜í”Œ")
        return tech_data
    
    def _create_advanced_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ íŠ¹ì„± ìƒì„±"""
        # ì»¬ëŸ¼ëª… ë§¤í•‘
        close_col = 'close' if 'close' in data.columns else 'Close'
        
        # ê¸°ë³¸ ê°€ê²© íŠ¹ì„±ë“¤
        data['returns_1d'] = data[close_col].pct_change()
        data['returns_5d'] = data[close_col].pct_change(5)
        data['returns_20d'] = data[close_col].pct_change(20)
        data['volatility_20d'] = data['returns_1d'].rolling(20).std()
        # ë°ì´í„° í¬ê¸°ì— ë§ê²Œ rolling window ì¡°ì •
        max_window = min(60, len(data) // 2)
        data['volatility_60d'] = data['returns_1d'].rolling(max_window).std()
        
        # ì´ë™í‰ê·  ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'sma_20' in data.columns and 'sma_50' in data.columns:
            data['sma_ratio'] = data['sma_20'] / data['sma_50']
            data['price_sma20_ratio'] = data[close_col] / data['sma_20']
            data['price_sma50_ratio'] = data[close_col] / data['sma_50']
        
        # RSI ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'rsi' in data.columns:
            data['rsi_ma'] = data['rsi'].rolling(14).mean()
            data['rsi_std'] = data['rsi'].rolling(14).std()
            data['rsi_zscore'] = (data['rsi'] - data['rsi_ma']) / data['rsi_std']
        
        # MACD ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'macd' in data.columns and 'macd_signal' in data.columns:
            data['macd_histogram'] = data['macd'] - data['macd_signal']
            data['macd_ratio'] = data['macd'] / (data['macd_signal'] + 1e-8)
        
        # ë³¼ë¦°ì € ë°´ë“œ ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'bb_upper' in data.columns and 'bb_lower' in data.columns:
            data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data[close_col]
            data['bb_position'] = (data[close_col] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'])
        
        # ATR ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'atr' in data.columns:
            data['atr_ratio'] = data['atr'] / data[close_col]
            data['atr_ma'] = data['atr'].rolling(14).mean()
            data['atr_ratio_ma'] = data['atr_ratio'].rolling(14).mean()
        
        # ê±°ë˜ëŸ‰ ê´€ë ¨ íŠ¹ì„±ë“¤
        if 'volume' in data.columns:
            data['volume_ma'] = data['volume'].rolling(20).mean()
            data['volume_ratio'] = data['volume'] / data['volume_ma']
            data['volume_price_trend'] = (data['volume'] * data['returns_1d']).rolling(20).sum()
        
        # ë§¤í¬ë¡œ íŠ¹ì„±ë“¤
        macro_features = ['^VIX_close', '^TNX_close', '^TYX_close', '^DXY_close', 'GC=F_close', '^TLT_close', '^TIP_close']
        
        for feature in macro_features:
            if feature in data.columns:
                # ë§¤í¬ë¡œ ì§€í‘œì˜ ë³€í™”ìœ¨
                data[f'{feature}_change'] = data[feature].pct_change()
                data[f'{feature}_ma'] = data[feature].rolling(20).mean()
                data[f'{feature}_ratio'] = data[feature] / data[f'{feature}_ma']
                
                # VIX íŠ¹ë³„ ì²˜ë¦¬
                if feature == '^VIX_close':
                    data['vix_volatility'] = data[feature].rolling(20).std()
                    data['vix_percentile'] = data[feature].rolling(252).rank(pct=True)
                
                # ê¸ˆë¦¬ ê´€ë ¨ íŠ¹ì„±ë“¤
                if feature in ['^TNX_close', '^TYX_close']:
                    data[f'{feature}_spread'] = data[feature] - data['^TNX_close'] if feature == '^TYX_close' else 0
        
        # ë³µí•© íŠ¹ì„±ë“¤
        if 'rsi' in data.columns and 'volatility_20d' in data.columns:
            data['rsi_volatility'] = data['rsi'] * data['volatility_20d']
        
        if 'macd' in data.columns and 'volume_ratio' in data.columns:
            data['macd_volume'] = data['macd'] * data['volume_ratio']
        
        if '^VIX_close' in data.columns and 'returns_1d' in data.columns:
            data['vix_return_correlation'] = data['^VIX_close'].rolling(20).corr(data['returns_1d'])
        
        return data
    
    def _create_labels(self, data: pd.DataFrame, params: Dict[str, Any] = None) -> pd.DataFrame:
        """ì‹œì¥ ìƒíƒœ ë¼ë²¨ ìƒì„± (ë¯¸ë˜ ìˆ˜ìµë¥  ê¸°ë°˜)"""
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        if params is None:
            params = {
                'sma_short': 20, 'sma_long': 50, 'rsi_period': 14,
                'rsi_overbought': 70, 'rsi_oversold': 30, 'atr_period': 14,
                'trend_weight': 0.4, 'momentum_weight': 0.3,
                'volatility_weight': 0.2, 'macro_weight': 0.1,
                'base_position': 0.8, 'trending_boost': 1.2, 'volatile_reduction': 0.5
            }
        
        # ë¯¸ë˜ ìˆ˜ìµë¥  ê¸°ë°˜ ë¼ë²¨ ìƒì„±
        labels = []
        close_col = 'close' if 'close' in data.columns else 'Close'
        
        # ë¯¸ë˜ ìˆ˜ìµë¥  ê³„ì‚° (5ì¼ í›„)
        future_returns = data[close_col].pct_change(5).shift(-5)
        
        for i in range(len(data)):
            # ê¸°ë³¸ê°’ì€ SIDEWAYS (3)
            label = 3
            
            # ë¯¸ë˜ ìˆ˜ìµë¥ ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¼ë²¨ ìƒì„±
            if i < len(data) - 5 and not pd.isna(future_returns.iloc[i]):
                future_return = future_returns.iloc[i]
                
                # ìˆ˜ìµë¥  ê¸°ì¤€ìœ¼ë¡œ ë¼ë²¨ ë¶„ë¥˜
                if future_return > 0.02:  # 2% ì´ìƒ ìƒìŠ¹
                    label = 0  # TRENDING_UP
                elif future_return < -0.02:  # 2% ì´ìƒ í•˜ë½
                    label = 1  # TRENDING_DOWN
                elif abs(future_return) > 0.01:  # 1% ì´ìƒ ë³€ë™
                    label = 2  # VOLATILE
                else:
                    label = 3  # SIDEWAYS (1% ë¯¸ë§Œ ë³€ë™)
            
            labels.append(label)
        
        data['regime_label'] = labels
        self._print(f"ë¯¸ë˜ ìˆ˜ìµë¥  ê¸°ë°˜ ë¼ë²¨ ìƒì„± ì™„ë£Œ: {len(labels)}ê°œ")
        
        # ë¼ë²¨ ë¶„í¬ í™•ì¸
        label_counts = pd.Series(labels).value_counts().sort_index()
        self._print(f"ë¼ë²¨ ë¶„í¬: {dict(label_counts)}")
        
        return data
    
    def prepare_features(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """íŠ¹ì„± ì¤€ë¹„"""
        # ì‚¬ìš©í•  íŠ¹ì„±ë“¤ ì„ íƒ (ë¼ë²¨ ìƒì„±ì— ì‚¬ìš©ëœ ì§€í‘œë“¤ ì œì™¸)
        feature_columns = [
            # ê¸°ë³¸ ê°€ê²© íŠ¹ì„±ë“¤ (ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë…ë¦½ì )
            'returns_1d', 'returns_5d', 'returns_20d', 'volatility_20d', 'volatility_60d',
            
            # ì´ë™í‰ê·  íŠ¹ì„±ë“¤ (ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë…ë¦½ì )
            'sma_ratio', 'price_sma20_ratio', 'price_sma50_ratio',
            
            # MACD íŠ¹ì„±ë“¤ (ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë…ë¦½ì )
            'macd', 'macd_signal', 'macd_histogram', 'macd_ratio',
            
            # ë³¼ë¦°ì € ë°´ë“œ íŠ¹ì„±ë“¤ (ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë…ë¦½ì )
            'bb_width', 'bb_position',
            
            # ê±°ë˜ëŸ‰ íŠ¹ì„±ë“¤ (ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë…ë¦½ì )
            'volume_ratio', 'volume_price_trend',
            
            # ë§¤í¬ë¡œ íŠ¹ì„±ë“¤ (ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë…ë¦½ì )
            '^VIX_close', '^VIX_close_change', '^VIX_close_ratio', 'vix_volatility', 'vix_percentile',
            '^TNX_close', '^TNX_close_change', '^TNX_close_ratio',
            '^TYX_close', '^TYX_close_change', '^TYX_close_ratio',
            '^DXY_close', '^DXY_close_change', '^DXY_close_ratio',
            'GC=F_close', 'GC=F_close_change', 'GC=F_close_ratio',
            '^TLT_close', '^TLT_close_change', '^TLT_close_ratio',
            '^TIP_close', '^TIP_close_change', '^TIP_close_ratio',
            
            # ë³µí•© íŠ¹ì„±ë“¤ (ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë…ë¦½ì )
            'macd_volume', 'vix_return_correlation'
        ]
        
        # ì œì™¸ëœ íŠ¹ì„±ë“¤: RSI, ATR ê´€ë ¨ (ë¼ë²¨ ìƒì„±ì— ì‚¬ìš©ë¨)
        # 'rsi', 'rsi_ma', 'rsi_std', 'rsi_zscore', 'atr', 'atr_ratio', 'atr_ma', 'atr_ratio_ma', 'rsi_volatility'
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŠ¹ì„±ë“¤ë§Œ ì„ íƒ
        available_features = [col for col in feature_columns if col in data.columns]
        
        # íŠ¹ì„± ë°ì´í„° ì¤€ë¹„
        X = data[available_features].copy()
        
        # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(method='ffill').fillna(0)
        
        return X, available_features
    
    def train_model(self, data: pd.DataFrame = None, params: Dict[str, Any] = None, save_model: bool = True) -> Dict[str, Any]:
        """
        Random Forest ëª¨ë¸ í•™ìŠµ
        
        Args:
            data: í•™ìŠµ ë°ì´í„° (Noneì´ë©´ ìë™ ìˆ˜ì§‘)
            params: ì‹œì¥ ìƒíƒœ ë¶„ë¥˜ì— ì‚¬ìš©í•  íŒŒë¼ë¯¸í„° (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            save_model: ëª¨ë¸ ì €ì¥ ì—¬ë¶€
            
        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        if data is None:
            data = self.collect_training_data()
        
        # ë¼ë²¨ ìƒì„± ì‹œ íŒŒë¼ë¯¸í„° ì „ë‹¬
        data = self._create_labels(data, params)
        
        # íŠ¹ì„±ê³¼ ë¼ë²¨ ì¤€ë¹„
        X, feature_names = self.prepare_features(data)
        y = data['regime_label'].dropna()
        
        # ì¸ë±ìŠ¤ ë§ì¶”ê¸°
        common_index = X.index.intersection(y.index)
        X = X.loc[common_index]
        y = y.loc[common_index]
        
        if len(X) < 100:
            raise ValueError(f"í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(X)}ê°œ")
        
        self._print(f"ëª¨ë¸ í•™ìŠµ ì‹œì‘: {len(X)}ê°œ ìƒ˜í”Œ, {len(feature_names)}ê°œ íŠ¹ì„±")
        
        # ì‹œê°„ì  ë¶„í•  (ê³¼ê±° 70%ë¡œ í•™ìŠµ, ìµœê·¼ 30%ë¡œ í…ŒìŠ¤íŠ¸)
        split_idx = int(len(X) * 0.7)
        X_train = X.iloc[:split_idx]
        X_test = X.iloc[split_idx:]
        y_train = y.iloc[:split_idx]
        y_test = y.iloc[split_idx:]
        
        self._print(f"ì‹œê°„ì  ë¶„í• : í•™ìŠµ {len(X_train)}ê°œ, í…ŒìŠ¤íŠ¸ {len(X_test)}ê°œ")
        self._print(f"í•™ìŠµ ê¸°ê°„: {X_train.index[0]} ~ {X_train.index[-1]}")
        self._print(f"í…ŒìŠ¤íŠ¸ ê¸°ê°„: {X_test.index[0]} ~ {X_test.index[-1]}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¼ë²¨ ë¶„í¬ í™•ì¸
        test_label_counts = y_test.value_counts().sort_index()
        self._print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬: {dict(test_label_counts)}")
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Random Forest ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train_scaled, y_train)
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        train_score = self.model.score(X_train_scaled, y_train)
        test_score = self.model.score(X_test_scaled, y_test)
        
        # êµì°¨ ê²€ì¦
        cv_scores = cross_val_score(self.model, X_train_scaled, y_train, cv=5)
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # ê²°ê³¼ ì €ì¥
        self.feature_names = feature_names
        self.is_trained = True
        
        results = {
            'train_score': train_score,
            'test_score': test_score,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'feature_importance': feature_importance,
            'n_samples': len(X),
            'n_features': len(feature_names)
        }
        
        self._print(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ:")
        self._print(f"  í›ˆë ¨ ì •í™•ë„: {train_score:.4f}")
        self._print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.4f}")
        self._print(f"  êµì°¨ ê²€ì¦: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        
        # ëª¨ë¸ ì €ì¥
        if save_model:
            self.save_model()
        
        return results
    
    def predict_probabilities(self, data: pd.DataFrame) -> Dict[str, float]:
        """
        í˜„ì¬ ì‹œì¥ ìƒíƒœ í™•ë¥  ì˜ˆì¸¡
        
        Args:
            data: ì˜ˆì¸¡í•  ë°ì´í„°
            
        Returns:
            ì‹œì¥ ìƒíƒœë³„ í™•ë¥ 
        """
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # íŠ¹ì„± ì¤€ë¹„
        X, _ = self.prepare_features(data)
        
        # ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
        latest_data = X.iloc[-1:].copy()
        
        # ìŠ¤ì¼€ì¼ë§
        latest_scaled = self.scaler.transform(latest_data)
        
        # í™•ë¥  ì˜ˆì¸¡
        probabilities = self.model.predict_proba(latest_scaled)[0]
        
        # ê²°ê³¼ ë§¤í•‘
        result = {}
        for i, prob in enumerate(probabilities):
            regime_name = self.regime_mapping[i].lower()
            result[regime_name] = float(prob)
        
        return result
    
    def save_model(self, filepath: str = None):
        """ëª¨ë¸ ì €ì¥"""
        if filepath is None:
            filepath = self.model_dir / "market_regime_rf_model.pkl"
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'regime_mapping': self.regime_mapping,
            'trained_at': datetime.now().isoformat()
        }
        
        joblib.dump(model_data, filepath)
        self._print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    def load_model(self, filepath: str = None):
        """ëª¨ë¸ ë¡œë“œ"""
        if filepath is None:
            filepath = self.model_dir / "market_regime_rf_model.pkl"
        
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_names = model_data['feature_names']
        self.regime_mapping = model_data['regime_mapping']
        self.is_trained = True
        
        self._print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
    
    def get_current_market_probabilities(self, data_dir: str = "data/macro") -> Dict[str, float]:
        """
        í˜„ì¬ ì‹œì¥ ìƒíƒœ í™•ë¥  ê³„ì‚°
        
        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            í˜„ì¬ ì‹œì¥ ìƒíƒœë³„ í™•ë¥ 
        """
        # ìµœê·¼ ë°ì´í„° ë¡œë“œ
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=100)).strftime('%Y-%m-%d')
        
        data = self.collect_training_data(start_date, end_date, data_dir)
        
        # í™•ë¥  ì˜ˆì¸¡
        probabilities = self.predict_probabilities(data)
        
        return probabilities


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # Random Forest ëª¨ë¸ ì´ˆê¸°í™”
    rf_model = MarketRegimeRF(verbose=True)
    
    try:
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        rf_model.load_model()
        print("ê¸°ì¡´ ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print("ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        # ëª¨ë¸ í•™ìŠµ
        results = rf_model.train_model()
        print(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: í…ŒìŠ¤íŠ¸ ì •í™•ë„ {results['test_score']:.4f}")
    
    # í˜„ì¬ ì‹œì¥ ìƒíƒœ í™•ë¥  ì˜ˆì¸¡
    probabilities = rf_model.get_current_market_probabilities()
    
    print("\nğŸ“Š í˜„ì¬ ì‹œì¥ ìƒíƒœ í™•ë¥  (ML ê¸°ë°˜):")
    for regime, prob in probabilities.items():
        print(f"  {regime.upper()}: {prob:.1%}")


if __name__ == "__main__":
    main()
