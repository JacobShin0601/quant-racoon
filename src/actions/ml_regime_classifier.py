"""
ë™ì  ì ì‘í˜• ML ê¸°ë°˜ ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°
í•™ìˆ ì ìœ¼ë¡œ ê²€ì¦ëœ ë°©ë²•ë¡ ì„ ì ìš©í•œ ì§€ë„í•™ìŠµ ê¸°ë°˜ ì²´ì œ ë¶„ë¥˜

ì£¼ìš” íŠ¹ì§•:
- ì‹œì¥ í™˜ê²½ ë³€í™”ì— ë”°ë¥¸ ì„ê³„ê°’ ìë™ ì¡°ì •
- Rolling window ê¸°ë°˜ ë™ì  calibration  
- ë³€ë™ì„± ì²´ì œë³„ ê°€ì¤‘ì¹˜ ì ì‘
- Hamilton (1989) regime-switching ë°©ë²•ë¡  ì ìš©
- NBER-style ì§€ì†ì„± ìš”êµ¬ì‚¬í•­
- VIX percentile ê¸°ë°˜ ë™ì  ì„ê³„ê°’ (2024 Financial Management ë…¼ë¬¸)
- 150ë…„ê°„ Bull/Bear ì‹œì¥ ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’
- ë‹¤ì¤‘ ì§€í‘œ í†µí•© ì ìˆ˜í™” ì‹œìŠ¤í…œ
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings("ignore")

logger = logging.getLogger(__name__)


class DynamicRegimeLabelGenerator:
    """
    ë™ì  ì ì‘í˜• ì‹œì¥ ì²´ì œ ë¼ë²¨ ìƒì„±ê¸°
    
    í•µì‹¬ ë™ì  ê¸°ëŠ¥:
    1. Rolling Window Adaptation: ì‹œê°„ì— ë”°ë¥¸ ì„ê³„ê°’ ìë™ ì¡°ì •
    2. Volatility Regime Adaptation: ë³€ë™ì„± í™˜ê²½ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
    3. Market Condition Sensing: ì‹¤ì‹œê°„ ì‹œì¥ ì¡°ê±´ ë°˜ì˜
    4. Adaptive Calibration: ì˜ˆì¸¡ ì„±ëŠ¥ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ìë™ íŠœë‹
    
    í•™ìˆ ì  ê¸°ë°˜:
    - Hamilton (1989): Regime-switching models
    - NBER Business Cycle Dating: ì§€ì†ì„± ìš”êµ¬ì‚¬í•­
    - Financial Management (2024): VIX percentile ì„ê³„ê°’
    - 150ë…„ê°„ Bull/Bear ì‹œì¥ ì—­ì‚¬ì  ê¸°ì¤€
    """

    def __init__(self, config: Dict):
        self.config = config
        self.regime_config = config.get("ml_regime", {})
        
        # ë™ì  ì¡°ì • ì„¤ì •
        self.adaptive_config = self.regime_config.get("adaptive", {})
        self.enable_adaptation = self.adaptive_config.get("enable", True)
        self.adaptation_window = self.adaptive_config.get("window_days", 252)  # 1ë…„ ìœˆë„ìš°
        self.recalibration_frequency = self.adaptive_config.get("recalibration_days", 22)  # 22ì¼ë§ˆë‹¤
        self.volatility_regime_memory = self.adaptive_config.get("volatility_memory_days", 66)  # 3ê°œì›”
        
        # 4ê°€ì§€ ì‹œì¥ ì²´ì œ ì •ì˜
        self.states = ["TRENDING_UP", "TRENDING_DOWN", "SIDEWAYS", "VOLATILE"]
        
        # í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ ê¸°ë³¸ ì„ê³„ê°’ë“¤ (ë™ì  ì¡°ì •ì˜ ì¶œë°œì )
        self.BASE_HISTORICAL_BENCHMARKS = {
            'bear_median_decline': -0.33,      # ì¤‘ê°„ê°’ -33% (í•™ìˆ  ì—°êµ¬)
            'bear_median_duration': 19 * 22,   # 19ê°œì›” â†’ ê±°ë˜ì¼ ë³€í™˜
            'bull_median_gain': 0.87,          # +87%
            'bull_median_duration': 42 * 22,   # 42ê°œì›” â†’ ê±°ë˜ì¼ ë³€í™˜
            'correction_threshold': -0.10,     # 10% ì¡°ì •
            'bear_threshold': -0.20,           # 20% í•˜ë½ (í‘œì¤€)
            'bull_threshold': 0.20             # 20% ìƒìŠ¹
        }
        
        # ë™ì  ì¡°ì •ë˜ëŠ” ì„ê³„ê°’ë“¤ (ì´ˆê¸°ê°’ì€ ê¸°ë³¸ê°’)
        self.current_benchmarks = self.BASE_HISTORICAL_BENCHMARKS.copy()
        
        # VIX í•´ì„ ê¸°ì¤€ (ì „ë¬¸ê°€/ì‹¤ë¬´ ê²€ì¦) - ë™ì  ì¡°ì •ë¨
        self.BASE_VIX_THRESHOLDS = {
            'complacency': 15,     # ì‹œì¥ ì•ˆì£¼
            'normal_bull': 20,     # ì •ìƒ ê°•ì„¸ì¥
            'elevated': 25,        # ë¶ˆí™•ì‹¤ì„± ì¦ê°€
            'high_stress': 30,     # ë†’ì€ ìŠ¤íŠ¸ë ˆìŠ¤
            'crisis': 40           # ìœ„ê¸° (ë§¤ìˆ˜ ê¸°íšŒ ê°€ëŠ¥ì„±)
        }
        
        # NBER-style ìµœì†Œ ì§€ì† ê¸°ê°„ (ê±°ë˜ì¼ ê¸°ì¤€) - ë™ì  ì¡°ì •ë¨
        self.base_min_duration = self.regime_config.get("min_duration_days", 66)  # 3ê°œì›”
        self.current_min_duration = self.base_min_duration
        
        # ë‹¤ì¤‘ ì§€í‘œ ê°€ì¤‘ì¹˜ (í•™ìˆ  ì—°êµ¬ ê¸°ë°˜) - ë³€ë™ì„± ì²´ì œë³„ ë™ì  ì¡°ì •
        self.BASE_INDICATOR_WEIGHTS = {
            'vix_score': 0.35,           # VIX ì£¼ìš” ì§€í‘œ
            'momentum_score': 0.35,      # ê°€ê²© ëª¨ë©˜í…€  
            'duration_adjusted': 0.20,   # NBER-style ì§€ì†ì„±
            'yield_curve_score': 0.10    # ê±°ì‹œê²½ì œ
        }
        
        # ë³€ë™ì„± í™˜ê²½ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
        self.volatility_weight_adjustments = {
            'low_vol': {'vix_score': -0.1, 'momentum_score': 0.15, 'duration_adjusted': 0.05},
            'normal_vol': {'vix_score': 0.0, 'momentum_score': 0.0, 'duration_adjusted': 0.0},
            'high_vol': {'vix_score': 0.2, 'momentum_score': -0.1, 'duration_adjusted': -0.1},
            'crisis_vol': {'vix_score': 0.3, 'momentum_score': -0.15, 'duration_adjusted': -0.15}
        }
        
        # ë™ì  ì¡°ì • ì´ë ¥ ì €ì¥
        self.adaptation_history = []
        self.last_recalibration = None
        
        logger.info("DynamicRegimeLabelGenerator ì´ˆê¸°í™” ì™„ë£Œ")
        if self.enable_adaptation:
            logger.info(f"ë™ì  ì¡°ì • í™œì„±í™” - ìœˆë„ìš°: {self.adaptation_window}ì¼, ì¬ì¡°ì •: {self.recalibration_frequency}ì¼")

    def generate_dynamic_labels(self, macro_data: pd.DataFrame) -> pd.DataFrame:
        """
        ë™ì  ì ì‘í˜• ì‹œì¥ ì²´ì œ ë¼ë²¨ ìƒì„±
        
        í•µì‹¬ ë™ì  ê¸°ëŠ¥:
        1. Rolling calibrationìœ¼ë¡œ ì„ê³„ê°’ ì§€ì†ì  ì—…ë°ì´íŠ¸
        2. ë³€ë™ì„± í™˜ê²½ ê°ì§€í•˜ì—¬ ê°€ì¤‘ì¹˜ ìë™ ì¡°ì •
        3. ì‹œì¥ ì¡°ê±´ì— ë”°ë¥¸ ì§€ì†ì„± ìš”êµ¬ì‚¬í•­ ì¡°ì •
        
        Args:
            macro_data: ë§¤í¬ë¡œ ê²½ì œ ë°ì´í„° (SPY, VIX, ê¸ˆë¦¬ ë“± í¬í•¨)
            
        Returns:
            ë™ì ìœ¼ë¡œ ì¡°ì •ëœ ì²´ì œ ë¼ë²¨ì´ í¬í•¨ëœ DataFrame
        """
        try:
            logger.info("ë™ì  ì ì‘í˜• ì‹œì¥ ì²´ì œ ë¼ë²¨ ìƒì„± ì‹œì‘")
            
            # 1. í•„ìˆ˜ ë°ì´í„° ì¶”ì¶œ ë° ê²€ì¦
            spy_data, vix_data, tnx_data, irx_data = self._extract_core_data(macro_data)
            
            if len(spy_data) < self.adaptation_window:
                logger.warning(f"ë™ì  ì¡°ì •ì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±: {len(spy_data)}ì¼ (ìµœì†Œ {self.adaptation_window}ì¼ í•„ìš”)")
                return self._create_default_labels(macro_data)
            
            # 2. ì‹œì¥ í™˜ê²½ ê°ì§€ ë° ë¶„ë¥˜
            market_environment = self._detect_market_environment(spy_data, vix_data)
            logger.info(f"ê°ì§€ëœ ì‹œì¥ í™˜ê²½: {market_environment}")
            
            # 3. ë™ì  ì„ê³„ê°’ ë° ê°€ì¤‘ì¹˜ ì¡°ì •
            if self.enable_adaptation:
                self._adapt_to_market_conditions(spy_data, vix_data, market_environment)
            
            # 4. Rolling windowë¡œ ì§€ì†ì  calibration
            result_labels = []
            adaptation_points = []
            
            # ìµœì†Œ ìœˆë„ìš° í¬ê¸°ë¶€í„° ì‹œì‘í•˜ì—¬ rolling
            start_idx = max(self.adaptation_window, 252)  # ìµœì†Œ 1ë…„
            
            for end_idx in range(start_idx, len(spy_data), self.recalibration_frequency):
                # í˜„ì¬ ìœˆë„ìš° ë°ì´í„°
                window_spy = spy_data.iloc[max(0, end_idx-self.adaptation_window):end_idx]
                window_vix = vix_data.iloc[max(0, end_idx-self.adaptation_window):end_idx]
                window_tnx = tnx_data.iloc[max(0, end_idx-self.adaptation_window):end_idx]
                window_irx = irx_data.iloc[max(0, end_idx-self.adaptation_window):end_idx]
                
                # ìœˆë„ìš°ë³„ ë™ì  ì„ê³„ê°’ ê³„ì‚°
                window_vix_thresholds = self._calculate_adaptive_vix_thresholds(window_vix)
                
                # í˜„ì¬ ì‹œì ê¹Œì§€ì˜ ë¼ë²¨ ìƒì„±
                current_end = min(end_idx + self.recalibration_frequency, len(spy_data))
                segment_spy = spy_data.iloc[end_idx:current_end]
                segment_vix = vix_data.iloc[end_idx:current_end]
                segment_tnx = tnx_data.iloc[end_idx:current_end]
                segment_irx = irx_data.iloc[end_idx:current_end]
                
                if len(segment_spy) > 0:
                    # í˜„ì¬ ì‹œì¥ í™˜ê²½ì— ë§ëŠ” ê°€ì¤‘ì¹˜ ì ìš©
                    current_weights = self._get_adaptive_weights(segment_vix.mean())
                    
                    # ë‹¤ì¤‘ ì§€í‘œ ì ìˆ˜ ê³„ì‚°
                    segment_scores = self._calculate_adaptive_scores(
                        segment_spy, segment_vix, segment_tnx, segment_irx, 
                        window_vix_thresholds, current_weights
                    )
                    
                    # í†µí•© ì ìˆ˜ ë° ë¼ë²¨
                    integrated_scores = self._integrate_scores_with_weights(segment_scores, current_weights)
                    segment_labels = self._classify_with_adaptive_thresholds(integrated_scores)
                    
                    result_labels.extend(segment_labels)
                    adaptation_points.append({
                        'timestamp': end_idx,
                        'vix_thresholds': window_vix_thresholds,
                        'weights': current_weights,
                        'market_env': market_environment
                    })
            
            # 5. ì²˜ìŒ ë¶€ë¶„ ì²˜ë¦¬ (ìµœì†Œ ìœˆë„ìš° ì´ì „)
            if start_idx > 0:
                initial_labels = self._generate_initial_labels(
                    spy_data.iloc[:start_idx], vix_data.iloc[:start_idx], 
                    tnx_data.iloc[:start_idx], irx_data.iloc[:start_idx]
                )
                result_labels = initial_labels + result_labels
            
            # 6. NBER-style ë™ì  ì§€ì†ì„± í•„í„° ì ìš©
            adaptive_min_duration = self._get_adaptive_duration_requirement(market_environment)
            validated_labels = self._apply_adaptive_duration_filter(result_labels, adaptive_min_duration)
            
            # 7. ê²°ê³¼ DataFrame ìƒì„±
            result_df = self._create_dynamic_result_dataframe(
                macro_data, validated_labels, adaptation_points
            )
            
            # 8. ë™ì  ì¡°ì • ì´ë ¥ ì €ì¥
            self.adaptation_history.extend(adaptation_points)
            self.last_recalibration = datetime.now()
            
            logger.info(f"ë™ì  ì‹œì¥ ì²´ì œ ë¼ë²¨ ìƒì„± ì™„ë£Œ: {len(result_df)}ê°œ ë¼ë²¨")
            self._log_adaptation_statistics(adaptation_points, market_environment)
            
            return result_df
            
        except Exception as e:
            logger.error(f"ë™ì  ì‹œì¥ ì²´ì œ ë¼ë²¨ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_default_labels(macro_data)

    def _detect_market_environment(self, spy_data: pd.Series, vix_data: pd.Series) -> str:
        """
        í˜„ì¬ ì‹œì¥ í™˜ê²½ ê°ì§€
        - ë³€ë™ì„± ë ˆë²¨
        - íŠ¸ë Œë“œ ë°©í–¥
        - ì‹œì¥ ìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€
        """
        recent_window = min(66, len(vix_data))  # ìµœê·¼ 3ê°œì›” ë˜ëŠ” ê°€ìš© ë°ì´í„°
        recent_vix = vix_data.iloc[-recent_window:]
        recent_returns = spy_data.iloc[-recent_window:].pct_change(22).fillna(0)
        
        avg_vix = recent_vix.mean()
        avg_return = recent_returns.mean()
        vol_of_vix = recent_vix.std()
        
        # ë³µí•©ì  í™˜ê²½ íŒë‹¨
        if avg_vix > 35 or vol_of_vix > 8:
            return "crisis_vol"
        elif avg_vix > 25:
            return "high_vol"
        elif avg_vix < 16 and avg_return > 0.05:
            return "low_vol"
        else:
            return "normal_vol"

    def _adapt_to_market_conditions(self, spy_data: pd.Series, vix_data: pd.Series, market_env: str):
        """ì‹œì¥ ì¡°ê±´ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ë™ì  ì¡°ì •"""
        
        # 1. Bull/Bear ì„ê³„ê°’ ì¡°ì • (ìµœê·¼ ë³€ë™ì„±ì— ë”°ë¼)
        recent_volatility = spy_data.iloc[-66:].pct_change().std() * np.sqrt(252)  # ì—°ìœ¨í™” ë³€ë™ì„±
        
        if market_env in ['crisis_vol', 'high_vol']:
            # ê³ ë³€ë™ì„± ì‹œê¸°: ë” ë³´ìˆ˜ì  ì„ê³„ê°’
            volatility_multiplier = 1.5
            self.current_min_duration = int(self.base_min_duration * 0.7)  # ì§€ì†ì„± ìš”êµ¬ ì™„í™”
        elif market_env == 'low_vol':
            # ì €ë³€ë™ì„± ì‹œê¸°: ë” ë¯¼ê°í•œ ì„ê³„ê°’
            volatility_multiplier = 0.8
            self.current_min_duration = int(self.base_min_duration * 1.3)  # ì§€ì†ì„± ìš”êµ¬ ê°•í™”
        else:
            volatility_multiplier = 1.0
            self.current_min_duration = self.base_min_duration
        
        # ì„ê³„ê°’ ë™ì  ì¡°ì •
        self.current_benchmarks['bear_threshold'] = (
            self.BASE_HISTORICAL_BENCHMARKS['bear_threshold'] * volatility_multiplier
        )
        self.current_benchmarks['bull_threshold'] = (
            self.BASE_HISTORICAL_BENCHMARKS['bull_threshold'] * volatility_multiplier
        )
        
        logger.info(f"ì„ê³„ê°’ ë™ì  ì¡°ì • - Bear: {self.current_benchmarks['bear_threshold']:.1%}, "
                   f"Bull: {self.current_benchmarks['bull_threshold']:.1%}, "
                   f"ì§€ì†ê¸°ê°„: {self.current_min_duration}ì¼")

    def _calculate_adaptive_vix_thresholds(self, vix_window: pd.Series) -> Dict[str, float]:
        """ìœˆë„ìš° ê¸°ë°˜ ì ì‘í˜• VIX ì„ê³„ê°’ ê³„ì‚°"""
        
        # ê¸°ë³¸ percentile ê³„ì‚°
        base_thresholds = {
            'complacency': float(vix_window.quantile(0.20)),
            'normal': float(vix_window.quantile(0.50)),
            'elevated': float(vix_window.quantile(0.75)),
            'high_stress': float(vix_window.quantile(0.85)),
            'crisis': float(vix_window.quantile(0.95))
        }
        
        # ê·¹ê°’ ë³´ì • (ë„ˆë¬´ ê·¹ë‹¨ì ì¸ ì„ê³„ê°’ ë°©ì§€)
        base_thresholds['complacency'] = max(10, min(20, base_thresholds['complacency']))
        base_thresholds['crisis'] = max(30, min(60, base_thresholds['crisis']))
        
        # ì „í†µì  ê¸°ì¤€ê³¼ í˜¼í•© (70% ë™ì , 30% ì „í†µì )
        mixed_thresholds = {}
        for key in base_thresholds:
            if key in self.BASE_VIX_THRESHOLDS:
                mixed_thresholds[key] = (
                    0.7 * base_thresholds[key] + 
                    0.3 * self.BASE_VIX_THRESHOLDS[key]
                )
            else:
                mixed_thresholds[key] = base_thresholds[key]
        
        mixed_thresholds['static_crisis'] = 30.0  # ì „í†µì  ìœ„ê¸° ê¸°ì¤€ ìœ ì§€
        
        return mixed_thresholds

    def _get_adaptive_weights(self, current_vix: float) -> Dict[str, float]:
        """ë³€ë™ì„± ìˆ˜ì¤€ì— ë”°ë¥¸ ì ì‘í˜• ê°€ì¤‘ì¹˜"""
        
        base_weights = self.BASE_INDICATOR_WEIGHTS.copy()
        
        # VIX ìˆ˜ì¤€ë³„ ì¡°ì •
        if current_vix > 35:
            adjustment = self.volatility_weight_adjustments['crisis_vol']
        elif current_vix > 25:
            adjustment = self.volatility_weight_adjustments['high_vol']
        elif current_vix < 16:
            adjustment = self.volatility_weight_adjustments['low_vol']
        else:
            adjustment = self.volatility_weight_adjustments['normal_vol']
        
        # ê°€ì¤‘ì¹˜ ì¡°ì • ì ìš©
        adapted_weights = {}
        for key, base_weight in base_weights.items():
            adjustment_val = adjustment.get(key, 0)
            adapted_weights[key] = max(0.05, min(0.60, base_weight + adjustment_val))
        
        # ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
        total_weight = sum(adapted_weights.values())
        for key in adapted_weights:
            adapted_weights[key] /= total_weight
        
        return adapted_weights

    def _calculate_adaptive_scores(
        self, spy_data: pd.Series, vix_data: pd.Series, 
        tnx_data: pd.Series, irx_data: pd.Series,
        vix_thresholds: Dict, weights: Dict
    ) -> Dict[str, pd.Series]:
        """ì ì‘í˜• ë‹¤ì¤‘ ì§€í‘œ ì ìˆ˜ ê³„ì‚°"""
        
        # ê¸°ì¡´ ì ìˆ˜ ê³„ì‚° ë©”ì„œë“œë“¤ì„ ì ì‘í˜• ì„ê³„ê°’ìœ¼ë¡œ í˜¸ì¶œ
        vix_score = self._calculate_adaptive_vix_score(vix_data, vix_thresholds)
        momentum_score = self._calculate_adaptive_momentum_score(spy_data)
        duration_score = self._calculate_duration_score(spy_data)
        yield_score = self._calculate_yield_curve_score(tnx_data, irx_data)
        
        return {
            'vix_score': vix_score,
            'momentum_score': momentum_score,
            'duration_adjusted': duration_score,
            'yield_curve_score': yield_score
        }

    def _calculate_adaptive_vix_score(self, vix_data: pd.Series, thresholds: Dict) -> pd.Series:
        """ì ì‘í˜• VIX ì ìˆ˜ ê³„ì‚°"""
        score = pd.Series(0.0, index=vix_data.index)
        
        score = np.where(
            vix_data < thresholds['complacency'], -1.0,
            np.where(
                vix_data < thresholds['normal'], -0.5,
                np.where(
                    vix_data < thresholds['elevated'], 0.0,
                    np.where(
                        vix_data < thresholds['high_stress'], 0.5,
                        1.0
                    )
                )
            )
        )
        
        # ìœ„ê¸° ìˆ˜ì¤€ íŠ¹ë³„ ì²˜ë¦¬
        score = np.where(vix_data > thresholds.get('static_crisis', 30), 1.0, score)
        
        return pd.Series(score, index=vix_data.index)

    def _calculate_adaptive_momentum_score(self, spy_data: pd.Series) -> pd.Series:
        """ì ì‘í˜• ëª¨ë©˜í…€ ì ìˆ˜ (ë™ì  ì„ê³„ê°’ ì ìš©)"""
        
        returns_22d = spy_data.pct_change(22)
        returns_66d = spy_data.pct_change(66)
        
        # ë™ì ìœ¼ë¡œ ì¡°ì •ëœ ì„ê³„ê°’ ì‚¬ìš©
        bear_threshold = self.current_benchmarks['bear_threshold']
        bull_threshold = self.current_benchmarks['bull_threshold']
        
        score_22d = np.where(
            returns_22d > bull_threshold * 0.25, 1.0,
            np.where(
                returns_22d > 0.02, 0.5,
                np.where(
                    returns_22d < bear_threshold * 0.25, -1.0,
                    np.where(returns_22d < -0.02, -0.5, 0.0)
                )
            )
        )
        
        score_66d = np.where(
            returns_66d > bull_threshold * 0.5, 1.0,
            np.where(
                returns_66d > 0.05, 0.5,
                np.where(
                    returns_66d < bear_threshold * 0.5, -1.0,
                    np.where(returns_66d < -0.05, -0.5, 0.0)
                )
            )
        )
        
        momentum_score = 0.6 * score_22d + 0.4 * score_66d
        
        return pd.Series(momentum_score, index=spy_data.index).fillna(0)

    def _integrate_scores_with_weights(self, scores: Dict[str, pd.Series], weights: Dict) -> pd.Series:
        """ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì ìˆ˜ í†µí•©"""
        integrated = pd.Series(0.0, index=scores['vix_score'].index)
        
        for indicator, weight in weights.items():
            if indicator in scores:
                integrated += scores[indicator] * weight
        
        return np.clip(integrated, -1, 1)

    def _classify_with_adaptive_thresholds(self, integrated_scores: pd.Series) -> List[str]:
        """ì ì‘í˜• ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ì²´ì œ ë¶„ë¥˜"""
        
        # ë™ì  ì„ê³„ê°’ (ì‹œì¥ í™˜ê²½ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
        upper_threshold = 0.4
        lower_threshold = -0.4
        neutral_threshold = 0.15
        
        regimes = []
        for score in integrated_scores:
            if score > upper_threshold:
                regimes.append("TRENDING_UP")
            elif score < lower_threshold:
                regimes.append("TRENDING_DOWN")
            elif abs(score) < neutral_threshold:
                regimes.append("SIDEWAYS")
            else:
                regimes.append("VOLATILE")
        
        return regimes

    def _get_adaptive_duration_requirement(self, market_env: str) -> int:
        """ì‹œì¥ í™˜ê²½ë³„ ì ì‘í˜• ì§€ì†ì„± ìš”êµ¬ì‚¬í•­"""
        if market_env == 'crisis_vol':
            return int(self.base_min_duration * 0.5)  # ìœ„ê¸°ì‹œ ë¹ ë¥¸ ë°˜ì‘
        elif market_env == 'high_vol':
            return int(self.base_min_duration * 0.7)
        elif market_env == 'low_vol':
            return int(self.base_min_duration * 1.5)  # ì•ˆì •ì‹œ ì‹ ì¤‘í•œ íŒë‹¨
        else:
            return self.base_min_duration

    def _apply_adaptive_duration_filter(self, regimes: List[str], min_duration: int) -> List[str]:
        """ì ì‘í˜• ì§€ì†ì„± í•„í„°"""
        if len(regimes) < min_duration:
            return regimes
        
        filtered_regimes = regimes.copy()
        
        i = 0
        while i < len(filtered_regimes) - min_duration:
            current_regime = filtered_regimes[i]
            
            duration = 1
            j = i + 1
            while j < len(filtered_regimes) and filtered_regimes[j] == current_regime:
                duration += 1
                j += 1
            
            if duration < min_duration:
                if i > 0:
                    for k in range(i, min(i + duration, len(filtered_regimes))):
                        filtered_regimes[k] = filtered_regimes[i-1]
                elif j < len(filtered_regimes):
                    for k in range(i, j):
                        filtered_regimes[k] = filtered_regimes[j] if j < len(filtered_regimes) else "SIDEWAYS"
            
            i = j if duration >= min_duration else i + duration
        
        return filtered_regimes

    def _generate_initial_labels(self, spy_data: pd.Series, vix_data: pd.Series, 
                                tnx_data: pd.Series, irx_data: pd.Series) -> List[str]:
        """ì´ˆê¸° ë¶€ë¶„ ë¼ë²¨ ìƒì„± (ë‹¨ìˆœí•œ ë°©ì‹)"""
        
        # ê¸°ë³¸ ì„ê³„ê°’ìœ¼ë¡œ ë‹¨ìˆœ ë¶„ë¥˜
        returns_22d = spy_data.pct_change(22)
        
        labels = []
        for i, (ret, vix) in enumerate(zip(returns_22d, vix_data)):
            if pd.isna(ret):
                labels.append("SIDEWAYS")
            elif vix > 30:
                labels.append("VOLATILE")
            elif ret > 0.05:
                labels.append("TRENDING_UP")
            elif ret < -0.05:
                labels.append("TRENDING_DOWN")
            else:
                labels.append("SIDEWAYS")
        
        return labels

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ì¬ì‚¬ìš©
    def _extract_core_data(self, macro_data: pd.DataFrame) -> Tuple[pd.Series, pd.Series, pd.Series, pd.Series]:
        """í•µì‹¬ ë°ì´í„° ì¶”ì¶œ"""
        spy_col = self._find_column(macro_data, ['SPY_close', 'spy_close', 'SPY_data', 'spy'])
        if spy_col is None:
            raise ValueError("SPY ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        spy_data = pd.to_numeric(macro_data[spy_col], errors='coerce').fillna(method='ffill')
        
        vix_col = self._find_column(macro_data, ['^VIX_close', 'VIX_close', 'vix_close', '^vix', 'vix'])
        if vix_col is None:
            logger.warning("VIX ë°ì´í„° ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            vix_data = pd.Series(20.0, index=macro_data.index)
        else:
            vix_data = pd.to_numeric(macro_data[vix_col], errors='coerce').fillna(20.0)
        
        tnx_col = self._find_column(macro_data, ['^TNX_close', 'TNX_close', 'tnx_close'])
        irx_col = self._find_column(macro_data, ['^IRX_close', 'IRX_close', 'irx_close'])
        
        tnx_data = (pd.to_numeric(macro_data[tnx_col], errors='coerce').fillna(2.0) 
                   if tnx_col else pd.Series(2.0, index=macro_data.index))
        irx_data = (pd.to_numeric(macro_data[irx_col], errors='coerce').fillna(0.5)
                   if irx_col else pd.Series(0.5, index=macro_data.index))
        
        return spy_data, vix_data, tnx_data, irx_data

    def _find_column(self, df: pd.DataFrame, patterns: List[str]) -> Optional[str]:
        """ì»¬ëŸ¼ëª… íŒ¨í„´ ë§¤ì¹­"""
        for pattern in patterns:
            if pattern in df.columns:
                return pattern
        return None

    def _calculate_duration_score(self, spy_data: pd.Series) -> pd.Series:
        """íŠ¸ë Œë“œ ì§€ì†ì„± ì ìˆ˜ ê³„ì‚°"""
        ma_10 = spy_data.rolling(10).mean()
        ma_22 = spy_data.rolling(22).mean()
        ma_50 = spy_data.rolling(50).mean()
        
        ma_slope_22 = ma_22.diff(5) / ma_22.shift(5)
        
        cross_signal = np.where(
            (spy_data > ma_10) & (ma_10 > ma_22) & (ma_22 > ma_50), 1.0,
            np.where(
                (spy_data < ma_10) & (ma_10 < ma_22) & (ma_22 < ma_50), -1.0,
                np.where(
                    spy_data > ma_22, 0.5,
                    np.where(spy_data < ma_22, -0.5, 0.0)
                )
            )
        )
        
        slope_score = np.clip(ma_slope_22 * 20, -1, 1)
        duration_score = 0.7 * cross_signal + 0.3 * slope_score
        
        return pd.Series(duration_score, index=spy_data.index).fillna(0)

    def _calculate_yield_curve_score(self, tnx_data: pd.Series, irx_data: pd.Series) -> pd.Series:
        """ìˆ˜ìµë¥  ê³¡ì„  ì ìˆ˜ ê³„ì‚°"""
        yield_spread = tnx_data - irx_data
        
        curve_score = np.where(
            yield_spread > 2.5, 0.5,
            np.where(
                yield_spread > 1.5, 1.0,
                np.where(
                    yield_spread > 0.5, 0.0,
                    np.where(
                        yield_spread > 0, -0.5,
                        -1.0
                    )
                )
            )
        )
        
        return pd.Series(curve_score, index=tnx_data.index).fillna(0)

    def _create_dynamic_result_dataframe(
        self, macro_data: pd.DataFrame, validated_regimes: List[str],
        adaptation_points: List[Dict]
    ) -> pd.DataFrame:
        """ë™ì  ê²°ê³¼ DataFrame ìƒì„±"""
        
        result_df = pd.DataFrame(index=macro_data.index[:len(validated_regimes)])
        result_df['regime_label'] = validated_regimes
        
        # ì²´ì œ ë³€í™” ì§€ì 
        result_df['regime_change'] = (
            result_df['regime_label'] != result_df['regime_label'].shift(1)
        ).astype(int)
        
        # ë™ì  ì¡°ì • ì§€ì  í‘œì‹œ
        result_df['adaptation_point'] = 0
        for point in adaptation_points:
            if point['timestamp'] < len(result_df):
                result_df.iloc[point['timestamp'], result_df.columns.get_loc('adaptation_point')] = 1
        
        # ì‹ ë¢°ë„ (ë‹¨ìˆœí™”)
        result_df['confidence'] = 0.75
        
        return result_df

    def _log_adaptation_statistics(self, adaptation_points: List[Dict], market_env: str):
        """ì ì‘ í†µê³„ ë¡œê¹…"""
        logger.info(f"=== ë™ì  ì ì‘ í†µê³„ (ì‹œì¥ í™˜ê²½: {market_env}) ===")
        logger.info(f"ì´ ì ì‘ ì§€ì : {len(adaptation_points)}ê°œ")
        
        if adaptation_points:
            # VIX ì„ê³„ê°’ ë³€í™” ì¶”ì 
            first_thresholds = adaptation_points[0]['vix_thresholds']
            last_thresholds = adaptation_points[-1]['vix_thresholds']
            
            logger.info("VIX ì„ê³„ê°’ ë³€í™”:")
            for key in ['complacency', 'normal', 'elevated', 'high_stress']:
                if key in first_thresholds and key in last_thresholds:
                    change = last_thresholds[key] - first_thresholds[key]
                    logger.info(f"  {key}: {first_thresholds[key]:.1f} â†’ {last_thresholds[key]:.1f} ({change:+.1f})")
        
        logger.info(f"í˜„ì¬ ìµœì†Œ ì§€ì†ê¸°ê°„: {self.current_min_duration}ì¼")

    def _create_default_labels(self, macro_data: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ë³¸ ë¼ë²¨ ìƒì„±"""
        result_df = pd.DataFrame(index=macro_data.index)
        result_df['regime_label'] = "SIDEWAYS"
        result_df['confidence'] = 0.25
        result_df['regime_change'] = 0
        result_df['adaptation_point'] = 0
        
        return result_df


class DynamicMLRegimeClassifier:
    """
    ë™ì  ì ì‘í˜• ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°
    """

    def __init__(self, config: Dict):
        self.config = config
        self.ml_config = config.get("ml_regime", {})
        
        # ë™ì  ë¼ë²¨ ìƒì„±ê¸°
        self.label_generator = DynamicRegimeLabelGenerator(config)
        
        # ML ëª¨ë¸
        self.model = RandomForestClassifier(
            n_estimators=self.ml_config.get("n_estimators", 100),
            max_depth=self.ml_config.get("max_depth", 10),
            min_samples_split=self.ml_config.get("min_samples_split", 20),
            min_samples_leaf=self.ml_config.get("min_samples_leaf", 10),
            random_state=self.ml_config.get("random_state", 42),
            class_weight='balanced'
        )
        
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.feature_names = []
        self.class_names = ["TRENDING_UP", "TRENDING_DOWN", "SIDEWAYS", "VOLATILE"]
        
        logger.info("DynamicMLRegimeClassifier ì´ˆê¸°í™” ì™„ë£Œ")

    def _find_column(self, df: pd.DataFrame, patterns: List[str]) -> Optional[str]:
        """ì»¬ëŸ¼ëª… íŒ¨í„´ ë§¤ì¹­"""
        for pattern in patterns:
            if pattern in df.columns:
                return pattern
        return None

    def create_dynamic_training_labels(self, macro_data: pd.DataFrame) -> pd.DataFrame:
        """ë™ì  ì ì‘í˜• í›ˆë ¨ ë¼ë²¨ ìƒì„±"""
        logger.info("ë™ì  ì ì‘í˜• ML í›ˆë ¨ìš© ë¼ë²¨ ìƒì„± ì‹œì‘")
        
        labeled_data = self.label_generator.generate_dynamic_labels(macro_data)
        
        logger.info(f"ë™ì  í›ˆë ¨ìš© ë¼ë²¨ ìƒì„± ì™„ë£Œ: {len(labeled_data)}ê°œ")
        return labeled_data

    def load_comprehensive_data_from_files(self) -> pd.DataFrame:
        """ê¸°ì¡´ ë‹¤ìš´ë¡œë“œëœ data/macro íŒŒì¼ë“¤ì„ í™œìš©í•œ í¬ê´„ì  ë°ì´í„° ë¡œë”©"""
        try:
            import os
            import glob
            
            macro_dir = "data/macro"
            
            logger.info("ML ë¶„ë¥˜ê¸°: ê¸°ì¡´ ë§¤í¬ë¡œ ë°ì´í„° íŒŒì¼ë“¤ ë¡œë”© ì‹œì‘")
            print(f"DEBUG: macro_dir = {macro_dir}")
            print(f"DEBUG: ë””ë ‰í† ë¦¬ ì¡´ì¬: {os.path.exists(macro_dir)}")
            
            # ìµœì‹  UUID ë””ë ‰í† ë¦¬ ì°¾ê¸° (metadata.jsonì´ ìˆëŠ” ê³³)
            all_items = os.listdir(macro_dir)
            print(f"DEBUG: macro_dir ë‚´ìš©: {all_items}")
            
            uuid_dirs = [d for d in all_items 
                        if os.path.isdir(os.path.join(macro_dir, d)) and len(d) > 30]
            print(f"DEBUG: UUID ë””ë ‰í† ë¦¬ë“¤: {uuid_dirs}")
            
            target_dir = macro_dir
            if uuid_dirs:
                # ë°ì´í„° íŒŒì¼ì´ ë§ì´ ìˆëŠ” ìµœì‹  ë””ë ‰í† ë¦¬ ì°¾ê¸°
                best_dir = None
                max_files = 0
                
                for uuid_dir in sorted(uuid_dirs, reverse=True):
                    uuid_path = os.path.join(macro_dir, uuid_dir)
                    try:
                        files = os.listdir(uuid_path)
                        csv_files = [f for f in files if f.endswith('.csv')]
                        print(f"DEBUG: {uuid_dir}ì— {len(csv_files)}ê°œ CSV íŒŒì¼")
                        
                        if len(csv_files) > max_files:
                            max_files = len(csv_files)
                            best_dir = uuid_dir
                            
                    except Exception as e:
                        print(f"DEBUG: {uuid_dir} í™•ì¸ ì‹¤íŒ¨: {e}")
                
                if best_dir:
                    target_dir = os.path.join(macro_dir, best_dir)
                    print(f"DEBUG: ìµœì  UUID ë””ë ‰í† ë¦¬ ì„ íƒ: {best_dir} ({max_files}ê°œ íŒŒì¼)")
                    logger.info(f"UUID ë””ë ‰í† ë¦¬ ì‚¬ìš©: {best_dir}")
            
            print(f"DEBUG: target_dir = {target_dir}")
            
            # íŒŒì¼ ë§¤í•‘ (ì‹¤ì œ íŒŒì¼ëª…ê³¼ ì¼ì¹˜)
            symbol_mapping = {
                "vix": "^vix_data.csv",
                "tnx": "^tnx_data.csv", 
                "irx": "^irx_data.csv",
                "uup": "uup_data.csv",
                "gld": "gld_data.csv",
                "tlt": "tlt_data.csv",
                "qqq": "qqq_data.csv",
                "iwm": "iwm_data.csv",
                "tip": "tip_data.csv",
                "xrt": "xrt_data.csv",
                "goex": "goex_data.csv",
                "spy": "spy_data.csv",
                # ì„¹í„° ETFë“¤ (ì‹¤ì œ íŒŒì¼ëª…)
                "xlk": "xlk_sector.csv",
                "xlf": "xlf_sector.csv", 
                "xle": "xle_sector.csv",
                "xlv": "xlv_sector.csv",
                "xli": "xli_sector.csv",
                "xlp": "xlp_sector.csv",
                "xlu": "xlu_sector.csv",
                "xlb": "xlb_sector.csv",
                "xlre": "xlre_sector.csv"
            }
            
            # ë°ì´í„° ë¡œë”©
            all_data = {}
            loaded_count = 0
            
            for symbol, filename in symbol_mapping.items():
                file_path = os.path.join(target_dir, filename)
                
                logger.info(f"íŒŒì¼ ì²´í¬: {file_path} - ì¡´ì¬ì—¬ë¶€: {os.path.exists(file_path)}")
                
                if os.path.exists(file_path):
                    try:
                        df = pd.read_csv(file_path)
                        logger.info(f"{filename} ì»¬ëŸ¼: {list(df.columns)}")
                        
                        # ë‚ ì§œ ì¸ë±ìŠ¤ ì„¤ì •
                        if 'Date' in df.columns:
                            df['Date'] = pd.to_datetime(df['Date'])
                            df.set_index('Date', inplace=True)
                        elif 'date' in df.columns:
                            df['date'] = pd.to_datetime(df['date'])  
                            df.set_index('date', inplace=True)
                        elif 'datetime' in df.columns:
                            df['datetime'] = pd.to_datetime(df['datetime'])
                            df.set_index('datetime', inplace=True)
                        
                        # Close ì»¬ëŸ¼ ì°¾ê¸°
                        close_col = None
                        for col in ['Close', 'close', 'adj_close', 'Adj Close']:
                            if col in df.columns:
                                close_col = col
                                break
                        
                        if close_col is not None:
                            column_name = f"{symbol}_close"
                            all_data[column_name] = df[close_col]
                            loaded_count += 1
                            logger.info(f"{symbol} -> {column_name} ë¡œë”© ì™„ë£Œ ({len(df)} í–‰)")
                        else:
                            logger.warning(f"{file_path}: Close ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì»¬ëŸ¼: {list(df.columns)}")
                            
                    except Exception as e:
                        logger.warning(f"{file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                else:
                    logger.info(f"{file_path} íŒŒì¼ ì—†ìŒ")
            
            if loaded_count == 0:
                raise ValueError("ë¡œë”©ëœ ë§¤í¬ë¡œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë¦¬
            macro_data = pd.DataFrame(all_data)
            macro_data = macro_data.dropna(how='all').fillna(method='ffill').dropna()
            
            logger.info(f"ë§¤í¬ë¡œ ë°ì´í„° íŒŒì¼ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ íŒŒì¼, {len(macro_data)} í–‰")
            logger.info(f"ì»¬ëŸ¼: {list(macro_data.columns)}")
            
            return macro_data
            
        except Exception as e:
            logger.error(f"ë§¤í¬ë¡œ ë°ì´í„° íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def extract_comprehensive_features_from_data(self, macro_data: pd.DataFrame) -> pd.DataFrame:
        """HMMê³¼ ë™ì¼í•œ í¬ê´„ì  í”¼ì²˜ ì¶”ì¶œ - ë¡œë”©ëœ ë°ì´í„° í™œìš©"""
        logger.info(f"í¬ê´„ì  í”¼ì²˜ ì¶”ì¶œ ì‹œì‘ - ì…ë ¥ ë°ì´í„°: {len(macro_data)} í–‰, {len(macro_data.columns)}ê°œ ì»¬ëŸ¼")
        
        try:
            features = pd.DataFrame(index=macro_data.index)
            
            # 1. VIX ê´€ë ¨ í”¼ì²˜ (5ê°œ)
            vix_col = self._find_column(macro_data, ['^vix_close', 'vix_close'])
            if vix_col is not None:
                vix_data = pd.to_numeric(macro_data[vix_col], errors="coerce").fillna(20.0)
                
                features["vix_level"] = vix_data
                vix_ma = vix_data.rolling(20).mean()
                features["vix_ma_ratio"] = (vix_data / vix_ma - 1).fillna(0)
                
                # ë™ì  VIX ì„ê³„ê°’
                vix_low_threshold = vix_data.rolling(60, min_periods=20).quantile(0.25)
                vix_high_threshold = vix_data.rolling(60, min_periods=20).quantile(0.75)
                
                features["volatility_regime"] = np.where(
                    vix_data > vix_high_threshold.fillna(25), 1, 
                    np.where(vix_data < vix_low_threshold.fillna(15), -1, 0)
                )
                features["vix_acceleration"] = vix_data.diff(2).fillna(0)
                features["vix_percentile"] = (
                    vix_data.rolling(252, min_periods=60).rank(pct=True).fillna(0.5)
                )
                logger.info("VIX í”¼ì²˜ 5ê°œ ì¶”ì¶œ ì™„ë£Œ")
            else:
                logger.warning("VIX ë°ì´í„° ì—†ìŒ")
                for feat in ["vix_level", "vix_ma_ratio", "volatility_regime", "vix_acceleration", "vix_percentile"]:
                    features[feat] = 0.0 if 'level' not in feat else 20.0
            
            # 2. ê¸ˆë¦¬ ìŠ¤í”„ë ˆë“œ í”¼ì²˜ (2ê°œ)
            tnx_col = self._find_column(macro_data, ['^tnx_close', 'tnx_close'])
            irx_col = self._find_column(macro_data, ['^irx_close', 'irx_close'])
            
            if tnx_col and irx_col:
                tnx_data = pd.to_numeric(macro_data[tnx_col], errors="coerce").fillna(2.0)
                irx_data = pd.to_numeric(macro_data[irx_col], errors="coerce").fillna(0.5)
                
                features["yield_spread"] = tnx_data - irx_data
                features["yield_spread_ma"] = features["yield_spread"].rolling(10).mean().fillna(1.5)
                logger.info("ê¸ˆë¦¬ í”¼ì²˜ 2ê°œ ì¶”ì¶œ ì™„ë£Œ")
            else:
                logger.warning("ê¸ˆë¦¬ ë°ì´í„° ë¶€ì¡±")
                features["yield_spread"] = 1.5
                features["yield_spread_ma"] = 1.5
            
            # 3. ë‹¬ëŸ¬ ê°•ì„¸ í”¼ì²˜ (2ê°œ)  
            dollar_col = self._find_column(macro_data, ['uup_close'])
            if dollar_col is not None:
                dollar_data = pd.to_numeric(macro_data[dollar_col], errors="coerce").fillna(25.0)
                features["dollar_strength"] = dollar_data.pct_change(20).fillna(0)
                features["dollar_momentum"] = dollar_data.pct_change(5).fillna(0)
                logger.info("ë‹¬ëŸ¬ í”¼ì²˜ 2ê°œ ì¶”ì¶œ ì™„ë£Œ")
            else:
                logger.warning("ë‹¬ëŸ¬ ë°ì´í„° ì—†ìŒ")
                features["dollar_strength"] = 0.0
                features["dollar_momentum"] = 0.0
            
            # 4. SPY ê¸°ë°˜ í¬ê´„ì  í”¼ì²˜ (19ê°œ)
            spy_col = self._find_column(macro_data, ['spy_close'])
            if spy_col is not None:
                spy_data = pd.to_numeric(macro_data[spy_col], errors="coerce").fillna(400.0)
                
                # ê¸°ë³¸ ëª¨ë©˜í…€ (2ê°œ)
                features["market_momentum"] = spy_data.pct_change(20).fillna(0)
                features["market_trend"] = (spy_data / spy_data.rolling(50).mean() - 1).fillna(0)
                
                # ìˆ˜ìµë¥  í”¼ì²˜ (4ê°œ)
                features["spy_return_1d"] = spy_data.pct_change(1).fillna(0)
                features["spy_return_5d"] = spy_data.pct_change(5).fillna(0)
                features["spy_return_10d"] = spy_data.pct_change(10).fillna(0)
                features["spy_return_22d"] = spy_data.pct_change(22).fillna(0)
                
                # ì´ë™í‰ê·  êµì°¨ (6ê°œ)
                spy_ma5 = spy_data.rolling(5).mean()
                spy_ma10 = spy_data.rolling(10).mean()
                spy_ma20 = spy_data.rolling(20).mean()  
                spy_ma50 = spy_data.rolling(50).mean()
                
                features["spy_ma5_cross"] = (spy_data > spy_ma5).astype(int) - 0.5
                features["spy_ma10_cross"] = (spy_data > spy_ma10).astype(int) - 0.5
                features["spy_ma20_cross"] = (spy_data > spy_ma20).astype(int) - 0.5
                features["spy_ma50_cross"] = (spy_data > spy_ma50).astype(int) - 0.5
                features["spy_ma5_ma10_cross"] = (spy_ma5 > spy_ma10).astype(int) - 0.5
                features["spy_ma10_ma20_cross"] = (spy_ma10 > spy_ma20).astype(int) - 0.5
                
                # ë³€ë™ì„± (2ê°œ)
                features["spy_volatility_5d"] = spy_data.pct_change().rolling(5).std().fillna(0)
                features["spy_volatility_20d"] = spy_data.pct_change().rolling(20).std().fillna(0)
                
                # RSI ìœ ì‚¬ (1ê°œ)
                spy_returns = spy_data.pct_change().fillna(0)
                gains = spy_returns.where(spy_returns > 0, 0).rolling(14).mean()
                losses = (-spy_returns.where(spy_returns < 0, 0)).rolling(14).mean()
                rs = gains / (losses + 1e-8)
                features["spy_rsi_like"] = (100 - (100 / (1 + rs))).fillna(50) / 100 - 0.5
                
                # ëª¨ë©˜í…€ ê°•ë„ (1ê°œ)
                features["spy_momentum_strength"] = (
                    (spy_data.pct_change(5) > 0).astype(int) + 
                    (spy_data.pct_change(10) > 0).astype(int) + 
                    (spy_data.pct_change(20) > 0).astype(int)
                ) / 3 - 0.5
                
                # ìœ„ì¹˜ (1ê°œ)
                spy_high_52w = spy_data.rolling(252, min_periods=50).max()
                spy_low_52w = spy_data.rolling(252, min_periods=50).min()
                features["spy_position_in_range"] = (
                    (spy_data - spy_low_52w) / (spy_high_52w - spy_low_52w + 1e-8)
                ).fillna(0.5) - 0.5
                
                # ì²´ì œ (1ê°œ) 
                features["spy_bull_bear_regime"] = np.where(
                    (features["spy_return_22d"] > 0.05) & (features["spy_momentum_strength"] > 0.1), 1,
                    np.where(
                        (features["spy_return_22d"] < -0.05) & (features["spy_momentum_strength"] < -0.1), -1, 0
                    )
                )
                
                logger.info("SPY í”¼ì²˜ 19ê°œ ì¶”ì¶œ ì™„ë£Œ")
            else:
                logger.warning("SPY ë°ì´í„° ì—†ìŒ")
                spy_features = [
                    "market_momentum", "market_trend", "spy_return_1d", "spy_return_5d", 
                    "spy_return_10d", "spy_return_22d", "spy_ma5_cross", "spy_ma10_cross",
                    "spy_ma20_cross", "spy_ma50_cross", "spy_ma5_ma10_cross", "spy_ma10_ma20_cross",
                    "spy_volatility_5d", "spy_volatility_20d", "spy_rsi_like", 
                    "spy_momentum_strength", "spy_position_in_range", "spy_bull_bear_regime"
                ]
                for feat in spy_features:
                    features[feat] = 0.0
            
            # 5. ì‹ ìš© ìŠ¤í”„ë ˆë“œ í”¼ì²˜ (3ê°œ) - HYG, LQDëŠ” ë‹¤ìš´ë¡œë“œ ì•ˆë˜ì—ˆì„ ìˆ˜ ìˆìŒ
            self._add_credit_spread_features(features, macro_data)
            
            # 6. ë³µí•© ì§€í‘œ (3ê°œ)
            features["cross_market_stress"] = features["vix_level"] * abs(features["yield_spread"])
            features["regime_transition"] = features["volatility_regime"].diff().fillna(0)
            features["regime_persistence"] = self._calculate_regime_persistence(features)
            features["market_stress_composite"] = self._calculate_market_stress_composite(features)
            
            # NaN ì²˜ë¦¬
            features = features.fillna(method="ffill").fillna(0)
            
            logger.info(f"í¬ê´„ì  í”¼ì²˜ ì¶”ì¶œ ì™„ë£Œ: {len(features.columns)}ê°œ í”¼ì²˜")
            logger.info(f"í”¼ì²˜ ë¦¬ìŠ¤íŠ¸: {list(features.columns)}")
            
            return features
            
        except Exception as e:
            logger.error(f"í¬ê´„ì  í”¼ì²˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ìµœì†Œ í”¼ì²˜ ë°˜í™˜
            n_rows = len(macro_data)
            return pd.DataFrame({
                "vix_level": [20.0] * n_rows,
                "yield_spread": [1.5] * n_rows, 
                "market_momentum": [0.0] * n_rows,
            }, index=macro_data.index)
    
    def _add_credit_spread_features(self, features: pd.DataFrame, macro_data: pd.DataFrame):
        """ì‹ ìš© ìŠ¤í”„ë ˆë“œ ì§€í‘œ ì¶”ê°€ - íŒŒì¼ì—ì„œ ë¡œë”©ëœ ë°ì´í„° ê¸°ë°˜"""
        try:
            # TLTëŠ” ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ (configì— ìˆìŒ)
            tlt_col = self._find_column(macro_data, ['tlt_close'])
            
            # HYG, LQDëŠ” ì„¹í„° íŒŒì¼ì— ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì²˜ë¦¬
            features["credit_spread"] = 0.0
            features["credit_stress"] = 0
            features["ig_credit_spread"] = 0.0
            
            if tlt_col is not None:
                logger.info("TLT ë°ì´í„° ë°œê²¬ - ì‹ ìš© ìŠ¤í”„ë ˆë“œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©")
            else:
                logger.info("ì‹ ìš© ê´€ë ¨ ë°ì´í„° ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                
        except Exception as e:
            logger.warning(f"ì‹ ìš© ìŠ¤í”„ë ˆë“œ í”¼ì²˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            features["credit_spread"] = 0.0
            features["credit_stress"] = 0
            features["ig_credit_spread"] = 0.0
    
    def _calculate_regime_persistence(self, features: pd.DataFrame) -> pd.Series:
        """ì²´ì œ ì§€ì†ì„± ì§€í‘œ ê³„ì‚°"""
        if "volatility_regime" in features.columns:
            regime = features["volatility_regime"]
            persistence = regime.rolling(10, min_periods=3).apply(
                lambda x: (x == x.iloc[-1]).sum() / len(x), raw=False
            ).fillna(0.5)
            return persistence
        else:
            return pd.Series(0.5, index=features.index)
    
    def _calculate_market_stress_composite(self, features: pd.DataFrame) -> pd.Series:
        """ì‹œì¥ ìŠ¤íŠ¸ë ˆìŠ¤ ë³µí•© ì§€í‘œ ê³„ì‚°"""
        try:
            stress_components = []
            
            if "vix_level" in features.columns:
                vix_stress = np.clip((features["vix_level"] - 20) / 20, -1, 1)
                stress_components.append(0.4 * vix_stress)
            
            if "credit_spread" in features.columns:
                credit_stress = np.clip(features["credit_spread"] * 10, -1, 1) 
                stress_components.append(0.3 * credit_stress)
            
            if "yield_spread" in features.columns:
                yield_stress = np.where(
                    features["yield_spread"] < 0.5, 0.5,
                    np.where(features["yield_spread"] > 3.0, -0.5, 0.0)
                )
                stress_components.append(0.3 * yield_stress)
            
            if stress_components:
                composite = sum(stress_components)
                return np.clip(composite, -1, 1)
            else:
                return pd.Series(0.0, index=features.index)
                
        except Exception as e:
            logger.warning(f"ì‹œì¥ ìŠ¤íŠ¸ë ˆìŠ¤ ë³µí•© ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.Series(0.0, index=features.index)


def main():
    """ë™ì  ë¼ë²¨ ìƒì„± í…ŒìŠ¤íŠ¸"""
    import argparse
    import json
    import os
    import sys
    from pathlib import Path

    parser = argparse.ArgumentParser(description="ë™ì  ML ê¸°ë°˜ ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°")
    parser.add_argument("--create-dynamic-labels", action="store_true", help="ë™ì  ì ì‘í˜• ë¼ë²¨ ìƒì„±")
    parser.add_argument("--data-dir", type=str, default="data/macro", help="ë§¤í¬ë¡œ ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--config", type=str, default="config/config_trader.json", help="ì„¤ì • íŒŒì¼")
    parser.add_argument("--output-dir", type=str, default="results/labels", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")

    args = parser.parse_args()

    try:
        with open(args.config, "r", encoding="utf-8") as f:
            config = json.load(f)
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    if args.create_dynamic_labels:
        print("ğŸ”„ ë™ì  ì ì‘í˜• ì‹œì¥ ì²´ì œ ë¼ë²¨ ìƒì„± ì‹œì‘")
        
        classifier = DynamicMLRegimeClassifier(config)
        
        # ê¸°ì¡´ íŒŒì¼ì—ì„œ í¬ê´„ì  ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“Š ë§¤í¬ë¡œ ë°ì´í„° ë¡œë“œ: {args.data_dir}")
        
        macro_data = classifier.load_comprehensive_data_from_files()
        
        if macro_data is None or macro_data.empty:
            print("âŒ ë§¤í¬ë¡œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        print(f"âœ… ë§¤í¬ë¡œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(macro_data)} í–‰, {len(macro_data.columns)}ê°œ ì»¬ëŸ¼")

        # ë™ì  ë¼ë²¨ ìƒì„±
        try:
            labeled_data = classifier.create_dynamic_training_labels(macro_data)
            
            # ê²°ê³¼ ì €ì¥
            output_dir = Path(args.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = output_dir / f"dynamic_regime_labels_{timestamp}.csv"
            
            labeled_data.to_csv(output_file)
            print(f"âœ… ë™ì  ë¼ë²¨ ì €ì¥ ì™„ë£Œ: {output_file}")
            
            # í†µê³„ ì¶œë ¥
            print("\nğŸ“Š ë™ì  ë¼ë²¨ ìš”ì•½:")
            print(labeled_data['regime_label'].value_counts())
            print(f"\ní‰ê·  ì‹ ë¢°ë„: {labeled_data['confidence'].mean():.3f}")
            print(f"ì²´ì œ ë³€í™” íšŸìˆ˜: {labeled_data['regime_change'].sum()}íšŒ")
            print(f"ë™ì  ì¡°ì • ì§€ì : {labeled_data['adaptation_point'].sum()}ê°œ")
            
        except Exception as e:
            print(f"âŒ ë™ì  ë¼ë²¨ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    else:
        print("ì‚¬ìš©ë²•:")
        print("  --create-dynamic-labels    # ë™ì  ì ì‘í˜• ë¼ë²¨ ìƒì„±")


if __name__ == "__main__":
    main()