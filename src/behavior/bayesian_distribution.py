#!/usr/bin/env python3
"""
ë² ì´ì§€ì•ˆ ë¶„í¬ ë¶„ì„ ëª¨ë“ˆ
ìˆ˜ìµë¥  ë¶„í¬ ë¶„ì„, ë² ì´ì§€ì•ˆ ì„ í˜•íšŒê·€, ë³€ë™ì„± ëª¨ë¸ë§
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any
from scipy import stats
from scipy.stats import norm, t, skewnorm, jarque_bera, shapiro
from sklearn.linear_model import BayesianRidge, ARDRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings("ignore")


class BayesianDistributionAnalyzer:
    """ë² ì´ì§€ì•ˆ ë¶„í¬ ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self):
        self.results = {}
        self.scaler = StandardScaler()

    def analyze_return_distribution(
        self, data: pd.DataFrame, target_column: str = "return"
    ) -> Dict[str, Any]:
        """
        ìˆ˜ìµë¥  ë¶„í¬ ë¶„ì„

        Args:
            data: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
            target_column: ìˆ˜ìµë¥  ì»¬ëŸ¼ëª…

        Returns:
            ë¶„í¬ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        returns = data[target_column].dropna()

        if len(returns) < 10:
            raise ValueError("ë¶„ì„ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ê¸°ë³¸ í†µê³„ëŸ‰
        basic_stats = {
            "mean": returns.mean(),
            "std": returns.std(),
            "skewness": returns.skew(),
            "kurtosis": returns.kurtosis(),
            "min": returns.min(),
            "max": returns.max(),
            "q25": returns.quantile(0.25),
            "q50": returns.quantile(0.50),
            "q75": returns.quantile(0.75),
            "n_samples": len(returns),
        }

        # ì •ê·œì„± ê²€ì •
        normality_tests = {}

        # Shapiro-Wilk ê²€ì •
        shapiro_stat, shapiro_p = shapiro(returns)
        normality_tests["shapiro_wilk"] = {
            "statistic": shapiro_stat,
            "p_value": shapiro_p,
            "is_normal": shapiro_p > 0.05,
        }

        # Jarque-Bera ê²€ì •
        jb_stat, jb_p = jarque_bera(returns)
        normality_tests["jarque_bera"] = {
            "statistic": jb_stat,
            "p_value": jb_p,
            "is_normal": jb_p > 0.05,
        }

        # ë¶„í¬ í”¼íŒ…
        distribution_fits = {}

        try:
            # ì •ê·œë¶„í¬ í”¼íŒ…
            norm_params = norm.fit(returns)
            norm_aic = self._calculate_aic(returns, norm, norm_params)
            distribution_fits["normal"] = {
                "params": norm_params,
                "aic": norm_aic,
                "log_likelihood": norm.logpdf(returns, *norm_params).sum(),
            }
        except Exception as e:
            print(f"ì •ê·œë¶„í¬ í”¼íŒ… ì˜¤ë¥˜: {e}")
            distribution_fits["normal"] = {
                "params": (returns.mean(), returns.std()),
                "aic": float("inf"),
                "log_likelihood": 0,
            }

        try:
            # t-ë¶„í¬ í”¼íŒ…
            t_params = t.fit(returns)
            t_aic = self._calculate_aic(returns, t, t_params)
            distribution_fits["t"] = {
                "params": t_params,
                "aic": t_aic,
                "log_likelihood": t.logpdf(returns, *t_params).sum(),
            }
        except Exception as e:
            print(f"t-ë¶„í¬ í”¼íŒ… ì˜¤ë¥˜: {e}")
            distribution_fits["t"] = {
                "params": (returns.mean(), returns.std(), 3),
                "aic": float("inf"),
                "log_likelihood": 0,
            }

        try:
            # ìŠ¤íë“œ ì •ê·œë¶„í¬ í”¼íŒ…
            skewnorm_params = skewnorm.fit(returns)
            skewnorm_aic = self._calculate_aic(returns, skewnorm, skewnorm_params)
            distribution_fits["skew_normal"] = {
                "params": skewnorm_params,
                "aic": skewnorm_aic,
                "log_likelihood": skewnorm.logpdf(returns, *skewnorm_params).sum(),
            }
        except Exception as e:
            print(f"ìŠ¤íë“œ ì •ê·œë¶„í¬ í”¼íŒ… ì˜¤ë¥˜: {e}")
            distribution_fits["skew_normal"] = {
                "params": (returns.mean(), returns.std(), 0),
                "aic": float("inf"),
                "log_likelihood": 0,
            }

        # ìµœì  ë¶„í¬ ì„ íƒ (AIC ê¸°ì¤€)
        best_dist = min(distribution_fits.items(), key=lambda x: x[1]["aic"])

        # VaR ê³„ì‚° (95%, 99% ì‹ ë¢°ìˆ˜ì¤€)
        var_95 = np.percentile(returns, 5)
        var_99 = np.percentile(returns, 1)

        # ì •ê·œë¶„í¬ ê°€ì •í•˜ì˜ VaR
        norm_var_95 = norm.ppf(0.05, norm_params[0], norm_params[1])
        norm_var_99 = norm.ppf(0.01, norm_params[0], norm_params[1])

        result = {
            "target_column": target_column,
            "basic_statistics": basic_stats,
            "normality_tests": normality_tests,
            "distribution_fits": distribution_fits,
            "best_distribution": best_dist[0],
            "best_distribution_aic": best_dist[1]["aic"],
            "var_95": var_95,
            "var_99": var_99,
            "norm_var_95": norm_var_95,
            "norm_var_99": norm_var_99,
            "returns": returns.values,
        }

        self.results["distribution"] = result
        return result

    def _calculate_aic(self, data: pd.Series, dist, params) -> float:
        """AIC ê³„ì‚°"""
        try:
            # paramsê°€ íŠœí”Œì´ ì•„ë‹Œ ê²½ìš° íŠœí”Œë¡œ ë³€í™˜
            if not isinstance(params, tuple):
                params = (params,)

            log_likelihood = dist.logpdf(data, *params).sum()
            n_params = len(params)
            return 2 * n_params - 2 * log_likelihood
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            print(f"AIC ê³„ì‚° ì˜¤ë¥˜: {e}")
            return float("inf")

    def analyze_bayesian_regression(
        self,
        data: pd.DataFrame,
        target_column: str = "return",
        feature_columns: List[str] = None,
        test_size: float = 0.2,
        random_state: int = 42,
    ) -> Dict[str, Any]:
        """
        ë² ì´ì§€ì•ˆ ì„ í˜•íšŒê·€ ë¶„ì„

        Args:
            data: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
            target_column: ì¢…ì†ë³€ìˆ˜ ì»¬ëŸ¼ëª…
            feature_columns: ë…ë¦½ë³€ìˆ˜ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
            random_state: ëœë¤ ì‹œë“œ

        Returns:
            ë² ì´ì§€ì•ˆ íšŒê·€ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if feature_columns is None:
            # ì‹œê³„ì—´ ê´€ë ¨ ì»¬ëŸ¼ë“¤ê³¼ ì¤‘ë³µ íŠ¹ì„±ë“¤ ì œì™¸
            excluded_columns = {
                "datetime",
                "date",
                "time",
                "timestamp",
                "open",
                "high",
                "low",
                "close",
                "volume",
                "adjusted_close",
                "dividend_amount",
                "split_coefficient",
                "returns",  # ì´ì „ ìˆ˜ìµë¥  - targetê³¼ ì¤‘ë³µë˜ì–´ ì œì™¸
            }
            feature_columns = [
                col
                for col in data.columns
                if col != target_column and col not in excluded_columns
            ]

        # ë°ì´í„° ì¤€ë¹„
        analysis_data = data[feature_columns + [target_column]].dropna()

        if len(analysis_data) < 10:
            raise ValueError("ë¶„ì„ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        X = analysis_data[feature_columns]
        y = analysis_data[target_column]

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )

        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # Bayesian Ridge Regression
        bayesian_ridge = BayesianRidge(
            alpha_1=1e-6, alpha_2=1e-6, lambda_1=1e-6, lambda_2=1e-6
        )
        bayesian_ridge.fit(X_train_scaled, y_train)

        # ARD Regression (Automatic Relevance Determination)
        ard = ARDRegression(alpha_1=1e-6, alpha_2=1e-6, lambda_1=1e-6, lambda_2=1e-6)
        ard.fit(X_train_scaled, y_train)

        # ì˜ˆì¸¡
        y_pred_train_br = bayesian_ridge.predict(X_train_scaled)
        y_pred_test_br = bayesian_ridge.predict(X_test_scaled)
        y_pred_train_ard = ard.predict(X_train_scaled)
        y_pred_test_ard = ard.predict(X_test_scaled)

        # ì„±ëŠ¥ ì§€í‘œ
        def calculate_metrics(y_true, y_pred, prefix):
            return {
                f"{prefix}_r2": r2_score(y_true, y_pred),
                f"{prefix}_rmse": np.sqrt(mean_squared_error(y_true, y_pred)),
                f"{prefix}_mae": mean_absolute_error(y_true, y_pred),
            }

        br_metrics = calculate_metrics(y_train, y_pred_train_br, "br_train")
        br_metrics.update(calculate_metrics(y_test, y_pred_test_br, "br_test"))
        ard_metrics = calculate_metrics(y_train, y_pred_train_ard, "ard_train")
        ard_metrics.update(calculate_metrics(y_test, y_pred_test_ard, "ard_test"))

        # íŠ¹ì„± ì¤‘ìš”ë„ (ARDì˜ alpha_ ê°’)
        if isinstance(ard.alpha_, float) or np.isscalar(ard.alpha_):
            alpha_arr = np.array([ard.alpha_])
        else:
            alpha_arr = np.array(ard.alpha_)
        feature_importance_ard = dict(zip(feature_columns, 1.0 / alpha_arr))
        sorted_importance = sorted(
            feature_importance_ard.items(), key=lambda x: x[1], reverse=True
        )

        # ë² ì´ì§€ì•ˆ Ridge ê³„ìˆ˜ ì •ë³´
        br_coefficients = dict(zip(feature_columns, bayesian_ridge.coef_))
        br_intercept = bayesian_ridge.intercept_

        # ARD ê³„ìˆ˜ ì •ë³´
        ard_coefficients = dict(zip(feature_columns, ard.coef_))
        ard_intercept = ard.intercept_

        # ì¤‘ìš” íŠ¹ì„± ì„ íƒ (ARD alpha_ ê¸°ì¤€)
        important_features_ard = [
            f
            for f, imp in feature_importance_ard.items()
            if imp > np.mean(list(feature_importance_ard.values()))
        ]

        result = {
            "model_type": "Bayesian Linear Regression",
            "target_column": target_column,
            "feature_columns": feature_columns,
            "n_samples": len(analysis_data),
            "n_features": len(feature_columns),
            "train_size": len(X_train),
            "test_size": len(X_test),
            # Bayesian Ridge ê²°ê³¼
            "bayesian_ridge": {
                "coefficients": br_coefficients,
                "intercept": br_intercept,
                "alpha_": bayesian_ridge.alpha_,
                "lambda_": bayesian_ridge.lambda_,
                "metrics": br_metrics,
                "y_pred_train": y_pred_train_br,
                "y_pred_test": y_pred_test_br,
            },
            # ARD ê²°ê³¼
            "ard_regression": {
                "coefficients": ard_coefficients,
                "intercept": ard_intercept,
                "alpha_": ard.alpha_,
                "lambda_": ard.lambda_,
                "feature_importance": feature_importance_ard,
                "sorted_importance": sorted_importance,
                "important_features": important_features_ard,
                "metrics": ard_metrics,
                "y_pred_train": y_pred_train_ard,
                "y_pred_test": y_pred_test_ard,
            },
            # ëª¨ë¸ ê°ì²´
            "bayesian_ridge_model": bayesian_ridge,
            "ard_model": ard,
            "scaler": self.scaler,
            # ì‹¤ì œê°’
            "y_train": y_train.values,
            "y_test": y_test.values,
        }

        self.results["bayesian_regression"] = result
        return result

    def analyze_volatility(
        self, data: pd.DataFrame, target_column: str = "return", window: int = 20
    ) -> Dict[str, Any]:
        """
        ë³€ë™ì„± ë¶„ì„

        Args:
            data: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
            target_column: ìˆ˜ìµë¥  ì»¬ëŸ¼ëª…
            window: ì´ë™í‰ê·  ìœˆë„ìš° í¬ê¸°

        Returns:
            ë³€ë™ì„± ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        returns = data[target_column].dropna()

        if len(returns) < window:
            raise ValueError("ë¶„ì„ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ë³€ë™ì„± ê³„ì‚°
        volatility = returns.rolling(window=window).std()
        volatility_annualized = volatility * np.sqrt(252)  # ì—°ìœ¨í™”

        # ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ (ë³€ë™ì„±ì˜ ë³€ë™ì„±)
        vol_of_vol = volatility.rolling(window=window).std()

        # ë³€ë™ì„± ë¶„í¬ ë¶„ì„
        vol_stats = {
            "mean": volatility.mean(),
            "std": volatility.std(),
            "skewness": volatility.skew(),
            "kurtosis": volatility.kurtosis(),
            "min": volatility.min(),
            "max": volatility.max(),
        }

        # ë³€ë™ì„±ê³¼ ìˆ˜ìµë¥ ì˜ ê´€ê³„
        vol_return_corr = returns.corr(volatility)

        # ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ (ê°„ë‹¨í•œ AR ëª¨ë¸)
        vol_lag1 = volatility.shift(1)
        vol_lag2 = volatility.shift(2)

        # ë³€ë™ì„± ì˜ˆì¸¡ íšŒê·€
        vol_pred_data = pd.DataFrame(
            {"vol": volatility, "vol_lag1": vol_lag1, "vol_lag2": vol_lag2}
        ).dropna()

        if len(vol_pred_data) > 10:
            from sklearn.linear_model import LinearRegression

            vol_model = LinearRegression()
            vol_model.fit(vol_pred_data[["vol_lag1", "vol_lag2"]], vol_pred_data["vol"])
            vol_pred_r2 = vol_model.score(
                vol_pred_data[["vol_lag1", "vol_lag2"]], vol_pred_data["vol"]
            )
        else:
            vol_pred_r2 = None

        result = {
            "target_column": target_column,
            "window": window,
            "volatility": volatility.values,
            "volatility_annualized": volatility_annualized.values,
            "volatility_of_volatility": vol_of_vol.values,
            "volatility_statistics": vol_stats,
            "volatility_return_correlation": vol_return_corr,
            "volatility_prediction_r2": vol_pred_r2,
            "returns": returns.values,
        }

        self.results["volatility"] = result
        return result

    def plot_distribution_analysis(
        self, figsize: Tuple[int, int] = (15, 10), save_path: str = None
    ) -> plt.Figure:
        """ë¶„í¬ ë¶„ì„ í”Œë¡¯"""
        if "distribution" not in self.results:
            raise ValueError("ë¨¼ì € ë¶„í¬ ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

        result = self.results["distribution"]
        returns = result["returns"]

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)

        # 1. íˆìŠ¤í† ê·¸ë¨ê³¼ ë¶„í¬ í”¼íŒ…
        ax1.hist(
            returns,
            bins=30,
            density=True,
            alpha=0.7,
            color="skyblue",
            edgecolor="black",
        )

        # ìµœì  ë¶„í¬ í”Œë¡¯
        best_dist_name = result["best_distribution"]
        best_params = result["distribution_fits"][best_dist_name]["params"]

        x = np.linspace(returns.min(), returns.max(), 100)
        if best_dist_name == "normal":
            ax1.plot(
                x,
                norm.pdf(x, *best_params),
                "r-",
                lw=2,
                label=f"Best: {best_dist_name}",
            )
        elif best_dist_name == "t":
            ax1.plot(
                x, t.pdf(x, *best_params), "r-", lw=2, label=f"Best: {best_dist_name}"
            )
        elif best_dist_name == "skew_normal":
            ax1.plot(
                x,
                skewnorm.pdf(x, *best_params),
                "r-",
                lw=2,
                label=f"Best: {best_dist_name}",
            )

        ax1.set_xlabel("Returns")
        ax1.set_ylabel("Density")
        ax1.set_title("Return Distribution")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. Q-Q í”Œë¡¯
        stats.probplot(returns, dist="norm", plot=ax2)
        ax2.set_title("Q-Q Plot (Normal)")
        ax2.grid(True, alpha=0.3)

        # 3. ë¶„í¬ ë¹„êµ (AIC ê¸°ì¤€)
        dist_names = list(result["distribution_fits"].keys())
        aic_values = [result["distribution_fits"][d]["aic"] for d in dist_names]

        bars = ax3.bar(
            dist_names, aic_values, color=["blue", "green", "red"], alpha=0.7
        )
        ax3.set_ylabel("AIC")
        ax3.set_title("Distribution Comparison (AIC)")
        ax3.grid(axis="y", alpha=0.3)

        # ê°’ í‘œì‹œ
        for bar, aic in zip(bars, aic_values):
            ax3.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 0.1,
                f"{aic:.1f}",
                ha="center",
                va="bottom",
            )

        # 4. VaR ë¹„êµ
        var_methods = ["Empirical", "Normal"]
        var_95_values = [result["var_95"], result["norm_var_95"]]
        var_99_values = [result["var_99"], result["norm_var_99"]]

        x_pos = np.arange(len(var_methods))
        width = 0.35

        bars1 = ax4.bar(
            x_pos - width / 2, var_95_values, width, label="VaR 95%", alpha=0.7
        )
        bars2 = ax4.bar(
            x_pos + width / 2, var_99_values, width, label="VaR 99%", alpha=0.7
        )

        ax4.set_xlabel("Method")
        ax4.set_ylabel("VaR")
        ax4.set_title("Value at Risk Comparison")
        ax4.set_xticks(x_pos)
        ax4.set_xticklabels(var_methods)
        ax4.legend()
        ax4.grid(axis="y", alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            print(f"ë¶„í¬ ë¶„ì„ í”Œë¡¯ ì €ì¥: {save_path}")

        return fig

    def plot_bayesian_regression_results(
        self, figsize: Tuple[int, int] = (15, 10), save_path: str = None
    ) -> plt.Figure:
        """ë² ì´ì§€ì•ˆ íšŒê·€ ê²°ê³¼ í”Œë¡¯"""
        if "bayesian_regression" not in self.results:
            raise ValueError("ë¨¼ì € ë² ì´ì§€ì•ˆ íšŒê·€ ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

        result = self.results["bayesian_regression"]

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)

        # 1. ARD íŠ¹ì„± ì¤‘ìš”ë„
        ard_importance = result["ard_regression"]["feature_importance"]
        top_features = sorted(ard_importance.items(), key=lambda x: x[1], reverse=True)[
            :10
        ]

        features = [f[0] for f in top_features]
        importance = [f[1] for f in top_features]

        bars = ax1.barh(features, importance, color="green", alpha=0.7)
        ax1.set_xlabel("Importance (1/alpha_)")
        ax1.set_title("ARD Feature Importance (Top 10)")
        ax1.grid(axis="x", alpha=0.3)

        # 2. Bayesian Ridge vs ARD ê³„ìˆ˜ ë¹„êµ
        br_coeffs = result["bayesian_ridge"]["coefficients"]
        ard_coeffs = result["ard_regression"]["coefficients"]

        common_features = list(br_coeffs.keys())[:10]  # ìƒìœ„ 10ê°œë§Œ
        br_values = [br_coeffs[f] for f in common_features]
        ard_values = [ard_coeffs[f] for f in common_features]

        x_pos = np.arange(len(common_features))
        width = 0.35

        bars1 = ax2.bar(
            x_pos - width / 2, br_values, width, label="Bayesian Ridge", alpha=0.7
        )
        bars2 = ax2.bar(x_pos + width / 2, ard_values, width, label="ARD", alpha=0.7)

        ax2.set_xlabel("Features")
        ax2.set_ylabel("Coefficients")
        ax2.set_title("Coefficient Comparison")
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(common_features, rotation=45, ha="right")
        ax2.legend()
        ax2.grid(axis="y", alpha=0.3)

        # 3. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ (Bayesian Ridge)
        y_test = result["y_test"]
        y_pred_br = result["bayesian_ridge"]["y_pred_test"]

        ax3.scatter(y_test, y_pred_br, alpha=0.6, s=20)
        ax3.plot(
            [y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", alpha=0.8
        )
        ax3.set_xlabel("Actual")
        ax3.set_ylabel("Predicted")
        ax3.set_title(
            f'Bayesian Ridge: RÂ² = {result["bayesian_ridge"]["metrics"]["br_test_r2"]:.4f}'
        )
        ax3.grid(True, alpha=0.3)

        # 4. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ (ARD)
        y_pred_ard = result["ard_regression"]["y_pred_test"]

        ax4.scatter(y_test, y_pred_ard, alpha=0.6, s=20)
        ax4.plot(
            [y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", alpha=0.8
        )
        ax4.set_xlabel("Actual")
        ax4.set_ylabel("Predicted")
        ax4.set_title(
            f'ARD: RÂ² = {result["ard_regression"]["metrics"]["ard_test_r2"]:.4f}'
        )
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            print(f"ë² ì´ì§€ì•ˆ íšŒê·€ ê²°ê³¼ í”Œë¡¯ ì €ì¥: {save_path}")

        return fig

    def get_summary(self) -> str:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë¬¸ìì—´ ë°˜í™˜"""
        if not self.results:
            return "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        summary_lines = []
        summary_lines.append("=" * 60)
        summary_lines.append("ë² ì´ì§€ì•ˆ ë¶„í¬ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        summary_lines.append("=" * 60)

        # ë¶„í¬ ë¶„ì„ ê²°ê³¼
        if "distribution" in self.results:
            dist_result = self.results["distribution"]
            summary_lines.append("\nğŸ“Š ìˆ˜ìµë¥  ë¶„í¬ ë¶„ì„:")
            summary_lines.append("-" * 40)
            summary_lines.append(
                f"ìƒ˜í”Œ ìˆ˜: {dist_result['basic_statistics']['n_samples']}"
            )
            summary_lines.append(f"í‰ê· : {dist_result['basic_statistics']['mean']:.4f}")
            summary_lines.append(
                f"í‘œì¤€í¸ì°¨: {dist_result['basic_statistics']['std']:.4f}"
            )
            summary_lines.append(
                f"ì™œë„: {dist_result['basic_statistics']['skewness']:.4f}"
            )
            summary_lines.append(
                f"ì²¨ë„: {dist_result['basic_statistics']['kurtosis']:.4f}"
            )

            # ì •ê·œì„± ê²€ì •
            sw_test = dist_result["normality_tests"]["shapiro_wilk"]
            summary_lines.append(
                f"Shapiro-Wilk: p={sw_test['p_value']:.3e} ({'ì •ê·œ' if sw_test['is_normal'] else 'ë¹„ì •ê·œ'})"
            )

            # ìµœì  ë¶„í¬
            summary_lines.append(
                f"ìµœì  ë¶„í¬: {dist_result['best_distribution']} (AIC: {dist_result['best_distribution_aic']:.2f})"
            )

            # VaR
            summary_lines.append(f"VaR 95%: {dist_result['var_95']:.4f}")
            summary_lines.append(f"VaR 99%: {dist_result['var_99']:.4f}")

        # ë² ì´ì§€ì•ˆ íšŒê·€ ê²°ê³¼
        if "bayesian_regression" in self.results:
            br_result = self.results["bayesian_regression"]
            summary_lines.append("\nğŸ¤– ë² ì´ì§€ì•ˆ íšŒê·€ ë¶„ì„:")
            summary_lines.append("-" * 40)
            summary_lines.append(f"íŠ¹ì„± ìˆ˜: {br_result['n_features']}")
            summary_lines.append(f"ìƒ˜í”Œ ìˆ˜: {br_result['n_samples']}")

            # ì„±ëŠ¥ ë¹„êµ
            br_r2 = br_result["bayesian_ridge"]["metrics"]["br_test_r2"]
            ard_r2 = br_result["ard_regression"]["metrics"]["ard_test_r2"]
            summary_lines.append(f"Bayesian Ridge RÂ²: {br_r2:.4f}")
            summary_lines.append(f"ARD RÂ²: {ard_r2:.4f}")

            # ì¤‘ìš” íŠ¹ì„±
            important_features = br_result["ard_regression"]["important_features"][:5]
            summary_lines.append(
                f"ì¤‘ìš” íŠ¹ì„± (ìƒìœ„ 5ê°œ): {', '.join(important_features)}"
            )

        # ë³€ë™ì„± ë¶„ì„ ê²°ê³¼
        if "volatility" in self.results:
            vol_result = self.results["volatility"]
            summary_lines.append("\nğŸ“ˆ ë³€ë™ì„± ë¶„ì„:")
            summary_lines.append("-" * 40)
            summary_lines.append(
                f"í‰ê·  ë³€ë™ì„±: {vol_result['volatility_statistics']['mean']:.4f}"
            )
            summary_lines.append(
                f"ë³€ë™ì„±-ìˆ˜ìµë¥  ìƒê´€ê´€ê³„: {vol_result['volatility_return_correlation']:.4f}"
            )
            if vol_result["volatility_prediction_r2"]:
                summary_lines.append(
                    f"ë³€ë™ì„± ì˜ˆì¸¡ RÂ²: {vol_result['volatility_prediction_r2']:.4f}"
                )

        return "\n".join(summary_lines)

    def print_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(self.get_summary())
