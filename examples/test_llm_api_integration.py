#!/usr/bin/env python3
"""
LLM API í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ

ì‹¤ì œ LLM API (Bedrock, OpenAI)ë¥¼ í™œìš©í•œ ì‹œì¥ ë¶„ì„ ê°•í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.agent.enhancements import LLMAPIIntegration, LLMConfig


def create_sample_macro_data():
    """ìƒ˜í”Œ ë§¤í¬ë¡œ ë°ì´í„° ìƒì„±"""
    dates = pd.date_range(start="2024-01-01", end="2024-12-31", freq="D")

    # VIX ë°ì´í„° (ë³€ë™ì„± ì§€ìˆ˜)
    vix_data = pd.DataFrame(
        {
            "close": np.random.normal(20, 5, len(dates)),
            "volume": np.random.randint(1000, 10000, len(dates)),
        },
        index=dates,
    )

    # 10ë…„ êµ­ì±„ ìˆ˜ìµë¥ 
    tnx_data = pd.DataFrame(
        {
            "close": np.random.normal(4.0, 0.5, len(dates)),
            "volume": np.random.randint(500, 5000, len(dates)),
        },
        index=dates,
    )

    # TIPS (ì¸í”Œë ˆì´ì…˜ ë³´í˜¸ êµ­ì±„)
    tips_data = pd.DataFrame(
        {
            "close": np.random.normal(105, 2, len(dates)),
            "volume": np.random.randint(200, 2000, len(dates)),
        },
        index=dates,
    )

    # ë‹¬ëŸ¬ ì¸ë±ìŠ¤
    dxy_data = pd.DataFrame(
        {
            "close": np.random.normal(100, 3, len(dates)),
            "volume": np.random.randint(100, 1000, len(dates)),
        },
        index=dates,
    )

    return {"^VIX": vix_data, "^TNX": tnx_data, "^TIP": tips_data, "DX-Y.NYB": dxy_data}


def create_sample_market_metrics():
    """ìƒ˜í”Œ ì‹œì¥ ë©”íŠ¸ë¦­ ìƒì„±"""
    return {
        "current_probabilities": {
            "TRENDING_UP": 0.65,
            "TRENDING_DOWN": 0.15,
            "VOLATILE": 0.12,
            "SIDEWAYS": 0.08,
        },
        "stat_arb_signal": {
            "direction": "BULLISH",
            "signal_strength": 0.72,
            "confidence": 0.85,
            "individual_signals": {"XRT": 0.8, "XLF": 0.7, "GTX": 0.6, "VIX": 0.5},
        },
        "vix_level": 22.5,
        "regime_shift_detected": False,
        "confidence": 0.78,
    }


def test_rule_only_mode():
    """ê·œì¹™ ê¸°ë°˜ ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ ê·œì¹™ ê¸°ë°˜ ëª¨ë“œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # ì„¤ì •
    config = LLMConfig(provider="rule_only", fallback_to_rules=True)

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    llm_system = LLMAPIIntegration(config)

    # ìƒ˜í”Œ ë°ì´í„°
    macro_data = create_sample_macro_data()
    market_metrics = create_sample_market_metrics()

    # ì¸ì‚¬ì´íŠ¸ íšë“
    insights = llm_system.get_enhanced_insights(
        current_regime="TRENDING_UP",
        macro_data=macro_data,
        market_metrics=market_metrics,
    )

    print("ğŸ“Š ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼:")
    print(f"Regime ì¼ê´€ì„±: {insights['regime_validation']['consistency']:.3f}")
    print(f"ì§€ì§€ ìš”ì¸: {insights['regime_validation']['supporting_factors']}")
    print(f"ì¶©ëŒ ìš”ì¸: {insights['regime_validation']['conflicting_factors']}")
    print(f"ì „ëµì  ì¶”ì²œ: {insights['strategic_recommendations'][:3]}")  # ì²˜ìŒ 3ê°œë§Œ
    print()


def test_hybrid_mode():
    """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ í…ŒìŠ¤íŠ¸ (LLM API + ê·œì¹™ ê¸°ë°˜)"""
    print("ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # ì„¤ì • (ì‹¤ì œ API í‚¤ê°€ ì—†ìœ¼ë©´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ fallback)
    config = LLMConfig(
        provider="hybrid",
        model_name="anthropic.claude-3-sonnet-20240229-v1:0",
        api_key=None,  # ì‹¤ì œ API í‚¤ í•„ìš”
        region="us-east-1",
        fallback_to_rules=True,
    )

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    llm_system = LLMAPIIntegration(config)

    # ìƒ˜í”Œ ë°ì´í„°
    macro_data = create_sample_macro_data()
    market_metrics = create_sample_market_metrics()

    # ì¸ì‚¬ì´íŠ¸ íšë“
    insights = llm_system.get_enhanced_insights(
        current_regime="TRENDING_UP",
        macro_data=macro_data,
        market_metrics=market_metrics,
    )

    print("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼:")
    print(f"Regime ì¼ê´€ì„±: {insights['regime_validation']['consistency']:.3f}")
    print(f"ì§€ì§€ ìš”ì¸: {insights['regime_validation']['supporting_factors']}")
    print(f"ì¶©ëŒ ìš”ì¸: {insights['regime_validation']['conflicting_factors']}")
    print(f"ì „ëµì  ì¶”ì²œ: {insights['strategic_recommendations'][:3]}")

    # API í†µê³„ í™•ì¸
    stats = llm_system.get_api_stats()
    print(f"\nğŸ“ˆ API í†µê³„: {stats}")
    print()


def test_different_regimes():
    """ë‹¤ì–‘í•œ ì‹œì¥ ì²´ì œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¯ ë‹¤ì–‘í•œ ì‹œì¥ ì²´ì œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    config = LLMConfig(provider="rule_only")
    llm_system = LLMAPIIntegration(config)

    macro_data = create_sample_macro_data()
    market_metrics = create_sample_market_metrics()

    regimes = ["TRENDING_UP", "TRENDING_DOWN", "VOLATILE", "SIDEWAYS"]

    for regime in regimes:
        print(f"\nğŸ“Š {regime} ì²´ì œ ë¶„ì„:")

        # ë©”íŠ¸ë¦­ ì¡°ì •
        adjusted_metrics = market_metrics.copy()
        if regime == "TRENDING_UP":
            adjusted_metrics["current_probabilities"] = {
                "TRENDING_UP": 0.75,
                "TRENDING_DOWN": 0.10,
                "VOLATILE": 0.10,
                "SIDEWAYS": 0.05,
            }
        elif regime == "TRENDING_DOWN":
            adjusted_metrics["current_probabilities"] = {
                "TRENDING_UP": 0.10,
                "TRENDING_DOWN": 0.75,
                "VOLATILE": 0.10,
                "SIDEWAYS": 0.05,
            }
        elif regime == "VOLATILE":
            adjusted_metrics["current_probabilities"] = {
                "TRENDING_UP": 0.20,
                "TRENDING_DOWN": 0.20,
                "VOLATILE": 0.50,
                "SIDEWAYS": 0.10,
            }
            adjusted_metrics["vix_level"] = 35.0
        else:  # SIDEWAYS
            adjusted_metrics["current_probabilities"] = {
                "TRENDING_UP": 0.25,
                "TRENDING_DOWN": 0.25,
                "VOLATILE": 0.20,
                "SIDEWAYS": 0.30,
            }
            adjusted_metrics["vix_level"] = 15.0

        insights = llm_system.get_enhanced_insights(
            current_regime=regime,
            macro_data=macro_data,
            market_metrics=adjusted_metrics,
        )

        print(f"  ì¼ê´€ì„±: {insights['regime_validation']['consistency']:.3f}")
        print(f"  ìœ„í—˜ ìˆ˜ì¤€: {insights['risk_adjustments']['risk_level']}")
        print(
            f"  ì¶”ì²œ ì „ëµ: {insights['strategic_recommendations'][0] if insights['strategic_recommendations'] else 'N/A'}"
        )


def test_api_configuration():
    """API ì„¤ì • í…ŒìŠ¤íŠ¸"""
    print("âš™ï¸ API ì„¤ì • í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # Bedrock ì„¤ì •
    bedrock_config = LLMConfig(
        provider="bedrock",
        model_name="anthropic.claude-3-sonnet-20240229-v1:0",
        region="us-east-1",
    )

    # OpenAI ì„¤ì •
    openai_config = LLMConfig(
        provider="openai", model_name="gpt-4", api_key="your_openai_api_key_here"
    )

    # í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì •
    hybrid_config = LLMConfig(
        provider="hybrid",
        model_name="anthropic.claude-3-sonnet-20240229-v1:0",
        fallback_to_rules=True,
    )

    configs = [
        ("Bedrock", bedrock_config),
        ("OpenAI", openai_config),
        ("Hybrid", hybrid_config),
    ]

    for name, config in configs:
        print(f"\nğŸ”§ {name} ì„¤ì •:")
        print(f"  Provider: {config.provider}")
        print(f"  Model: {config.model_name}")
        print(f"  Region: {config.region}")
        print(f"  Max Tokens: {config.max_tokens}")
        print(f"  Temperature: {config.temperature}")
        print(f"  Fallback to Rules: {config.fallback_to_rules}")

        # ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        try:
            llm_system = LLMAPIIntegration(config)
            print(f"  âœ… ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            print(f"  âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


def test_performance_monitoring():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    config = LLMConfig(provider="rule_only")
    llm_system = LLMAPIIntegration(config)

    macro_data = create_sample_macro_data()
    market_metrics = create_sample_market_metrics()

    # ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ì—¬ í†µê³„ ìƒì„±
    for i in range(5):
        insights = llm_system.get_enhanced_insights(
            current_regime="TRENDING_UP",
            macro_data=macro_data,
            market_metrics=market_metrics,
        )

        stats = llm_system.get_api_stats()
        print(
            f"í˜¸ì¶œ {i+1}: ì„±ê³µë¥  {stats['success_rate']:.2f}, í‰ê·  ì‘ë‹µì‹œê°„ {stats['avg_response_time']:.3f}s"
        )

    # ìµœì¢… í†µê³„
    final_stats = llm_system.get_api_stats()
    print(f"\nğŸ“ˆ ìµœì¢… í†µê³„:")
    print(f"  ì´ í˜¸ì¶œ: {final_stats['total_calls']}")
    print(f"  ì„±ê³µ: {final_stats['successful_calls']}")
    print(f"  ì‹¤íŒ¨: {final_stats['failed_calls']}")
    print(f"  ì„±ê³µë¥ : {final_stats['success_rate']:.2%}")
    print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {final_stats['avg_response_time']:.3f}s")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ LLM API í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    try:
        # 1. ê·œì¹™ ê¸°ë°˜ ëª¨ë“œ í…ŒìŠ¤íŠ¸
        test_rule_only_mode()

        # 2. í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ í…ŒìŠ¤íŠ¸
        test_hybrid_mode()

        # 3. ë‹¤ì–‘í•œ ì‹œì¥ ì²´ì œ í…ŒìŠ¤íŠ¸
        test_different_regimes()

        # 4. API ì„¤ì • í…ŒìŠ¤íŠ¸
        test_api_configuration()

        # 5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
        test_performance_monitoring()

        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
